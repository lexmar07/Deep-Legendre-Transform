{"cells":[{"cell_type":"markdown","source":["# Deep Legendre Transform vs Classical Grid Methods\n","\n","This notebook implements and compares the **Deep Legendre Transform (DLT)** algorithm with classical grid-based methods for computing convex conjugates (Legendre-Fenchel transforms).\n","\n","## What this code does:\n","\n","### Methods Compared:\n","- **DLT (Deep Legendre Transform)**: Neural network-based approach using the implicit formulation `g(∇f(x)) ≈ ⟨x, ∇f(x)⟩ - f(x)`\n","  - Architectures: ResNet, MLP, ICNN (Input Convex Neural Networks)\n","- **Classical Methods**:\n","  - **Lucet's algorithm**: Efficient nested 1D convex hull computations with O(dN^(d+1)) complexity\n","  - **Direct method**: Brute-force grid evaluation with O(N^(2d)) complexity\n","\n","### Test Functions:\n","- **Quadratic**: `f(x) = 0.5 ∑x_i²` (closed-form conjugate available)\n","- **Negative Log**: `f(x) = -∑log(x_i)` (closed-form conjugate available)\n","- **Negative Entropy**: `f(x) = ∑x_i log(x_i)` (closed-form conjugate available)\n","\n","\n","\n","| **Function** | **$u(x)$** | **Domain $C$** | **$u^*(y)$** | **$D = \\nabla u(C)$** |\n","|--------------|------------|----------------|--------------|----------------------|\n","| Quadratic | $\\frac{1}{2} \\sum x_i^2$ | $\\mathcal{N}(0,1)^d$ | $\\frac{1}{2} \\sum y_i^2$ | $\\mathcal{N}(0,1)^d$ |\n","| Neg-Log | $-\\sum \\log(x_i)$ | $\\exp(U[-2.3,2.3])^d$ | $-d - \\sum \\log(-y_i)$ | $\\{-1/x : x\\in C\\} \\approx [-10,-0.1]^d$ |\n","| Neg-Entropy | $\\sum x_i \\log x_i$ | $\\exp(U[-2.3,2.3])^d$ | $\\sum \\exp(y_i - 1)$ | $\\{\\log x + 1 : x\\in C\\} \\approx [-1.3,3.3]^d$ |\n","\n","\n","### Key Features:\n","- Early stopping with patience for DLT training\n","- Optional log-uniform sampling for positive-domain functions\n","- Memory and compute feasibility checks for classical methods\n","- Comprehensive evaluation metrics (RMSE, relative L2 error, max error)\n","- Performance comparison across dimensions d ∈ {2, 3, ..., 50, 100, 200}\n","\n","### Output:\n","- CSV table with detailed performance metrics\n","- Log-log plots comparing accuracy vs computation time and memory usage\n","- Shows DLT scales to high dimensions where classical methods become infeasible\n","\n"],"metadata":{"id":"p4CKxcb7Nn_L"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Dmz9FCAqy04c","outputId":"c99a4d47-4ef0-415c-effb-59790f88f81a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Function        d | method   | model    | hidden       |   Nx |   Ns |    Ntr |    B |   steps |  tSolve(s) |  tEval(s) |   MB(act) |    max_err |       RMSE |      relL2\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","Quadratic       2 | Lucet    |          |              |   10 |   10 |        |      |         |       0.00 |      0.12 |       0.0 |   1.11e-01 |   7.71e-02 |   2.17e-02\n","Quadratic       2 | Direct   |          |              |   10 |   10 |        |      |         |       0.00 |      0.12 |       0.0 |   1.11e-01 |   7.71e-02 |   2.17e-02\n","Quadratic       2 | DLT      | RESNET   | 128,128      |      |      |    600 |   60 |   10000 |      21.48 |      2.78 |       1.1 |   6.73e-02 |   6.11e-03 |   1.72e-03\n","Quadratic       3 | Lucet    |          |              |   10 |   10 |        |      |         |       0.01 |      0.20 |       0.0 |   1.16e+01 |   4.24e+00 |   8.34e-01\n","Quadratic       3 | Direct   |          |              |   10 |   10 |        |      |         |       0.02 |      0.20 |       0.0 |   1.66e-01 |   1.15e-01 |   2.27e-02\n","Quadratic       3 | DLT      | RESNET   | 128,128      |      |      |    900 |   64 |   10000 |      20.30 |      0.62 |       1.1 |   1.14e-01 |   1.61e-02 |   3.17e-03\n","Quadratic       4 | Lucet    |          |              |   10 |   10 |        |      |         |       0.10 |      0.32 |       0.2 |   1.79e+01 |   9.92e+00 |   1.50e+00\n","Quadratic       4 | Direct   |          |              |   10 |   10 |        |      |         |       0.65 |      0.31 |       0.5 |   2.20e-01 |   1.51e-01 |   2.28e-02\n","Quadratic       4 | DLT      | RESNET   | 128,128      |      |      |   1200 |   64 |   10000 |      16.44 |      0.60 |       1.1 |   2.55e-01 |   4.07e-02 |   6.13e-03\n","Quadratic       5 | Lucet    |          |              |   10 |   10 |        |      |         |       1.25 |      0.55 |       1.5 |   3.56e+01 |   2.18e+01 |   2.68e+00\n","Quadratic       5 | Direct   |          |              |   10 |   10 |        |      |         |      75.37 |      0.55 |       6.1 |   2.74e-01 |   1.89e-01 |   2.32e-02\n","Quadratic       5 | DLT      | RESNET   | 128,128      |      |      |   1500 |   64 |   10000 |      16.45 |      0.68 |       1.1 |   2.71e-01 |   6.02e-02 |   7.40e-03\n","Quadratic       6 | Lucet    |          |              |   10 |   10 |        |      |         |      15.28 |      1.01 |      15.3 |   3.56e+01 |   2.16e+01 |   2.26e+00\n","Quadratic       6 | Direct   |          |              |   10 |   10 |        |      |         |   13198.88 |      1.00 |      68.7 |   3.29e-01 |   2.27e-01 |   2.37e-02\n","Quadratic       6 | DLT      | RESNET   | 128,128      |      |      |   1800 |   64 |   10751 |      17.58 |      0.64 |       1.1 |   3.97e-01 |   6.58e-02 |   6.88e-03\n","Quadratic       8 | Lucet    |          |              |   10 |   10 |        |      |         |    1939.32 |      4.07 |    1525.9 |   3.32e+01 |   1.90e+01 |   1.51e+00\n","Quadratic       8 | DLT      | RESNET   | 128,128      |      |      |   2400 |   64 |   16636 |      16.58 |      0.62 |       1.1 |   8.67e-01 |   1.27e-01 |   1.01e-02\n","Quadratic      10 | DLT      | RESNET   | 128,128      |      |      |   3000 |   64 |   20000 |      30.51 |      0.63 |       1.1 |   9.64e-01 |   1.15e-01 |   7.45e-03\n","Quadratic      20 | DLT      | RESNET   | 128,128      |      |      |   6000 |   64 |   59915 |      87.76 |      0.81 |       1.1 |   1.08e+00 |   2.24e-01 |   7.36e-03\n","Quadratic      50 | DLT      | RESNET   | 128,128      |      |      |  15000 |   64 |  100000 |     146.27 |      1.44 |       1.2 |   6.25e+00 |   1.13e+00 |   1.49e-02\n","Neg. Log        2 | Lucet    |          |              |   10 |   10 |        |      |         |       0.00 |      0.12 |       0.0 |   7.81e-01 |   3.65e-01 |   1.02e-01\n","Neg. Log        2 | Direct   |          |              |   10 |   10 |        |      |         |       0.00 |      0.13 |       0.0 |   7.81e-01 |   3.65e-01 |   1.02e-01\n","Neg. Log        2 | DLT      | RESNET   | 128,128      |      |      |    600 |   60 |   10000 |      16.97 |      0.02 |       1.1 |   2.26e-01 |   2.14e-02 |   5.96e-03\n","Neg. Log        3 | Lucet    |          |              |   10 |   10 |        |      |         |       0.01 |      0.20 |       0.0 |   1.51e+01 |   1.10e+01 |   2.10e+00\n","Neg. Log        3 | Direct   |          |              |   10 |   10 |        |      |         |       0.02 |      0.19 |       0.0 |   1.15e+00 |   4.94e-01 |   9.40e-02\n","Neg. Log        3 | DLT      | RESNET   | 128,128      |      |      |    900 |   64 |   10000 |      16.40 |      0.02 |       1.1 |   4.57e-01 |   3.66e-02 |   6.96e-03\n","Neg. Log        4 | Lucet    |          |              |   10 |   10 |        |      |         |       0.10 |      0.32 |       0.2 |   1.95e+01 |   1.48e+01 |   2.12e+00\n","Neg. Log        4 | Direct   |          |              |   10 |   10 |        |      |         |       0.68 |      0.31 |       0.5 |   1.43e+00 |   6.36e-01 |   9.13e-02\n","Neg. Log        4 | DLT      | RESNET   | 128,128      |      |      |   1200 |   64 |   10000 |      16.96 |      0.02 |       1.1 |   4.69e-01 |   5.34e-02 |   7.66e-03\n","Neg. Log        5 | Lucet    |          |              |   10 |   10 |        |      |         |       1.23 |      0.55 |       1.5 |   6.93e+00 |   1.78e+00 |   2.05e-01\n","Neg. Log        5 | Direct   |          |              |   10 |   10 |        |      |         |      72.67 |      0.56 |       6.1 |   1.73e+00 |   7.69e-01 |   8.89e-02\n","Neg. Log        5 | DLT      | RESNET   | 128,128      |      |      |   1500 |   64 |   10000 |      16.86 |      0.02 |       1.1 |   5.18e-01 |   6.54e-02 |   7.56e-03\n","Neg. Log        6 | Lucet    |          |              |   10 |   10 |        |      |         |      15.17 |      1.02 |      15.3 |   6.99e+00 |   1.83e+00 |   1.77e-01\n","Neg. Log        6 | Direct   |          |              |   10 |   10 |        |      |         |   10188.50 |      1.03 |      68.7 |   2.04e+00 |   8.93e-01 |   8.65e-02\n","Neg. Log        6 | DLT      | RESNET   | 128,128      |      |      |   1800 |   64 |   10751 |      17.90 |      0.02 |       1.1 |   5.37e-01 |   8.11e-02 |   7.85e-03\n","Neg. Log        8 | Lucet    |          |              |   10 |   10 |        |      |         |    1963.52 |      3.96 |    1525.9 |   3.78e+01 |   2.93e+01 |   2.13e+00\n","Neg. Log        8 | DLT      | RESNET   | 128,128      |      |      |   2400 |   64 |   16636 |      26.22 |      0.02 |       1.1 |   7.94e-01 |   1.33e-01 |   9.68e-03\n","Neg. Log       10 | DLT      | RESNET   | 128,128      |      |      |   3000 |   64 |   20000 |      32.09 |      0.02 |       1.1 |   9.19e-01 |   1.32e-01 |   7.74e-03\n","Neg. Log       20 | DLT      | RESNET   | 128,128      |      |      |   6000 |   64 |   59915 |      92.20 |      0.02 |       1.1 |   1.03e+00 |   1.92e-01 |   5.66e-03\n","Neg. Log       50 | DLT      | RESNET   | 128,128      |      |      |  15000 |   64 |  100000 |     154.58 |      0.02 |       1.2 |   5.00e+00 |   9.00e-01 |   1.06e-02\n","Neg. Entropy    2 | Lucet    |          |              |   10 |   10 |        |      |         |       0.00 |      0.12 |       0.0 |   4.90e-01 |   1.42e-01 |   2.59e-02\n","Neg. Entropy    2 | Direct   |          |              |   10 |   10 |        |      |         |       0.00 |      0.14 |       0.0 |   4.90e-01 |   1.42e-01 |   2.59e-02\n","Neg. Entropy    2 | DLT      | RESNET   | 128,128      |      |      |    600 |   60 |   10000 |      17.13 |      0.02 |       1.1 |   1.50e-01 |   2.02e-02 |   3.68e-03\n","Neg. Entropy    3 | Lucet    |          |              |   10 |   10 |        |      |         |       0.01 |      0.20 |       0.0 |   4.54e+01 |   2.58e+01 |   3.32e+00\n","Neg. Entropy    3 | Direct   |          |              |   10 |   10 |        |      |         |       0.02 |      0.19 |       0.0 |   6.07e-01 |   1.75e-01 |   2.25e-02\n","Neg. Entropy    3 | DLT      | RESNET   | 128,128      |      |      |    900 |   64 |   10000 |      14.73 |      0.02 |       1.1 |   3.39e-01 |   6.98e-02 |   8.98e-03\n","Neg. Entropy    4 | Lucet    |          |              |   10 |   10 |        |      |         |       0.10 |      0.32 |       0.2 |   9.08e+01 |   5.57e+01 |   5.58e+00\n","Neg. Entropy    4 | Direct   |          |              |   10 |   10 |        |      |         |       0.67 |      0.31 |       0.5 |   7.45e-01 |   2.05e-01 |   2.05e-02\n","Neg. Entropy    4 | DLT      | RESNET   | 128,128      |      |      |   1200 |   64 |   10000 |      15.33 |      0.02 |       1.1 |   1.01e+00 |   7.09e-02 |   7.10e-03\n","Neg. Entropy    5 | Lucet    |          |              |   10 |   10 |        |      |         |       1.24 |      0.55 |       1.5 |   1.30e+02 |   7.33e+01 |   6.03e+00\n","Neg. Entropy    5 | Direct   |          |              |   10 |   10 |        |      |         |      72.89 |      0.57 |       6.1 |   9.17e-01 |   2.30e-01 |   1.89e-02\n","Neg. Entropy    5 | DLT      | RESNET   | 128,128      |      |      |   1500 |   64 |   10000 |      17.28 |      0.02 |       1.1 |   1.70e+00 |   9.75e-02 |   8.04e-03\n","Neg. Entropy    6 | Lucet    |          |              |   10 |   10 |        |      |         |      15.11 |      1.01 |      15.3 |   1.30e+02 |   7.32e+01 |   5.13e+00\n","Neg. Entropy    6 | Direct   |          |              |   10 |   10 |        |      |         |   10328.59 |      1.02 |      68.7 |   1.04e+00 |   2.49e-01 |   1.75e-02\n","Neg. Entropy    6 | DLT      | RESNET   | 128,128      |      |      |   1800 |   64 |   10751 |      18.74 |      0.02 |       1.1 |   4.58e-01 |   7.99e-02 |   5.60e-03\n","Neg. Entropy    8 | Lucet    |          |              |   10 |   10 |        |      |         |    1935.47 |      3.98 |    1525.9 |   1.75e+02 |   1.08e+02 |   5.86e+00\n","Neg. Entropy    8 | DLT      | RESNET   | 128,128      |      |      |   2400 |   64 |   16636 |      27.16 |      0.02 |       1.1 |   9.55e-01 |   1.40e-01 |   7.57e-03\n","Neg. Entropy   10 | DLT      | RESNET   | 128,128      |      |      |   3000 |   64 |   20000 |      32.51 |      0.02 |       1.1 |   3.02e+00 |   4.47e-01 |   1.95e-02\n","Neg. Entropy   20 | DLT      | RESNET   | 128,128      |      |      |   6000 |   64 |   59915 |      92.63 |      0.02 |       1.1 |   1.98e+00 |   4.64e-01 |   1.05e-02\n","Neg. Entropy   50 | DLT      | RESNET   | 128,128      |      |      |  15000 |   64 |  100000 |     102.67 |      0.02 |       1.2 |   1.46e+01 |   3.42e+00 |   3.13e-02\n","\n","Wrote CSV: results_dlt_vs_classical.csv  (60 rows)\n","Saved figures to: figs\n"]}],"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","DLT (implicit) vs Classical (Nested Lucet + Direct) on SAME D = ∇u(C)\n","\n","What’s included\n","---------------\n","- Fix for the original math-domain error (no scalar-boolean `jnp.where`).\n","- Optional **log-uniform x-sampling** for positive-domain functions\n","  (neg_log, neg_entropy) via --x_loguniform / --no_x_loguniform.\n","- Fast Nested Lucet (`lucet_nd_fast`) with preallocated buffers (no nditer).\n","- DLT training with streaming minibatches + early stopping (Stopper).\n","- Evaluation enabled for quadratic, neg_log, **and neg_entropy**.\n","- CSV output and simple log-log plots.\n","\n","Examples\n","--------\n","# Paper-like domains, 10-per-dim classical grids, progress for Lucet\n","python dlt_vs_classical.py --dims 2 3 4 5 6 --domain_profile paper --lucet_progress\n","\n","# Disable log-uniform x-sampling (use uniform x on positive domains)\n","python dlt_vs_classical.py --dims 8 10 20 --no_x_loguniform\n","\n","# Force a specific DLT architecture and steps\n","python dlt_vs_classical.py --dims 10 20 --arch RESNET:256,256 --steps 40000\n","\"\"\"\n","\n","import os, sys, time, math, argparse, subprocess\n","import numpy as np\n","from typing import Callable, List, Tuple, Dict, Sequence\n","from functools import partial\n","from itertools import product\n","\n","# ---- minimal dependency check (Colab-safe) ----\n","def _ensure(pkgs):\n","    import importlib\n","    miss=[]\n","    for p in pkgs:\n","        try: importlib.import_module(p)\n","        except Exception: miss.append(p)\n","    if miss:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + miss)\n","\n","_ensure([\"flax\",\"optax\",\"pandas\",\"matplotlib\"])\n","\n","import jax, jax.numpy as jnp, optax\n","from jax import random\n","from flax import linen as nn\n","from flax.training import train_state\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# ========================= 1) Test functions & domains =========================\n","f_quad    = lambda x: 0.5*jnp.sum(x**2, -1)\n","grad_quad = lambda x: x\n","fst_quad  = lambda y: 0.5*jnp.sum(y**2, -1)\n","\n","f_nlog    = lambda x:-jnp.sum(jnp.log(x), -1)                # dom x>0\n","grad_nlog = lambda x:-1.0/x\n","fst_nlog  = lambda y:-jnp.sum(jnp.log(-y), -1) - y.shape[-1] # dom y<0\n","\n","f_nent    = lambda x:jnp.sum(x*jnp.log(x), -1)               # dom x>0\n","grad_nent = lambda x:jnp.log(x)+1.0\n","fst_nent  = lambda y:jnp.sum(jnp.exp(y-1.0), -1)\n","\n","# ---- Domain profiles ----------------------------------------------------------\n","DOMAINS_PAPER = {\n","    \"quadratic\":   dict(C=(-3.0,  3.0), D=(-3.0,  3.0)),\n","    \"neg_log\":     dict(C=( 0.1,  5.0), D=(-5.0, -0.1)),\n","    \"neg_entropy\": dict(C=(math.exp(-2.3), math.exp(2.3)), D=(-1.3, 3.3)),\n","}\n","DOMAINS_WIDE = {\n","    \"quadratic\":   dict(C=(-3.0,  3.0), D=(-3.0,  3.0)),\n","    \"neg_log\":     dict(C=( 0.1, 10.0), D=(-10.0,-0.1)),\n","    \"neg_entropy\": dict(C=(math.exp(-2.3), math.exp(2.3)), D=(-1.3, 3.3)),\n","}\n","\n","FUNS = {\n","    \"quadratic\": dict(f=f_quad, g=grad_quad, fst=fst_quad, printable=\"Quadratic\"),\n","    \"neg_log\": dict(f=f_nlog, g=grad_nlog, fst=fst_nlog, printable=\"Neg. Log\"),\n","    \"neg_entropy\": dict(f=f_nent, g=grad_nent, fst=fst_nent, printable=\"Neg. Entropy\"),\n","}\n","# Evaluate DLT error wherever u* is closed-form (all three here)\n","EVAL_DLT_FUNS = {\"quadratic\", \"neg_log\", \"neg_entropy\"}\n","\n","# ========================= 2) DLT model & training ============================\n","def _act(name: str) -> Callable:\n","    n=name.lower()\n","    if n==\"relu\": return nn.relu\n","    if n==\"gelu\": return jax.nn.gelu\n","    if n==\"softplus\": return jax.nn.softplus\n","    raise ValueError(f\"Unknown activation {name}\")\n","\n","class DensePos(nn.Module):\n","    features:int\n","    use_bias:bool=True\n","    @nn.compact\n","    def __call__(self,x):\n","        W=nn.softplus(self.param(\"rawW\", nn.initializers.lecun_normal(),\n","                                 (x.shape[-1], self.features)))\n","        y=x@W\n","        if self.use_bias: y += self.param(\"b\", nn.initializers.zeros, (self.features,))\n","        return y\n","\n","class ICNN(nn.Module):\n","    hidden:Sequence[int]\n","    act: Callable = nn.softplus\n","    @nn.compact\n","    def __call__(self,x):\n","        z=jnp.zeros((x.shape[0],1))\n","        for h in self.hidden:\n","            z=self.act(DensePos(h)(z) + nn.Dense(h)(x))\n","        out=DensePos(1, use_bias=False)(z) + nn.Dense(1, use_bias=False)(x)\n","        return jnp.squeeze(out, -1)\n","\n","class MLP(nn.Module):\n","    hidden:Sequence[int]\n","    act: Callable = nn.relu\n","    @nn.compact\n","    def __call__(self,x):\n","        for h in self.hidden: x=self.act(nn.Dense(h)(x))\n","        return jnp.squeeze(nn.Dense(1)(x), -1)\n","\n","class ResBlock(nn.Module):\n","    features:int\n","    act: Callable\n","    @nn.compact\n","    def __call__(self, x):\n","        h = self.act(nn.Dense(self.features)(x))\n","        h = nn.Dense(self.features)(h)\n","        if x.shape[-1] != self.features:\n","            x = nn.Dense(self.features, use_bias=False)(x)\n","        return self.act(h + x)\n","\n","class ResNet(nn.Module):\n","    hidden:Sequence[int]\n","    act: Callable = jax.nn.gelu\n","    @nn.compact\n","    def __call__(self, x):\n","        assert len(self.hidden) >= 1\n","        h = self.act(nn.Dense(self.hidden[0])(x))\n","        for w in self.hidden:\n","            h = ResBlock(w, act=self.act)(h)\n","        out = nn.Dense(1)(h)\n","        return jnp.squeeze(out, -1)\n","\n","def parse_hidden(s): return tuple(int(v) for v in s.split(\",\") if v)\n","\n","class State(train_state.TrainState): ...\n","def schedule(lr): return optax.exponential_decay(lr, 20_000, 0.5, staircase=True)\n","def new_state(rng, model, d, lr):\n","    params = model.init(rng, jnp.zeros((1,d), jnp.float32))[\"params\"]\n","    return State.create(apply_fn=model.apply, params=params, tx=optax.adam(schedule(lr)))\n","\n","# implicit DLT loss: g(∇u(x)) ≈ <x,∇u(x)> − u(x)\n","def loss_impl(p,af,x,f,g):\n","    y=g(x); target=jnp.sum(x*y, -1) - f(x); pred=af({\"params\":p}, y)\n","    return jnp.mean((pred - target)**2)\n","\n","@partial(jax.jit, static_argnums=(2,3))\n","def step_impl(st, x, f, g):\n","    l,gr=jax.value_and_grad(loss_impl)(st.params, st.apply_fn, x, f, g)\n","    return st.apply_gradients(grads=gr), l\n","\n","def count_param_bytes(params):\n","    leaves = jax.tree_util.tree_leaves(params)\n","    return sum(np.prod(l.shape) * np.dtype(l.dtype).itemsize for l in leaves)\n","\n","def estimate_dlt_mem_mb(params, d, bs, model_name, hidden):\n","    pbytes = count_param_bytes(params); adam=2*pbytes; grads=pbytes\n","    act_dim = d + sum(hidden) + 1\n","    act_bytes = bs * act_dim * 4  # float32\n","    return (pbytes + adam + grads + act_bytes) / (1024**2)\n","\n","def clamp(v, lo, hi): return max(lo, min(hi, v))\n","def auto_steps_from_dim(d: int) -> int:\n","    base = max(10_000, int(round(1000 * d * math.log(max(d, 2)))))\n","    if d <= 10:   return clamp(base, 5_000, 20_000)\n","    if d <= 100:  return clamp(base, 20_000, 100_000)\n","    return max(base, 100_000)\n","def auto_patience_for_steps(steps: int, d: int) -> int:\n","    if d <= 10:   return max(3000, min(steps//5, 10_000))\n","    if d <= 100:  return max(5000, min(steps//4, 20_000))\n","    return max(10_000, min(steps//3, 30_000))\n","def auto_samples_and_batch(d: int, k_train: int) -> Tuple[int,int]:\n","    k = int(clamp(k_train, 100, 1000))\n","    N = max(d, k * d)\n","    B = max(16, min(64, max(1, N // 10)))\n","    return N, B\n","\n","class Stopper:\n","    def __init__(self, pat:int, tol:float=1e-6):\n","        self.best=float(\"inf\"); self.pat=int(pat); self.tol=float(tol)\n","        self.cnt=0; self.bp=None\n","    def update(self, loss, params):\n","        lv=float(loss)\n","        if lv + self.tol < self.best:\n","            self.best, self.cnt, self.bp = lv, 0, params\n","        else:\n","            self.cnt += 1\n","        return self.cnt >= self.pat or self.best < self.tol\n","    def res(self): return self.best, self.bp\n","\n","def train_dlt_dataset(model_maker, d, f, g, samp_x, steps, lr, patience, seed, N, bs):\n","    key0 = random.PRNGKey(seed)\n","    st   = new_state(key0, model_maker(), d, lr)\n","    stop = Stopper(patience)\n","    t0   = time.perf_counter()\n","    for i in range(int(steps)):\n","        mb = samp_x(random.fold_in(key0, i), (int(bs), d))\n","        st, loss = step_impl(st, mb, f, g)\n","        if stop.update(loss, st.params): break\n","    best_loss, best_params = stop.res()\n","    if best_params is None: best_params = st.params\n","    return best_params, time.perf_counter() - t0, int(bs)\n","\n","# ========================= 3) Classical: fast Lucet & Direct ==================\n","def llt_1d(x,u,s):\n","    x,u,s = map(np.asarray,(x,u,s))\n","    hx,hu=np.empty_like(x),np.empty_like(u); h=0\n","    for xi,ui in zip(x,u):\n","        while h>=2 and (hu[h-1]-hu[h-2])*(xi-hx[h-1]) >= (ui-hu[h-1])*(hx[h-1]-hx[h-2]):\n","            h-=1\n","        hx[h],hu[h]=xi,ui; h+=1\n","    hx,hu=hx[:h],hu[:h]\n","    edge=np.concatenate(([-np.inf],np.diff(hu)/np.diff(hx),[np.inf]))\n","    out,k=np.empty_like(s),0\n","    for j,sj in enumerate(s):\n","        while sj>edge[k+1]: k+=1\n","        out[j]=sj*hx[k]-hu[k]\n","    return out\n","\n","def _llt_1d_inplace_signed(x, u, s, hx, hu, edge, out, negate_u: bool):\n","    n = x.shape[0]; m = s.shape[0]\n","    h = 0\n","    for i in range(n):\n","        xi = x[i]\n","        ui = -u[i] if negate_u else u[i]\n","        while h >= 2 and (hu[h-1]-hu[h-2])*(xi-hx[h-1]) >= (ui-hu[h-1])*(hx[h-1]-hx[h-2]):\n","            h -= 1\n","        hx[h] = xi\n","        hu[h] = ui\n","        h += 1\n","    edge[0] = -np.inf\n","    for i in range(h-1):\n","        edge[i+1] = (hu[i+1] - hu[i]) / (hx[i+1] - hx[i])\n","    edge[h] = np.inf\n","    k = 0\n","    for j in range(m):\n","        sj = s[j]\n","        while sj > edge[k+1]:\n","            k += 1\n","        out[j] = sj * hx[k] - hu[k]\n","\n","def lucet_nd_fast(x_arrs, f, s_arrs, progress=False):\n","    d = len(x_arrs)\n","    X = np.meshgrid(*x_arrs, indexing='ij', sparse=False)\n","    V = f(*X)  # shape (Nx,)*d\n","    flip = False\n","    for ax in reversed(range(d)):\n","        x = x_arrs[ax]; s = s_arrs[ax]\n","        V = np.moveaxis(V, ax, 0)\n","        L = len(x)\n","        rest_shape = V.shape[1:]\n","        M = int(np.prod(rest_shape, dtype=np.int64)) if rest_shape else 1\n","        V2 = V.reshape(L, M)\n","        out2 = np.empty((len(s), M), dtype=float)\n","        hx = np.empty(L, dtype=float)\n","        hu = np.empty(L, dtype=float)\n","        edge = np.empty(L+1, dtype=float)\n","        wout = np.empty(len(s), dtype=float)\n","        if progress:\n","            print(f\"[Lucet] axis={ax} lines={M:,} L={L} Ns={len(s)} flip={flip}\")\n","        for j in range(M):\n","            uline = V2[:, j]\n","            _llt_1d_inplace_signed(x, uline, s, hx, hu, edge, wout, negate_u=flip)\n","            out2[:, j] = wout\n","        V = out2.reshape((len(s),) + rest_shape)\n","        V = np.moveaxis(V, 0, ax)\n","        flip = not flip\n","    return V\n","\n","def lucet_nd(x_arrs,f,s_arrs):\n","    # reference (unused – fast version is called)\n","    V=f(*np.meshgrid(*x_arrs,indexing='ij',sparse=False))\n","    flip=False\n","    for axis in reversed(range(len(x_arrs))):\n","        x,s=x_arrs[axis],s_arrs[axis]\n","        V=np.moveaxis(V,axis,0)\n","        out=np.empty((len(s),)+V.shape[1:],float)\n","        it=np.nditer(V[0],flags=['multi_index'])\n","        while not it.finished:\n","            idx=it.multi_index\n","            line=V[(slice(None),)+idx]\n","            out[(slice(None),)+idx]=llt_1d(x,-line if flip else line,s)\n","            it.iternext()\n","        V=np.moveaxis(out,0,axis); flip=True\n","    return V\n","\n","def direct_nd(x_arrs,f,s_arrs):\n","    X=np.meshgrid(*x_arrs,indexing='ij',sparse=False)\n","    U=f(*X)\n","    out=np.empty(tuple(len(sa) for sa in s_arrs))\n","    it=np.nditer(out,flags=['multi_index'],op_flags=['writeonly'])\n","    while not it.finished:\n","        slopes=[s_arrs[k][it.multi_index[k]] for k in range(len(x_arrs))]\n","        it[0]=np.max(sum(s*Xk for s,Xk in zip(slopes,X))-U)\n","        it.iternext()\n","    return out\n","\n","def interp_nd(grid, axes, pt):\n","    idx_low, t = [], []\n","    for p,ax in zip(pt,axes):\n","        j=np.searchsorted(ax,p)-1\n","        j=np.clip(j,0,len(ax)-2)\n","        idx_low.append(j)\n","        t.append((p-ax[j])/(ax[j+1]-ax[j]))\n","    val=0.0\n","    for corners in product((0,1), repeat=len(pt)):\n","        w, idx = 1.0, []\n","        for c,tl,jl in zip(corners,t,idx_low):\n","            w*=tl if c else 1-tl\n","            idx.append(jl+c)\n","        val+=w*grid[tuple(idx)]\n","    return val\n","\n","def _prod_len(arrs): return int(np.prod([len(a) for a in arrs], dtype=int))\n","def lucet_mem_mb_est(x_arrs, s_arrs):\n","    prod_x = _prod_len(x_arrs); prod_s = _prod_len(s_arrs)\n","    peak = 2 * max(prod_x, prod_s) * 8.0\n","    return peak/(1024**2)\n","def direct_mem_mb_est(x_arrs, s_arrs, d):\n","    prod_x = _prod_len(x_arrs); prod_s = _prod_len(s_arrs)\n","    bytes_total = ((d + 2) * prod_x + prod_s) * 8.0\n","    return bytes_total/(1024**2)\n","def feasible_lucet(x_arrs, s_arrs, mem_gb_cap, flop_cap):\n","    mem_mb = lucet_mem_mb_est(x_arrs, s_arrs)\n","    mem_ok = (mem_mb/1024.0) <= mem_gb_cap\n","    d=len(x_arrs); N=max(max(len(a) for a in x_arrs), max(len(a) for a in s_arrs))\n","    flops = float(d) * float(N**d) * 20.0\n","    return (mem_ok and flops <= flop_cap), mem_mb, flops\n","def feasible_direct(x_arrs, s_arrs, mem_gb_cap, flop_cap, d):\n","    prod_x = _prod_len(x_arrs); prod_s = _prod_len(s_arrs)\n","    mem_mb = direct_mem_mb_est(x_arrs, s_arrs, d); mem_ok = (mem_mb/1024.0) <= mem_gb_cap\n","    flops  = float(prod_x) * float(prod_s)\n","    return (mem_ok and flops <= flop_cap), mem_mb, flops\n","\n","# ========================= 4) Build grids & shared Y_eval =====================\n","def make_samplers(domain, fn_key, x_loguniform: bool):\n","    \"\"\"\n","    Optional log-uniform x sampling for positive-domain functions.\n","    - If fn_key in {neg_log, neg_entropy} and x_loguniform is True:\n","        x ~ log-uniform on [C_lo, C_hi]  (so y=g(x) is more uniform on D)\n","    - Else:\n","        x ~ uniform on [C_lo, C_hi]\n","    Slopes y=s are always sampled uniform on D for classical eval.\n","    \"\"\"\n","    (x_lo,x_hi) = domain[fn_key][\"C\"]\n","    (s_lo,s_hi) = domain[fn_key][\"D\"]\n","\n","    if fn_key in (\"neg_log\", \"neg_entropy\") and x_loguniform:\n","        if not (x_lo > 0 and x_hi > 0):\n","            raise ValueError(f\"{fn_key} requires x>0, got C=({x_lo}, {x_hi})\")\n","        log_x_lo = float(math.log(x_lo))\n","        log_x_hi = float(math.log(x_hi))\n","        def samp_x(k, sh):\n","            return jnp.exp(\n","                random.uniform(k, sh, minval=log_x_lo, maxval=log_x_hi, dtype=jnp.float32)\n","            )\n","    else:\n","        def samp_x(k, sh):\n","            return random.uniform(k, sh, minval=float(x_lo), maxval=float(x_hi), dtype=jnp.float32)\n","\n","    def samp_y(k, sh):\n","        return random.uniform(k, sh, minval=float(s_lo), maxval=float(s_hi), dtype=jnp.float32)\n","\n","    return samp_x, samp_y\n","\n","def build_grids_and_eval(fn_key, d, Nx, Ns, n_eval, domain):\n","    (x_lo,x_hi)=domain[fn_key][\"C\"]; (s_lo,s_hi)=domain[fn_key][\"D\"]\n","    x_axes=[np.linspace(x_lo, x_hi, Nx)]*d\n","    s_axes=[np.linspace(s_lo, s_hi, Ns)]*d\n","    eps = 1e-6\n","    lo = s_lo + eps*(s_hi - s_lo)\n","    hi = s_hi - eps*(s_hi - s_lo)\n","    Y_eval = np.array(\n","        random.uniform(random.PRNGKey(1234+d), (n_eval, d),\n","                       minval=lo, maxval=hi, dtype=jnp.float32)\n","    )\n","    def f_np(*Xs):\n","        V=np.stack(Xs, axis=-1)\n","        if fn_key==\"quadratic\":       return 0.5*np.sum(V*V, axis=-1)\n","        if fn_key==\"neg_log\":         return -np.sum(np.log(np.maximum(V,1e-10)), axis=-1)\n","        if fn_key==\"neg_entropy\":\n","            Xs_=np.maximum(V,1e-10)\n","            return np.sum(Xs_*np.log(Xs_), axis=-1)\n","        raise ValueError\n","    def fst_np(Y):\n","        if fn_key==\"quadratic\":   return 0.5*np.sum(Y*Y, axis=-1)\n","        if fn_key==\"neg_log\":     return -np.sum(np.log(-Y), axis=-1) - d\n","        if fn_key==\"neg_entropy\": return np.sum(np.exp(Y-1.0), axis=-1)\n","        raise ValueError\n","    return x_axes, s_axes, Y_eval, f_np, fst_np\n","\n","# ========================= 5) Metrics helpers =================================\n","def relative_l2(pred: np.ndarray, true: np.ndarray) -> float:\n","    denom = float(np.linalg.norm(true))\n","    if denom == 0.0: return float(\"nan\")\n","    return float(np.linalg.norm(pred - true) / denom)\n","\n","# ========================= 6) One (fn, d, Ns, arch) run =======================\n","def dlt_model_factory(model_name: str, hidden: Tuple[int, ...], act_name: str):\n","    act = _act(act_name)\n","    name = model_name.upper()\n","    if name == \"ICNN\": return lambda: ICNN(hidden=hidden, act=act)\n","    if name == \"MLP\":  return lambda: MLP(hidden=hidden, act=act)\n","    return lambda: ResNet(hidden=hidden, act=act)  # default: RESNET\n","\n","def run_case_classical(fn_key, d, Nx, Ns, args, Y_eval, f_np, fst_np, domain):\n","    rows=[]\n","    (x_lo,x_hi)=domain[fn_key][\"C\"]; (s_lo,s_hi)=domain[fn_key][\"D\"]\n","    x_axes=[np.linspace(x_lo, x_hi, Nx)]*d\n","    s_axes=[np.linspace(s_lo, s_hi, Ns)]*d\n","\n","    lo_ok = np.all(Y_eval >= np.array([ax[0] for ax in s_axes]), axis=1)\n","    hi_ok = np.all(Y_eval <= np.array([ax[-1] for ax in s_axes]), axis=1)\n","    mask  = np.logical_and(lo_ok, hi_ok)\n","    true  = fst_np(Y_eval[mask]) if np.any(mask) else None\n","\n","    # ----- Lucet (fast) -----\n","    t_luc = None; mb_luc = None; rel_l2_l=None; rmse_l=None; maxe_l=None; t_eval_l=None\n","    if d <= 12:\n","        feas_L, mem_mb_L, _ = feasible_lucet(x_axes, s_axes, args.mem_gb_cap, args.flop_cap)\n","        if feas_L:\n","            t0=time.perf_counter()\n","            U_luc = lucet_nd_fast(x_axes, f_np, s_axes, progress=args.lucet_progress)\n","            t_luc = time.perf_counter() - t0\n","            if np.any(mask):\n","                t0=time.perf_counter()\n","                luc_vals = np.array([interp_nd(U_luc, s_axes, y) for y in Y_eval[mask]])\n","                t_eval_l = time.perf_counter() - t0\n","                diff = luc_vals - true\n","                maxe_l   = float(np.max(np.abs(diff)))\n","                rmse_l   = float(np.sqrt(np.mean(diff**2)))\n","                rel_l2_l = relative_l2(luc_vals, true)\n","            mb_luc = mem_mb_L\n","            rows.append(dict(\n","                Function=FUNS[fn_key][\"printable\"], fn_key=fn_key, d=d,\n","                method=\"Lucet\", model=\"\", hidden=\"\",\n","                Nx=Nx, Ns=Ns, N_train=None, batch=None, steps=None, patience=None,\n","                t_solve=t_luc, t_eval=t_eval_l, mem_MB=mb_luc,\n","                max_err=maxe_l, rmse=rmse_l, relL2=rel_l2_l\n","            ))\n","\n","    # ----- Direct -----\n","    t_def=None; mb_def=None; rel_l2_d=None; rmse_d=None; maxe_d=None; t_eval_d=None\n","    if d <= args.def_dim_max:\n","        feas_D, mem_mb_D, _ = feasible_direct(x_axes, s_axes, args.mem_gb_cap_def, args.flop_cap_def, d)\n","        if feas_D:\n","            t0=time.perf_counter()\n","            U_def = direct_nd(x_axes, f_np, s_axes)\n","            t_def = time.perf_counter() - t0\n","            if np.any(mask):\n","                t0=time.perf_counter()\n","                def_vals = np.array([interp_nd(U_def, s_axes, y) for y in Y_eval[mask]])\n","                t_eval_d = time.perf_counter() - t0\n","                diff = def_vals - true\n","                maxe_d   = float(np.max(np.abs(diff)))\n","                rmse_d   = float(np.sqrt(np.mean(diff**2)))\n","                rel_l2_d = relative_l2(def_vals, true)\n","            mb_def = mem_mb_D\n","            rows.append(dict(\n","                Function=FUNS[fn_key][\"printable\"], fn_key=fn_key, d=d,\n","                method=\"Direct\", model=\"\", hidden=\"\",\n","                Nx=Nx, Ns=Ns, N_train=None, batch=None, steps=None, patience=None,\n","                t_solve=t_def, t_eval=t_eval_d, mem_MB=mb_def,\n","                max_err=maxe_d, rmse=rmse_d, relL2=rel_l2_d\n","            ))\n","    return rows\n","\n","def run_case_dlt(fn_key, d, arch_name, hidden, act_name, args, Y_eval, f, g, fst, samp_x):\n","    steps = auto_steps_from_dim(d) if args.steps == \"auto\" else int(args.steps)\n","    N_train, batch = auto_samples_and_batch(d, args.k_train) if args.batch == \"auto\" else (max(d, args.k_train*d), int(args.batch))\n","    patience = auto_patience_for_steps(steps, d) if args.patience == \"auto\" else int(args.patience)\n","\n","    maker = dlt_model_factory(arch_name, hidden, args.act)\n","    params, t_dlt, bs = train_dlt_dataset(\n","        maker, d, f, g, samp_x, steps=steps, lr=args.lr, patience=patience,\n","        seed=7000+d*17+hash(arch_name)%1000, N=N_train, bs=batch\n","    )\n","    mb_dlt = estimate_dlt_mem_mb(params, d, bs, arch_name, hidden)\n","\n","    t_eval=None; maxe=None; rmse=None; rel=None\n","    if fn_key in EVAL_DLT_FUNS:\n","        yj = jnp.asarray(Y_eval, jnp.float32)\n","        t0=time.perf_counter()\n","        pred = np.array(maker().apply({\"params\":params}, yj))\n","        t_eval = time.perf_counter() - t0\n","        true   = np.array(fst(Y_eval))\n","        diff   = pred - true\n","        maxe   = float(np.max(np.abs(diff)))\n","        rmse   = float(np.sqrt(np.mean(diff**2)))\n","        rel    = relative_l2(pred, true)\n","\n","    return dict(\n","        Function=FUNS[fn_key][\"printable\"], fn_key=fn_key, d=d,\n","        method=\"DLT\", model=arch_name.upper(), hidden=\",\".join(map(str,hidden)),\n","        Nx=None, Ns=None, N_train=N_train, batch=bs, steps=steps, patience=patience,\n","        t_solve=t_dlt, t_eval=t_eval, mem_MB=mb_dlt,\n","        max_err=maxe, rmse=rmse, relL2=rel\n","    )\n","\n","# ========================= 7) CLI & orchestration =============================\n","def parse_dim_map(tokens, default_map) -> Dict[int,int]:\n","    m = dict(default_map)\n","    for tok in tokens or []:\n","        d,v = tok.split(\":\"); m[int(d)] = int(v)\n","    return m\n","def parse_ns_sweep(tokens: List[str], default_map: Dict[int, List[int]]) -> Dict[int, List[int]]:\n","    m = {k:list(v) for k,v in default_map.items()}\n","    for tok in tokens or []:\n","        d, vs = tok.split(\":\")\n","        m[int(d)] = [int(x) for x in vs.split(\",\") if x]\n","    return m\n","def parse_arch_list(tokens: List[str], default: List[Tuple[str,Tuple[int,...]]]) -> List[Tuple[str,Tuple[int,...]]]:\n","    if not tokens: return default\n","    out=[]\n","    for t in tokens:\n","        name, widths = t.split(\":\")\n","        out.append( (name.upper(), parse_hidden(widths)) )\n","    return out\n","\n","def build_parser():\n","    P = argparse.ArgumentParser(add_help=True)\n","    P.add_argument(\"--dims\", nargs=\"+\", type=int, default=[2,3,4,5,6,8,10,20,50],\n","                   help=\"Run both up to 12; DLT-only for larger dims.\")\n","    P.add_argument(\"--n_eval\", type=int, default=5000, help=\"size of shared Y_eval ⊂ D\")\n","    P.add_argument(\"--domain_profile\", choices=[\"paper\",\"wide\"], default=\"paper\")\n","\n","    # Classical feasibility gates\n","    P.add_argument(\"--mem_gb_cap\", type=float, default=3.0, help=\"peak GiB cap for Lucet\")\n","    P.add_argument(\"--flop_cap\",   type=float, default=3e10, help=\"flop cap proxy for Lucet\")\n","    # Direct feasibility gates\n","    P.add_argument(\"--mem_gb_cap_def\", type=float, default=1.0, help=\"peak GiB cap for Direct\")\n","    P.add_argument(\"--flop_cap_def\",   type=float, default=1e12, help=\"flop cap proxy for Direct\")\n","    P.add_argument(\"--def_dim_max\", type=int, default=6, help=\"Attempt Direct only up to this dimension\")\n","\n","    # DLT training (auto by default)\n","    P.add_argument(\"--act\", default=\"softplus\", choices=[\"relu\",\"gelu\",\"softplus\"])\n","    P.add_argument(\"--arch\", nargs=\"*\", default=[], help='DLT architectures, e.g. \"RESNET:128,128\" \"ICNN:128,128\"')\n","    P.add_argument(\"--steps\", default=\"auto\", help='int or \"auto\"')\n","    P.add_argument(\"--k_train\", type=int, default=300, help=\"N ≈ k * d, clamped to [100,1000]\")\n","    P.add_argument(\"--batch\", default=\"auto\", help='int or \"auto\" (min 16, max 64, ≈ N/10)')\n","    P.add_argument(\"--patience\", default=\"auto\", help='int or \"auto\" (early stopping)')\n","    P.add_argument(\"--lr\", type=float, default=1e-3)\n","\n","    # Grid overrides / sweeps\n","    P.add_argument(\"--nx\", nargs=\"*\", default=[], help='override x-grid per d, e.g. \"2:10\" \"5:10\"')\n","    P.add_argument(\"--ns\", nargs=\"*\", default=[], help='override slope-grid per d, e.g. \"2:10\" \"5:10\"')\n","    P.add_argument(\"--ns_sweep\", nargs=\"*\", default=[], help='sweep Ns per d, e.g. \"2:9,11\" \"5:9,11\"')\n","\n","    # Lucet fast progress\n","    P.add_argument(\"--lucet_progress\", action=\"store_true\", help=\"print per-axis progress for Lucet\")\n","\n","    # Optional: log-uniform x sampling for positive-domain functions\n","    P.add_argument(\"--x_loguniform\", dest=\"x_loguniform\", action=\"store_true\", default=True,\n","                   help=\"Use log-uniform x for neg_log and neg_entropy (default: on)\")\n","    P.add_argument(\"--no_x_loguniform\", dest=\"x_loguniform\", action=\"store_false\",\n","                   help=\"Disable log-uniform x; use uniform x on [C_lo, C_hi]\")\n","\n","    # Output\n","    P.add_argument(\"--csv\", default=\"results_dlt_vs_classical.csv\")\n","    P.add_argument(\"--outdir\", default=\"figs\")\n","    return P\n","\n","def default_grids():\n","    NX_default = {d:10 for d in [2,3,4,5,6,8,10,12,20,50,200]}\n","    NS_default = {d:10 for d in [2,3,4,5,6,8,10,12,20,50,200]}\n","    NS_SWEEP_DEFAULT = {d:[10] for d in [2,3,4,5,6,8,10,12]}\n","    return NX_default, NS_default, NS_SWEEP_DEFAULT\n","\n","# ========================= 8) Plotting ========================================\n","def make_plots(df: pd.DataFrame, outdir: str):\n","    os.makedirs(outdir, exist_ok=True)\n","    if df.empty: return\n","    for (fn_key, d), g in df.groupby([\"fn_key\",\"d\"]):\n","        g = g.dropna(subset=[\"relL2\",\"t_solve\"])\n","        if g.empty: continue\n","\n","        plt.figure()\n","        for meth, gm in g.groupby(\"method\"):\n","            plt.scatter(gm[\"t_solve\"], gm[\"relL2\"], label=meth)\n","        plt.xscale(\"log\"); plt.yscale(\"log\")\n","        plt.xlabel(\"Solve time (s)\"); plt.ylabel(\"Relative L2 error\")\n","        title = f\"{fn_key} (d={d}) — relL2 vs time\"\n","        plt.title(title); plt.legend()\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(outdir, f\"{fn_key}_d{d}_relL2_vs_time.png\"), dpi=150)\n","        plt.close()\n","\n","        g_mem = g.dropna(subset=[\"mem_MB\"])\n","        if not g_mem.empty:\n","            plt.figure()\n","            for meth, gm in g_mem.groupby(\"method\"):\n","                plt.scatter(gm[\"mem_MB\"], gm[\"relL2\"], label=meth)\n","            plt.xscale(\"log\"); plt.yscale(\"log\")\n","            plt.xlabel(\"Active memory (MiB)\"); plt.ylabel(\"Relative L2 error\")\n","            plt.title(f\"{fn_key} (d={d}) — relL2 vs memory\")\n","            plt.legend(); plt.tight_layout()\n","            plt.savefig(os.path.join(outdir, f\"{fn_key}_d{d}_relL2_vs_mem.png\"), dpi=150)\n","            plt.close()\n","\n","# ========================= 9) Main ============================================\n","def main(argv=None):\n","    parser = build_parser()\n","    if argv is None: argv = sys.argv[1:]\n","    args, _ = parser.parse_known_args(argv)\n","\n","    domain = DOMAINS_PAPER if args.domain_profile == \"paper\" else DOMAINS_WIDE\n","    NX_default, NS_default, NS_SWEEP_DEFAULT = default_grids()\n","    NX_map = parse_dim_map(args.nx, NX_default)\n","    NS_map = parse_dim_map(args.ns, NS_default)\n","    NS_sweep_map = parse_ns_sweep(args.ns_sweep, NS_SWEEP_DEFAULT)\n","    archs = parse_arch_list(args.arch, default=[(\"RESNET\", (128,128))])\n","\n","    rows = []\n","    hdr = (\n","        f\"{'Function':<12} {'d':>4} | \"\n","        f\"{'method':<8} | {'model':<8} | {'hidden':<12} | \"\n","        f\"{'Nx':>4} | {'Ns':>4} | {'Ntr':>6} | {'B':>4} | {'steps':>7} | \"\n","        f\"{'tSolve(s)':>10} | {'tEval(s)':>9} | {'MB(act)':>9} | \"\n","        f\"{'max_err':>10} | {'RMSE':>10} | {'relL2':>10}\"\n","    )\n","    print(hdr); print(\"-\"*len(hdr))\n","\n","    for fn_key in [\"quadratic\",\"neg_log\",\"neg_entropy\"]:\n","        f = FUNS[fn_key][\"f\"]; g = FUNS[fn_key][\"g\"]; fst = FUNS[fn_key][\"fst\"]\n","        samp_x, samp_y = make_samplers(domain, fn_key, args.x_loguniform)\n","\n","        for d in args.dims:\n","            Nx_default = NX_map.get(d, 10)\n","            Ns_default = NS_map.get(d, 10)\n","            _, _, Y_eval, f_np, fst_np = build_grids_and_eval(fn_key, d, Nx_default, Ns_default, args.n_eval, domain)\n","\n","            # ---- Classical sweeps over Ns ----\n","            Ns_list = NS_sweep_map.get(d, [Ns_default])\n","            for Ns in Ns_list:\n","                Nx = Nx_default\n","                try:\n","                    c_rows = run_case_classical(fn_key, d, Nx, Ns, args, Y_eval, f_np, fst_np, domain)\n","                    for r in c_rows:\n","                        rows.append(r)\n","                        print(f\"{r['Function']:<12} {r['d']:>4} | {r['method']:<8} | \"\n","                              f\"{r['model']:<8} | {r['hidden']:<12} | \"\n","                              f\"{str(r['Nx'] or ''):>4} | {str(r['Ns'] or ''):>4} | \"\n","                              f\"{str(r['N_train'] or ''):>6} | {str(r['batch'] or ''):>4} | \"\n","                              f\"{str(r['steps'] or ''):>7} | \"\n","                              f\"{(r['t_solve'] or float('nan')):>10.2f} | {(r['t_eval'] or float('nan')):>9.2f} | \"\n","                              f\"{(r['mem_MB'] or float('nan')):>9.1f} | \"\n","                              f\"{(r['max_err'] or float('nan')):>10.2e} | {(r['rmse'] or float('nan')):>10.2e} | \"\n","                              f\"{(r['relL2'] or float('nan')):>10.2e}\")\n","                except Exception as e:\n","                    print(f\"[skip classical {fn_key} d={d} Ns={Ns}] {e}\")\n","\n","            # ---- DLT architectures ----\n","            for (arch_name, hidden) in archs:\n","                try:\n","                    r = run_case_dlt(fn_key, d, arch_name, hidden, args.act, args, Y_eval, f, g, fst, samp_x)\n","                    rows.append(r)\n","                    print(f\"{r['Function']:<12} {r['d']:>4} | {r['method']:<8} | \"\n","                          f\"{r['model']:<8} | {r['hidden']:<12} | \"\n","                          f\"{str(r['Nx'] or ''):>4} | {str(r['Ns'] or ''):>4} | \"\n","                          f\"{str(r['N_train'] or ''):>6} | {str(r['batch'] or ''):>4} | \"\n","                          f\"{str(r['steps'] or ''):>7} | \"\n","                          f\"{(r['t_solve'] or float('nan')):>10.2f} | {(r['t_eval'] or float('nan')):>9.2f} | \"\n","                          f\"{(r['mem_MB'] or float('nan')):>9.1f} | \"\n","                          f\"{(r['max_err'] or float('nan')):>10.2e} | {(r['rmse'] or float('nan')):>10.2e} | \"\n","                          f\"{(r['relL2'] or float('nan')):>10.2e}\")\n","                except Exception as e:\n","                    print(f\"[skip DLT {arch_name} {fn_key} d={d}] {e}\")\n","\n","    df = pd.DataFrame(rows)\n","    df.to_csv(args.csv, index=False)\n","    print(f\"\\nWrote CSV: {args.csv}  ({len(df)} rows)\")\n","    try:\n","        make_plots(df, args.outdir)\n","        print(f\"Saved figures to: {args.outdir}\")\n","    except Exception as e:\n","        print(f\"[plotting skipped] {e}\")\n","\n","if __name__==\"__main__\":\n","    if \"ipykernel\" in sys.modules or \"google.colab\" in sys.modules:\n","        main([])\n","    else:\n","        main(None)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"rm3wCY47hHdz"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"authorship_tag":"ABX9TyOmHP52aHWZQeWVhADeSwZz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}