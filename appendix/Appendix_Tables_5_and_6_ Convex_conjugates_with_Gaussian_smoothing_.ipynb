{"cells":[{"cell_type":"markdown","metadata":{"id":"tU_cusXJXgwZ"},"source":["\n","\n","**Softmax convex conjugate:**\n","$$\n","f^*_{\\varepsilon}(y) := \\varepsilon \\log \\left(\\int_{\\text{dom} f} \\exp \\left(\\frac{1}{\\varepsilon} \\{\\langle y,x \\rangle - f(x)\\}\\right) dx \\right),\n","$$\n","which converges to $f^*$ as $\\varepsilon \\downarrow 0$. Peyr\\'e (2020) has shown that $f^*_{\\varepsilon}$ can be expressed via Gaussian smoothing as\n","$$\n","f^*_{\\varepsilon}(y) = Q^{-1}_{\\varepsilon} \\left(\\frac{1}{Q_{\\varepsilon}(f) * G_{\\varepsilon}}\\right)(y),\n","$$\n","where\n","$$\n","G_{\\varepsilon}(x) = \\exp \\left(- \\frac{1}{2 \\varepsilon} \\|x\\|_2^2\\right) \\quad \\text{and} \\quad\n","Q_{\\varepsilon}(f) = \\exp \\left(\\frac{1}{2 \\varepsilon} \\|x\\|_2^2 - \\frac{1}{\\varepsilon} f(x)\\right).\n","$$\n","\n","$$\n","Q^{-1}_{\\varepsilon}\\{F\\} := \\frac{1}{2} \\|\\cdot\\|^2 - \\varepsilon \\log(F(\\cdot)).\n","$$"]},{"cell_type":"markdown","metadata":{"id":"jm9TR7gvWW3q"},"source":["MCMC implementation:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixG4GjhYPT2-","outputId":"bc249e95-ef5c-4b80-c78b-eea876717a72"},"outputs":[{"name":"stdout","output_type":"stream","text":["Available columns to display in result tables:\n","1. d\n","2. eps\n","3. sample_size\n","4. time\n","5. memory\n","6. max_err\n","7. max_validation\n","8. mean_err\n","9. rmse\n","10. conv_rate\n","11. valid_percentage\n","\n","Selected columns: d\n","\n","Monte Carlo configuration:\n","1. basic - Standard uniform sampling\n","2. quasi - Quasi-Monte Carlo with Sobol sequences (default)\n","3. importance - Importance sampling based on integrand values\n","4. adaptive - Adaptive refinement of sampling in high-contribution regions\n","\n","Using quasi Monte Carlo with 100000 samples\n","\n","=== Running benchmark for neg_log ===\n","\n","===== BENCHMARK FOR NEG_LOG (MONTE CARLO: QUASI, 100000 SAMPLES) =====\n","\n"," d |    eps |  sample size |  time(s) |     MB |  max err |  max val | mean err |     RMSE | conv rate\n","------------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/scipy/stats/_qmc.py:993: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n","  sample = self._random(n, workers=workers)\n"]},{"name":"stdout","output_type":"stream","text":[" 1 |  0.001 |   MC(100000) |    1.690 | 139.43 | 1.98e-01 | 3.91e-03 | 1.30e-02 | 4.43e-02 |       N/A\n"," 1 |  0.010 |   MC(100000) |    1.714 |   6.13 | 2.17e-01 | 2.76e-02 | 3.14e-02 | 5.33e-02 | 0.8487293673999331\n"," 1 |  0.100 |   MC(100000) |    1.711 |   6.14 | 2.21e-01 | 1.61e-01 | 1.17e-01 | 1.30e-01 | 0.7643840993965804\n"," 1 |  0.500 |   MC(100000) |    1.793 |   6.13 | 6.38e-01 | 4.06e-01 | 3.10e-01 | 3.55e-01 | 0.5767909402722317\n","------------------------------------------------------------------------------------------------------\n"," 2 |  0.001 |   MC(100000) |    3.833 |   9.26 | 3.95e-01 | 7.95e-03 | 2.61e-02 | 6.54e-02 |       N/A\n"," 2 |  0.010 |   MC(100000) |    3.936 |   9.26 | 4.33e-01 | 5.48e-02 | 6.28e-02 | 8.75e-02 | 0.8381800891494996\n"," 2 |  0.100 |   MC(100000) |    3.361 |   9.26 | 4.42e-01 | 3.21e-01 | 2.18e-01 | 2.39e-01 | 0.7680961755329327\n"," 2 |  0.500 |   MC(100000) |    3.365 |   9.29 | 1.28e+00 | 8.12e-01 | 4.33e-01 | 5.20e-01 | 0.5764520846989345\n","------------------------------------------------------------------------------------------------------\n"," 3 |  0.001 |   MC(100000) |   23.794 |  13.16 | 6.03e-01 | 6.70e-02 | 4.60e-02 | 8.73e-02 |       N/A\n"," 3 |  0.010 |   MC(100000) |   29.116 |  12.46 | 6.49e-01 | 1.19e-01 | 9.55e-02 | 1.21e-01 | 0.24771864234775645\n"," 3 |  0.100 |   MC(100000) |   16.801 |  12.46 | 6.64e-01 | 4.86e-01 | 3.23e-01 | 3.47e-01 | 0.6130456114252054\n"," 3 |  0.500 |   MC(100000) |   16.960 |  12.46 | 1.91e+00 | 1.22e+00 | 5.42e-01 | 6.56e-01 | 0.5702467990083941\n","------------------------------------------------------------------------------------------------------\n"," 4 |  0.001 |   MC(100000) |  419.791 |  18.21 | 8.29e-01 | 3.09e-01 | 1.24e-01 | 1.60e-01 |       N/A\n"," 4 |  0.010 |   MC(100000) |  561.630 |  18.66 | 8.65e-01 | 3.79e-01 | 1.65e-01 | 1.93e-01 | 0.08934288220407458\n"," 4 |  0.100 |   MC(100000) |  317.030 |  20.06 | 8.85e-01 | 6.11e-01 | 4.29e-01 | 4.53e-01 | 0.20708567810472508\n"," 4 |  0.500 |   MC(100000) |  317.895 |  20.06 | 2.55e+00 | 1.62e+00 | 6.42e-01 | 7.80e-01 | 0.6065495962197818\n","------------------------------------------------------------------------------------------------------\n"," 5 |  0.001 |   MC(100000) | 9245.286 |  68.33 | 1.65e+00 | 1.05e+00 | 3.24e-01 | 3.87e-01 |       N/A\n"," 5 |  0.010 |   MC(100000) | 12102.669 |  67.22 | 1.89e+00 | 9.67e-01 | 3.56e-01 | 4.14e-01 | -0.03691126338339949\n"," 5 |  0.100 |   MC(100000) | 7426.683 |  67.22 | 1.50e+00 | 1.21e+00 | 5.85e-01 | 6.27e-01 | 0.09751335739209288\n"," 5 |  0.500 |   MC(100000) | 7461.687 |  67.22 | 3.19e+00 | 1.85e+00 | 7.38e-01 | 8.93e-01 | 0.2647701701412446\n","------------------------------------------------------------------------------------------------------\n"]}],"source":["import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from itertools import product\n","import matplotlib.colors as colors\n","import psutil\n","import os\n","import gc\n","import pandas as pd\n","from tabulate import tabulate\n","import tracemalloc\n","from scipy.stats import qmc\n","\n","# ---------- Memory tracking functions ------------------------------\n","def start_memory_tracking():\n","    \"\"\"Start tracking memory allocations\"\"\"\n","    gc.collect()  # Force garbage collection\n","    tracemalloc.start()\n","    return tracemalloc.get_traced_memory()[0] / (1024 * 1024)  # Current memory in MB\n","\n","def get_peak_memory():\n","    \"\"\"Get peak memory usage in MB since tracking started\"\"\"\n","    current, peak = tracemalloc.get_traced_memory()\n","    return peak / (1024 * 1024)  # Peak memory in MB\n","\n","def stop_memory_tracking():\n","    \"\"\"Stop tracking memory allocations\"\"\"\n","    current, peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    return peak / (1024 * 1024)  # Peak memory in MB\n","\n","def free_memory():\n","    \"\"\"Aggressively free memory\"\"\"\n","    gc.collect()  # Collect garbage\n","    if hasattr(plt, 'close'):\n","        plt.close('all')  # Close all matplotlib figures\n","\n","    # Try to clear large variables from memory\n","    for name in list(globals().keys()):\n","        if name.startswith('__'):\n","            continue\n","        obj = globals()[name]\n","        if isinstance(obj, np.ndarray) and obj.size > 1000000:\n","            del globals()[name]\n","\n","    gc.collect()  # Collect garbage again after deleting variables\n","\n","# ---------- Test functions -----------------------------------------\n","def neg_log(d):\n","    \"\"\"d-dimensional negative logarithm function: u(x) = -sum(log(x_i))\n","    Domain: x_i > 0\n","    LF transform: u*(s) = -d - sum(log(-s_i)) for s_i < 0\n","    \"\"\"\n","    def func(*args):\n","        # Handle domain constraints - inputs should be positive\n","        result = 0\n","        for x in args:\n","            x_safe = np.maximum(x, 1e-10)  # Avoid log(0)\n","            result -= np.log(x_safe)\n","        return result\n","    return func\n","\n","def neg_entropy(d):\n","    \"\"\"d-dimensional negative entropy function: u(x) = sum(x_i * log(x_i))\n","    Domain: x_i > 0\n","    LF transform: u*(s) = sum(exp(s_i - 1))\n","    \"\"\"\n","    def func(*args):\n","        result = 0\n","        for x in args:\n","            # Handle domain constraints and avoid singularities\n","            x_safe = np.maximum(x, 1e-10)\n","            x_log_x = x_safe * np.log(x_safe)\n","            # Set values to 0 for x close to 0 (lim x→0 x log(x) = 0)\n","            x_log_x = np.where(x_safe < 1e-8, 0, x_log_x)\n","            result += x_log_x\n","        return result\n","    return func\n","\n","# ---------- Analytical LF transforms for validation ----------------\n","def neg_log_transform_analytical(s_arrs):\n","    \"\"\"\n","    Analytical LF transform for negative log function:\n","    f*(s) = -d - sum(log(-s_i)) for s_i < 0\n","    \"\"\"\n","    d = len(s_arrs)\n","    S = np.meshgrid(*s_arrs, indexing='ij', sparse=False)\n","\n","    # Initialize with -d term\n","    result = np.full(tuple(len(s) for s in s_arrs), -d, dtype=float)\n","\n","    # Add log terms for each dimension\n","    for i in range(d):\n","        # Apply only where s < 0 (domain constraint)\n","        valid_mask = S[i] < 0\n","        # Set large negative values for invalid regions\n","        log_term = np.log(-S[i])\n","        result = np.where(valid_mask, result - log_term, -np.inf)\n","\n","    return result\n","\n","def neg_entropy_transform_analytical(s_arrs):\n","    \"\"\"\n","    Analytical LF transform for negative entropy function:\n","    f*(s) = sum(exp(s_i - 1))\n","    \"\"\"\n","    d = len(s_arrs)\n","    S = np.meshgrid(*s_arrs, indexing='ij', sparse=False)\n","\n","    # Initialize with zeros\n","    result = np.zeros(tuple(len(s) for s in s_arrs), dtype=float)\n","\n","    # Add exp terms for each dimension\n","    for i in range(d):\n","        result += np.exp(S[i] - 1)\n","\n","    return result\n","\n","# ---------- Monte Carlo LF Transform Methods ----------------------\n","def lf_transform_monte_carlo(x_ranges, f, s_arrs, eps=0.1, n_samples=100000, method='quasi'):\n","    \"\"\"\n","    Monte Carlo approximation of Legendre-Fenchel transform.\n","\n","    Parameters:\n","    x_ranges : list of tuples\n","        Range for each dimension of x (min, max)\n","    f : function\n","        The function to transform\n","    s_arrs : list of arrays\n","        Grid points in dual space (one array per dimension)\n","    eps : float\n","        Smoothing parameter\n","    n_samples : int\n","        Number of Monte Carlo samples\n","    method : str\n","        Monte Carlo method to use: 'basic', 'quasi', 'importance', or 'adaptive'\n","\n","    Returns:\n","    numpy.ndarray\n","        The approximate Legendre-Fenchel transform f*_eps(s)\n","    \"\"\"\n","    # Get dimension\n","    d = len(x_ranges)\n","\n","    # Create output array for the transform\n","    result = np.empty([len(s) for s in s_arrs])\n","\n","    # Create separate error estimates array if needed\n","    error_estimates = None\n","    if method in ['basic', 'quasi']:\n","        error_estimates = np.empty_like(result)\n","\n","    # Calculate domain volume\n","    domain_volume = np.prod([x_max - x_min for x_min, x_max in x_ranges])\n","\n","    # Generate samples based on the selected method\n","    if method == 'basic':\n","        # Simple uniform random sampling\n","        samples = np.array([np.random.uniform(x_min, x_max, n_samples)\n","                           for x_min, x_max in x_ranges]).T\n","\n","    elif method == 'quasi':\n","        # Quasi-Monte Carlo with Sobol sequences for better coverage\n","        # Initialize Sobol sequence generator\n","        sampler = qmc.Sobol(d=d, scramble=True)\n","\n","        # Generate samples in [0, 1]\n","        unit_samples = sampler.random(n_samples)\n","\n","        # Scale to the domain range\n","        samples = np.zeros((n_samples, d))\n","        for i in range(d):\n","            x_min, x_max = x_ranges[i]\n","            samples[:, i] = unit_samples[:, i] * (x_max - x_min) + x_min\n","\n","    elif method == 'importance' or method == 'adaptive':\n","        # For importance sampling, we need an initial set of samples to identify high-contribution regions\n","        # Start with uniform samples\n","        initial_samples = np.array([np.random.uniform(x_min, x_max, min(10000, n_samples // 10))\n","                                   for x_min, x_max in x_ranges]).T\n","\n","        # Pre-compute function values\n","        f_values_initial = np.array([f(*x) for x in initial_samples])\n","\n","        # For each output point, we'll generate specialized importance samples\n","        # This makes this method more computationally intensive but potentially more accurate\n","        samples_per_point = max(1000, n_samples // (np.prod([len(s) for s in s_arrs])))\n","\n","    else:\n","        raise ValueError(f\"Unknown Monte Carlo method: {method}\")\n","\n","    # For basic and quasi-Monte Carlo, pre-compute function values for all samples\n","    if method in ['basic', 'quasi']:\n","        f_values = np.array([f(*x) for x in samples])\n","\n","    # Iterate through the output grid\n","    it = np.nditer(result, flags=['multi_index'], op_flags=['writeonly'])\n","    while not it.finished:\n","        # Get current slope point\n","        s_point = [s_arrs[i][it.multi_index[i]] for i in range(d)]\n","\n","        if method in ['basic', 'quasi']:\n","            # Compute inner products for all samples\n","            inner_products = np.sum([s_point[i] * samples[:, i] for i in range(d)], axis=0)\n","\n","            # Compute exponents\n","            exponents = (inner_products - f_values) / eps\n","\n","            # Shift to prevent overflow\n","            max_exp = np.max(exponents)\n","            exponents -= max_exp\n","\n","            # Compute Monte Carlo approximation\n","            mc_sum = np.sum(np.exp(exponents))\n","            mc_avg = mc_sum / n_samples\n","\n","            # Apply scaling and logarithm\n","            it[0] = eps * (np.log(domain_volume * mc_avg) + max_exp)\n","\n","            # Calculate error estimate (for standard Monte Carlo)\n","            # Standard error = sigma / sqrt(N)\n","            if error_estimates is not None and method == 'basic':\n","                variance = np.var(np.exp(exponents)) * (domain_volume**2)\n","                std_error = np.sqrt(variance / n_samples)\n","                error_estimates[it.multi_index] = eps * std_error / (domain_volume * mc_avg)\n","\n","\n","        elif method == 'importance':\n","            # For importance sampling, we want to sample more heavily in regions where the integrand is large\n","            # Compute integrand values for initial samples for the current s_point\n","            inner_products_init = np.sum([s_point[i] * initial_samples[:, i] for i in range(d)], axis=0)\n","            integrand_values = np.exp((inner_products_init - f_values_initial) / eps)\n","\n","            # Normalize to create a probability distribution\n","            if np.sum(integrand_values) > 0:\n","                prob_dist = integrand_values / np.sum(integrand_values)\n","            else:\n","                # If all values are zero (underflow), use uniform distribution\n","                prob_dist = np.ones_like(integrand_values) / len(integrand_values)\n","\n","            # Generate importance samples for this specific s_point\n","            importance_indices = np.random.choice(\n","                range(len(initial_samples)),\n","                size=samples_per_point,\n","                replace=True,\n","                p=prob_dist\n","            )\n","            importance_samples = initial_samples[importance_indices]\n","\n","            # Compute function values for importance samples\n","            f_values_importance = f_values_initial[importance_indices]\n","\n","            # Compute integrand with importance sampling correction\n","            inner_products = np.sum([s_point[i] * importance_samples[:, i] for i in range(d)], axis=0)\n","            exponents = (inner_products - f_values_importance) / eps\n","\n","            # Apply importance sampling correction: divide by probability used for sampling\n","            importance_weights = 1.0 / (prob_dist[importance_indices] * len(prob_dist))\n","\n","            # Avoid overflow\n","            max_exp = np.max(exponents)\n","            exponents -= max_exp\n","            weighted_sum = np.sum(np.exp(exponents) * importance_weights)\n","\n","            # Compute final result\n","            it[0] = eps * (np.log(domain_volume * weighted_sum / samples_per_point) + max_exp)\n","\n","        elif method == 'adaptive':\n","            # Similar to importance sampling but with adaptive refinement\n","            # First pass with initial samples\n","            inner_products_init = np.sum([s_point[i] * initial_samples[:, i] for i in range(d)], axis=0)\n","            integrand_values = np.exp((inner_products_init - f_values_initial) / eps)\n","\n","            # Identify regions needing refinement (high integrand value or high variance)\n","            if len(integrand_values) > 0:\n","                threshold = np.percentile(integrand_values, 95)  # Focus on top 5%\n","                high_contribution_indices = np.where(integrand_values > threshold)[0]\n","\n","                if len(high_contribution_indices) > 0:\n","                    # Extract high-contribution samples\n","                    high_samples = initial_samples[high_contribution_indices]\n","\n","                    # Define a refined sampling region around these samples\n","                    refined_ranges = []\n","                    for dim in range(d):\n","                        min_val = max(x_ranges[dim][0], np.min(high_samples[:, dim]) - 0.1 * (x_ranges[dim][1] - x_ranges[dim][0]))\n","                        max_val = min(x_ranges[dim][1], np.max(high_samples[:, dim]) + 0.1 * (x_ranges[dim][1] - x_ranges[dim][0]))\n","                        refined_ranges.append((min_val, max_val))\n","\n","                    # Generate refined samples\n","                    refined_samples = np.array([np.random.uniform(r_min, r_max, samples_per_point)\n","                                              for r_min, r_max in refined_ranges]).T\n","\n","                    # Compute function values for refined samples\n","                    f_values_refined = np.array([f(*x) for x in refined_samples])\n","\n","                    # Compute integrand for refined samples\n","                    inner_products_refined = np.sum([s_point[i] * refined_samples[:, i] for i in range(d)], axis=0)\n","                    exponents_refined = (inner_products_refined - f_values_refined) / eps\n","\n","                    # Calculate refined volume\n","                    refined_volume = np.prod([r_max - r_min for r_min, r_max in refined_ranges])\n","                    ratio = refined_volume / domain_volume\n","\n","                    # Combine with initial estimate (weighted by volume ratio)\n","                    max_exp_init = np.max(inner_products_init - f_values_initial) / eps if len(inner_products_init) > 0 else 0\n","                    init_sum = np.sum(np.exp(((inner_products_init - f_values_initial) / eps) - max_exp_init))\n","\n","                    max_exp_refined = np.max(exponents_refined) if len(exponents_refined) > 0 else 0\n","                    refined_sum = np.sum(np.exp(exponents_refined - max_exp_refined))\n","\n","                    # Combine estimates with appropriate volume scaling\n","                    max_exp_combined = max(max_exp_init, max_exp_refined)\n","                    init_contribution = init_sum * np.exp(max_exp_init - max_exp_combined) * (1 - ratio)\n","                    refined_contribution = refined_sum * np.exp(max_exp_refined - max_exp_combined) * ratio\n","\n","                    combined_sum = init_contribution + refined_contribution\n","                    it[0] = eps * (np.log(domain_volume * combined_sum / samples_per_point) + max_exp_combined)\n","                else:\n","                    # Fall back to basic MC if no high-contribution regions found\n","                    max_exp = np.max((inner_products_init - f_values_initial) / eps)\n","                    mc_sum = np.sum(np.exp(((inner_products_init - f_values_initial) / eps) - max_exp))\n","                    mc_avg = mc_sum / len(initial_samples)\n","                    it[0] = eps * (np.log(domain_volume * mc_avg) + max_exp)\n","            else:\n","                # No valid samples, fall back to analytical approximation if possible\n","                it[0] = 0  # Default value\n","\n","        it.iternext()\n","\n","    # Return both result and error estimates if available\n","    if error_estimates is not None:\n","        return result, error_estimates\n","    return result\n","\n","# ---------- Error calculation functions ---------------------\n","def calculate_errors(U_mc, U_ana, validation_set=None):\n","    \"\"\"\n","    Calculate various error metrics between Monte Carlo approximation and analytical solution.\n","\n","    Parameters:\n","    U_mc : numpy.ndarray\n","        Monte Carlo approximation\n","    U_ana : numpy.ndarray\n","        Analytical solution\n","    validation_set : tuple, optional\n","        Tuple of slice objects defining a validation set region\n","\n","    Returns:\n","    dict\n","        Dictionary of error metrics\n","    \"\"\"\n","    # Get valid points (exclude -inf values)\n","    valid_mask = ~np.isinf(U_ana) & ~np.isinf(U_mc) & ~np.isnan(U_ana) & ~np.isnan(U_mc)\n","\n","    if np.sum(valid_mask) == 0:\n","        return {\n","            'max': np.nan,\n","            'mean': np.nan,\n","            'median': np.nan,\n","            'max_validation': np.nan,\n","            'rmse': np.nan,\n","            'valid_percentage': 0\n","        }\n","\n","    # Extract valid points\n","    U_mc_valid = U_mc[valid_mask]\n","    U_ana_valid = U_ana[valid_mask]\n","\n","    # Calculate absolute errors\n","    abs_errors = np.abs(U_mc_valid - U_ana_valid)\n","\n","    # Calculate max error over validation set if provided\n","    max_validation_error = np.nan\n","    if validation_set is not None:\n","        # Apply validation set slices\n","        validation_mask = valid_mask.copy()\n","        if isinstance(validation_set, tuple):\n","            validation_mask[validation_set] &= valid_mask[validation_set]\n","\n","            # Check if there are valid points in the validation set\n","            if np.any(validation_mask):\n","                # Calculate max error over validation set\n","                validation_errors = np.abs(U_mc[validation_mask] - U_ana[validation_mask])\n","                max_validation_error = np.max(validation_errors)\n","    else:\n","        # If no validation set provided, use a central region\n","        # (exclude 20% of points from each edge)\n","        shape = valid_mask.shape\n","        ndim = len(shape)\n","\n","        validation_mask = valid_mask.copy()\n","        for d in range(ndim):\n","            size = shape[d]\n","            margin = max(1, int(size * 0.2))\n","\n","            # Create a slice for this dimension\n","            idx = tuple(slice(margin, size - margin) if i == d else slice(None)\n","                        for i in range(ndim))\n","\n","            # Update validation mask\n","            subregion = np.zeros_like(valid_mask, dtype=bool)\n","            subregion[idx] = True\n","            validation_mask &= subregion\n","\n","        # Calculate max error over validation set\n","        if np.any(validation_mask):\n","            validation_errors = np.abs(U_mc[validation_mask] - U_ana[validation_mask])\n","            max_validation_error = np.max(validation_errors)\n","\n","    # Calculate RMSE\n","    squared_errors = (U_mc_valid - U_ana_valid)**2\n","    rmse = np.sqrt(np.mean(squared_errors))  # Root Mean Square Error\n","\n","    # Calculate percentage of valid points\n","    valid_percentage = 100.0 * np.sum(valid_mask) / valid_mask.size\n","\n","    # Free memory for large arrays\n","    del validation_mask, valid_mask\n","    gc.collect()\n","\n","    return {\n","        'max': np.max(abs_errors),\n","        'mean': np.mean(abs_errors),\n","        'median': np.median(abs_errors),\n","        'max_validation': max_validation_error,\n","        'rmse': rmse,\n","        'valid_percentage': valid_percentage\n","    }\n","\n","# ---------- Plot functions -----------------------------------------\n","def plot_1d_results(x_ranges, f, s_arrays, u_star_mc, u_star_analytical, eps, func_name, mc_method, n_samples):\n","    \"\"\"Plot results for 1D transform.\"\"\"\n","    plt.figure(figsize=(15, 5))\n","\n","    # Sample points to plot original function\n","    x = np.linspace(x_ranges[0][0], x_ranges[0][1], 100)\n","    u_x = np.array([f(xi) for xi in x])\n","\n","    # Plot original function\n","    plt.subplot(1, 3, 1)\n","    plt.plot(x, u_x)\n","    plt.xlabel('x')\n","    plt.ylabel('u(x)')\n","    plt.title(f'Original {func_name.capitalize()} Function')\n","    plt.grid(True)\n","\n","    # Plot transforms\n","    plt.subplot(1, 3, 2)\n","    s = s_arrays[0]\n","    plt.plot(s, u_star_mc, 'g-.', label=f'{mc_method.capitalize()} MC (ε={eps}, N={n_samples})')\n","    plt.plot(s, u_star_analytical, 'k-', label='Analytical')\n","\n","    plt.xlabel('s')\n","    plt.ylabel('u*(s)')\n","    plt.title('Legendre Transform Comparison')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Plot error\n","    plt.subplot(1, 3, 3)\n","    error = np.abs(u_star_mc - u_star_analytical)\n","    plt.semilogy(s, error, 'r-', label='|MC-Analytical|')\n","\n","    # Add reference line showing MC error scaling O(1/√N)\n","    ref_line = 1.0 / np.sqrt(n_samples) * np.ones_like(s)\n","    plt.semilogy(s, ref_line, 'k--', label=f'1/√N reference (N={n_samples})')\n","\n","    plt.xlabel('s')\n","    plt.ylabel('Absolute Error')\n","    plt.title('Error vs Analytical')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","\n","# ---------- Run comprehensive benchmark ---------------------------\n","def run_comprehensive_benchmark(func_name, dimensions=[1, 2, 3, 4, 5], eps_values=[0.001, 0.01, 0.1, 0.5],\n","                               verbose=True, columns_to_display=None, mc_method='quasi', n_samples=100000):\n","    \"\"\"\n","    Run comprehensive benchmark for the specified function with multiple dimensions and epsilon values.\n","\n","    Parameters:\n","    func_name : str\n","        Name of the function to test ('neg_log' or 'neg_entropy')\n","    dimensions : list of int\n","        List of dimensions to test\n","    eps_values : list of float\n","        List of epsilon values to test\n","    verbose : bool\n","        Whether to print progress information\n","    columns_to_display : list of str\n","        List of column names to display in the final table\n","    mc_method : str\n","        Monte Carlo method to use ('basic', 'quasi', 'importance', 'adaptive')\n","    n_samples : int\n","        Number of Monte Carlo samples to use\n","\n","    Returns:\n","    pandas.DataFrame\n","        DataFrame containing benchmark results\n","    \"\"\"\n","    # Select function based on name\n","    if func_name == 'neg_log':\n","        f_creator = neg_log\n","        analytical_transform = neg_log_transform_analytical\n","        # Domain constraints: x > 0, s < 0\n","        x_range = (0.1, 5.0)\n","        s_range = (-5.0, -0.1)\n","    elif func_name == 'neg_entropy':\n","        f_creator = neg_entropy\n","        analytical_transform = neg_entropy_transform_analytical\n","        # Domain constraints: x > 0, s can be any real number\n","        x_range = (0.1, 5.0)\n","        s_range = (-3.0, 3.0)\n","    else:\n","        raise ValueError(f\"Unknown function type: {func_name}\")\n","\n","    # Create results list to convert to DataFrame later\n","    results_list = []\n","\n","    # If no columns specified, use default set\n","    if columns_to_display is None:\n","        columns_to_display = ['d', 'eps', 'sample_size', 'time', 'memory',\n","                             'max_err', 'max_validation', 'mean_err', 'rmse', 'conv_rate']\n","\n","    # Print header\n","    if verbose:\n","        print(f\"\\n===== BENCHMARK FOR {func_name.upper()} (MONTE CARLO: {mc_method.upper()}, {n_samples} SAMPLES) =====\\n\")\n","        header = f\"{'d':>2} | {'eps':>6} | {'sample size':>12} | {'time(s)':>8} | {'MB':>6} | {'max err':>8} | {'max val':>8} | {'mean err':>8} | {'RMSE':>8} | {'conv rate':>9}\"\n","        print(header)\n","        print(\"-\" * len(header))\n","\n","    # Previous errors for convergence rate calculation\n","    prev_eps = {}\n","    prev_errors = {}\n","\n","    # Loop through dimensions\n","    for d in dimensions:\n","        # Create evaluation grids for output and analytical comparison\n","        n_eval_pts = min(20, 100 // d)  # Reduce grid size for high dimensions\n","        s_arrs = [np.linspace(s_range[0], s_range[1], n_eval_pts) for _ in range(d)]\n","        x_ranges = [(x_range[0], x_range[1]) for _ in range(d)]\n","        f = f_creator(d)\n","\n","        # Calculate analytical solution for comparison\n","        try:\n","            # Calculate analytical solution\n","            U_ana = analytical_transform(s_arrs)\n","\n","            # Add a horizontal rule before each dimension\n","            if d > dimensions[0] and verbose:\n","                print(\"-\" * len(header))\n","\n","            # Loop through epsilon values\n","            for eps in sorted(eps_values):\n","                try:\n","                    # Measure memory and time using tracemalloc\n","                    start_memory_tracking()\n","\n","                    t0 = time.perf_counter()\n","\n","                    # Use Monte Carlo method - handle both result and possible error estimates\n","                    result_data = lf_transform_monte_carlo(\n","                        x_ranges=x_ranges,\n","                        f=f,\n","                        s_arrs=s_arrs,\n","                        eps=eps,\n","                        n_samples=n_samples,\n","                        method=mc_method\n","                    )\n","\n","                    # Handle either single result or (result, error_estimates) tuple\n","                    if isinstance(result_data, tuple) and len(result_data) == 2:\n","                        U_mc, error_estimates = result_data\n","                    else:\n","                        U_mc = result_data\n","                        error_estimates = None\n","\n","                    t_mc = time.perf_counter() - t0\n","\n","                    # Get peak memory usage\n","                    peak_memory = get_peak_memory()\n","                    stop_memory_tracking()\n","\n","                    # Define validation set (central region)\n","                    validation_set = None  # Using default central region\n","\n","                    # Calculate errors\n","                    errors = calculate_errors(U_mc, U_ana, validation_set)\n","\n","                    # Calculate convergence rate if possible\n","                    conv_rate = np.nan\n","                    if d in prev_eps and d in prev_errors and prev_eps[d] is not None:\n","                        if prev_errors[d] > 0 and errors['max_validation'] > 0:  # Avoid division by zero or log(0)\n","                            conv_rate = np.log(errors['max_validation']/prev_errors[d]) / np.log(eps/prev_eps[d])\n","\n","                    # Store results in dictionary\n","                    size_info = f\"MC({n_samples})\"\n","\n","                    result_dict = {\n","                        'd': d,\n","                        'eps': eps,\n","                        'sample_size': size_info,\n","                        'time': t_mc,\n","                        'memory': peak_memory,\n","                        'max_err': errors['max'],\n","                        'max_validation': errors['max_validation'],\n","                        'mean_err': errors['mean'],\n","                        'rmse': errors['rmse'],\n","                        'conv_rate': conv_rate,\n","                        'valid_percentage': errors['valid_percentage']\n","                    }\n","\n","                    # Add error estimate info if available\n","                    if error_estimates is not None:\n","                        result_dict['mc_error_est'] = np.mean(error_estimates)\n","\n","                    # Add to results list\n","                    results_list.append(result_dict)\n","\n","                    # Print results\n","                    if verbose:\n","                        size_display = f\"MC({n_samples})\"\n","\n","                        print(f\"{d:2d} | {eps:6.3f} | {size_display:>12} | {t_mc:8.3f} | {peak_memory:6.2f} | \"\n","                              f\"{errors['max']:8.2e} | {errors['max_validation']:8.2e} | {errors['mean']:8.2e} | {errors['rmse']:8.2e} | \"\n","                              f\"{conv_rate if not np.isnan(conv_rate) else 'N/A':>9}\")\n","\n","                    # Save for convergence rate calculation in the next iteration\n","                    prev_eps[d] = eps\n","                    prev_errors[d] = errors['max_validation']\n","\n","                    # Create visualizations for 1D\n","                    if d == 1:\n","                        plot_1d_results(x_ranges, f, s_arrs, U_mc, U_ana, eps, func_name, mc_method, n_samples)\n","                        plt.savefig(f'{func_name}_1d_mc_{mc_method}_eps{eps}_N{n_samples}.png', dpi=300)\n","                        plt.close()\n","\n","                    # Free memory\n","                    del U_mc\n","                    if error_estimates is not None:\n","                        del error_estimates\n","                    gc.collect()\n","\n","                except Exception as e:\n","                    if verbose:\n","                        print(f\"{d:2d} | {eps:6.3f} | {'ERROR':>12} | {'ERROR':>8} | {'ERROR':>6} | \"\n","                              f\"{'ERROR':>8} | {'ERROR':>8} | {'ERROR':>8} | {'ERROR':>8} | {'ERROR':>9}\")\n","                        print(f\"  Error: {e}\")\n","\n","                    # Add error entry to results\n","                    results_list.append({\n","                        'd': d,\n","                        'eps': eps,\n","                        'sample_size': f\"MC({n_samples})\",\n","                        'time': np.nan,\n","                        'memory': np.nan,\n","                        'max_err': np.nan,\n","                        'max_validation': np.nan,\n","                        'mean_err': np.nan,\n","                        'rmse': np.nan,\n","                        'conv_rate': np.nan,\n","                        'valid_percentage': np.nan\n","                    })\n","\n","            # Free analytical solution to save memory\n","            del U_ana\n","            gc.collect()\n","\n","        except Exception as e:\n","            if verbose:\n","                print(f\"Error with dimension {d}: {e}\")\n","\n","    # Convert results list to DataFrame\n","    results_df = pd.DataFrame(results_list)\n","\n","    # Create a nice table for display\n","    if verbose:\n","        print(\"\\nResults Table:\")\n","        # Use only the requested columns for display\n","        display_df = results_df[columns_to_display].copy()\n","        print(tabulate(display_df, headers='keys', tablefmt='grid', showindex=False, floatfmt='.3e'))\n","\n","    # Free memory before returning\n","    free_memory()\n","\n","    return results_df\n","\n","# ---------- Main function -----------------------------------------\n","def main():\n","    # Define dimensions and epsilon values to test\n","    dimensions = [1, 2, 3, 4, 5, 6, 7, 8]  # Can go to higher dimensions with Monte Carlo\n","    eps_values = [0.001, 0.01, 0.1, 0.5]\n","\n","    # Dictionary to store all results\n","    all_results = {}\n","\n","    # Interactive selection of columns to display\n","    print(\"Available columns to display in result tables:\")\n","    all_columns = ['d', 'eps', 'sample_size', 'time', 'memory',\n","                   'max_err', 'max_validation', 'mean_err', 'rmse', 'conv_rate', 'valid_percentage']\n","\n","    for i, col in enumerate(all_columns):\n","        print(f\"{i+1}. {col}\")\n","\n","    try:\n","        selected_indices = input(\"\\nEnter column numbers to display (comma-separated, e.g., 1,2,3,4): \")\n","        selected_indices = [int(idx.strip()) - 1 for idx in selected_indices.split(',')]\n","        columns_to_display = [all_columns[idx] for idx in selected_indices if 0 <= idx < len(all_columns)]\n","\n","        # If no valid selections, use default\n","        if not columns_to_display:\n","            print(\"No valid columns selected, using default columns\")\n","            columns_to_display = ['d', 'eps', 'sample_size', 'time', 'memory',\n","                                 'max_err', 'max_validation', 'mean_err', 'rmse']\n","    except Exception as e:\n","        print(f\"Error parsing column selection: {e}\")\n","        print(\"Using default columns\")\n","        columns_to_display = ['d', 'eps', 'sample_size', 'time', 'memory',\n","                             'max_err', 'max_validation', 'mean_err', 'rmse']\n","\n","    print(f\"\\nSelected columns: {', '.join(columns_to_display)}\")\n","\n","    # Monte Carlo configuration\n","    print(\"\\nMonte Carlo configuration:\")\n","    print(\"1. basic - Standard uniform sampling\")\n","    print(\"2. quasi - Quasi-Monte Carlo with Sobol sequences (default)\")\n","    print(\"3. importance - Importance sampling based on integrand values\")\n","    print(\"4. adaptive - Adaptive refinement of sampling in high-contribution regions\")\n","\n","    try:\n","        mc_method_idx = input(\"\\nSelect Monte Carlo method (1-4, default=2): \").strip()\n","        mc_methods = ['basic', 'quasi', 'importance', 'adaptive']\n","\n","        if mc_method_idx == '':\n","            mc_method = 'quasi'  # Default\n","        else:\n","            try:\n","                mc_method = mc_methods[int(mc_method_idx) - 1]\n","            except (ValueError, IndexError):\n","                print(\"Invalid selection, using quasi-Monte Carlo\")\n","                mc_method = 'quasi'\n","    except Exception:\n","        mc_method = 'quasi'\n","        print(\"Invalid input, using quasi-Monte Carlo\")\n","\n","    try:\n","        n_samples_input = input(\"\\nNumber of Monte Carlo samples (default: 100000): \").strip()\n","        if n_samples_input == '':\n","            n_samples = 100000  # Default\n","        else:\n","            n_samples = int(n_samples_input)\n","            if n_samples <= 0:\n","                n_samples = 100000\n","                print(\"Invalid number of samples, using default: 100000\")\n","    except Exception:\n","        n_samples = 100000\n","        print(\"Invalid input, using default: 100000 samples\")\n","\n","    print(f\"\\nUsing {mc_method} Monte Carlo with {n_samples} samples\")\n","\n","    # Run benchmarks for both function types\n","    for func in ['neg_log', 'neg_entropy']:\n","        try:\n","            print(f\"\\n=== Running benchmark for {func} ===\")\n","\n","            # Run comprehensive benchmark\n","            results_df = run_comprehensive_benchmark(\n","                func_name=func,\n","                dimensions=dimensions,\n","                eps_values=eps_values,\n","                columns_to_display=columns_to_display,\n","                mc_method=mc_method,\n","                n_samples=n_samples\n","            )\n","\n","            # Store results\n","            all_results[func] = results_df\n","\n","            # Save results to CSV\n","            results_df.to_csv(f'{func}_mc_{mc_method}_N{n_samples}_results.csv', index=False)\n","\n","            # Free memory\n","            free_memory()\n","\n","        except Exception as e:\n","            print(f\"Error running benchmark for {func}: {e}\")\n","\n","    # Create combined visualization for convergence rates\n","    try:\n","        plt.figure(figsize=(12, 8))\n","\n","        colors = {\n","            'neg_log': 'blue',\n","            'neg_entropy': 'red'\n","        }\n","\n","        markers = {\n","            1: 'o', 2: '^', 3: 's', 4: 'D', 5: 'p',\n","            6: '*', 7: 'X', 8: 'h', 9: '+', 10: 'x'\n","        }\n","\n","        for func, results_df in all_results.items():\n","            # Group by dimension\n","            for d, group in results_df.groupby('d'):\n","                if len(group) < 2:  # Skip if not enough points for a line\n","                    continue\n","\n","                # Extract values\n","                eps_values = group['eps'].values\n","                # Use max_validation instead of rmse for convergence plot\n","                err_values = group['max_validation'].values\n","\n","                # Skip if any error is NaN\n","                if np.any(np.isnan(err_values)):\n","                    continue\n","\n","                # Plot convergence\n","                label = f\"{func.capitalize()}, d={d}, {mc_method} MC\"\n","                plt.loglog(eps_values, err_values,\n","                           marker=markers.get(d, 'o'),\n","                           color=colors.get(func, 'black'),\n","                           label=label, alpha=0.8)\n","\n","        # Add reference slopes\n","        x_ref = np.array([min(eps_values), max(eps_values)])\n","        y_ref1 = 0.1 * x_ref  # O(ε)\n","        y_ref2 = 0.1 * x_ref**2  # O(ε²)\n","        y_ref_mc = 0.1 * np.ones_like(x_ref)  # O(1/√N), independent of ε\n","\n","        plt.loglog(x_ref, y_ref1, 'k--', label='O(ε)', alpha=0.5)\n","        plt.loglog(x_ref, y_ref2, 'k:', label='O(ε²)', alpha=0.5)\n","        plt.loglog(x_ref, y_ref_mc, 'k-.', label=f'O(1/√N), N={n_samples}', alpha=0.5)\n","\n","        plt.xlabel('ε (smoothing parameter)')\n","        plt.ylabel('Maximum Error (Validation Set)')\n","        plt.title(f'Convergence of Monte Carlo Approximation ({mc_method.capitalize()})')\n","        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","        plt.grid(True)\n","        plt.tight_layout()\n","\n","        plt.savefig(f'convergence_mc_{mc_method}_N{n_samples}.png', dpi=300)\n","        plt.close()\n","\n","    except Exception as e:\n","        print(f\"Error creating convergence visualization: {e}\")\n","\n","    # Free memory again\n","    free_memory()\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"LWI2Fs7j6C6E","outputId":"4fef3c49-ca78-4aca-864d-aba68c2463e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Available columns to display in result tables:\n","1. d\n","2. eps\n","3. sample_size\n","4. time\n","5. memory\n","6. max_err\n","7. max_validation\n","8. mean_err\n","9. rmse\n","10. conv_rate\n","11. valid_percentage\n","\n","Selected columns: d, eps, sample_size, time, memory, max_err, max_validation, mean_err, rmse, valid_percentage\n","\n","Monte Carlo configuration:\n","1. basic - Standard uniform sampling\n","2. quasi - Quasi-Monte Carlo with Sobol sequences (default)\n","3. importance - Importance sampling based on integrand values\n","4. adaptive - Adaptive refinement of sampling in high-contribution regions\n","\n","Using quasi Monte Carlo with 100000 samples\n","\n","=== Running benchmark for neg_entropy ===\n","\n","===== BENCHMARK FOR NEG_ENTROPY (MONTE CARLO: QUASI, 100000 SAMPLES) =====\n","\n"," d |    eps |  sample size |  time(s) |     MB |  max err |  max val | mean err |     RMSE | conv rate\n","------------------------------------------------------------------------------------------------------\n"," 1 |  0.001 |   MC(100000) |    4.379 |  11.61 | 4.42e-01 | 1.43e-02 | 3.78e-02 | 1.03e-01 |       N/A\n"," 1 |  0.010 |   MC(100000) |    4.196 |   6.15 | 4.73e-01 | 4.86e-02 | 5.96e-02 | 1.18e-01 | 0.530597797123935\n"," 1 |  0.100 |   MC(100000) |    4.145 |   6.13 | 5.83e-01 | 2.34e-01 | 1.58e-01 | 2.19e-01 | 0.6834473996000413\n"," 1 |  0.500 |   MC(100000) |    4.150 |   6.14 | 9.02e-01 | 5.64e-01 | 4.10e-01 | 4.75e-01 | 0.5450947879143101\n","------------------------------------------------------------------------------------------------------\n"," 2 |  0.001 |   MC(100000) |    8.207 |   9.26 | 8.87e-01 | 3.29e-02 | 7.69e-02 | 1.57e-01 |       N/A\n"," 2 |  0.010 |   MC(100000) |    8.206 |   9.28 | 9.47e-01 | 9.87e-02 | 1.20e-01 | 1.87e-01 | 0.4772872917154633\n"," 2 |  0.100 |   MC(100000) |    7.933 |   9.26 | 1.17e+00 | 4.69e-01 | 3.03e-01 | 3.75e-01 | 0.6767557100992315\n"," 2 |  0.500 |   MC(100000) |    7.993 |   9.26 | 1.80e+00 | 1.13e+00 | 5.73e-01 | 7.04e-01 | 0.5451516699476471\n","------------------------------------------------------------------------------------------------------\n"," 3 |  0.001 |   MC(100000) |   30.088 |  12.47 | 1.39e+00 | 1.73e-01 | 1.39e-01 | 2.29e-01 |       N/A\n"," 3 |  0.010 |   MC(100000) |   33.914 |  12.50 | 1.44e+00 | 1.50e-01 | 1.85e-01 | 2.60e-01 | -0.061858158775182116\n"," 3 |  0.100 |   MC(100000) |   22.726 |  13.92 | 1.75e+00 | 6.84e-01 | 4.50e-01 | 5.26e-01 | 0.6582423385217329\n"," 3 |  0.500 |   MC(100000) |   22.507 |  13.22 | 2.70e+00 | 1.69e+00 | 7.25e-01 | 8.98e-01 | 0.5623481991802886\n","------------------------------------------------------------------------------------------------------\n"," 4 |  0.001 |   MC(100000) |  403.168 |  18.61 | 2.01e+00 | 8.49e-01 | 2.95e-01 | 4.10e-01 |       N/A\n"," 4 |  0.010 |   MC(100000) |  524.356 |  17.96 | 2.00e+00 | 7.75e-01 | 3.32e-01 | 4.33e-01 | -0.0396320230038654\n"," 4 |  0.100 |   MC(100000) |  301.965 |  20.07 | 2.51e+00 | 1.28e+00 | 6.22e-01 | 7.17e-01 | 0.2178664988151153\n"," 4 |  0.500 |   MC(100000) |  306.520 |  17.91 | 3.57e+00 | 2.20e+00 | 8.68e-01 | 1.07e+00 | 0.3367855078094212\n","------------------------------------------------------------------------------------------------------\n"," 5 |  0.001 |   MC(100000) | 8433.029 |  67.29 | 3.79e+00 | 1.41e+00 | 5.63e-01 | 7.03e-01 |       N/A\n"," 5 |  0.010 |   MC(100000) | 11433.637 |  67.22 | 4.06e+00 | 1.94e+00 | 6.07e-01 | 7.55e-01 | 0.13893214488272376\n"," 5 |  0.100 |   MC(100000) | 6954.801 |  67.22 | 5.05e+00 | 2.64e+00 | 8.57e-01 | 1.00e+00 | 0.13322608069492115\n"," 5 |  0.500 |   MC(100000) | 6799.464 |  67.22 | 5.59e+00 | 3.20e+00 | 1.02e+00 | 1.27e+00 | 0.11982429210762627\n","------------------------------------------------------------------------------------------------------\n"," 6 |  0.001 |   MC(100000) | 49094.921 | 277.44 | 5.79e+00 | 2.59e+00 | 1.05e+00 | 1.24e+00 |       N/A\n"]}],"source":["import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from itertools import product\n","import matplotlib.colors as colors\n","import psutil\n","import os\n","import gc\n","import pandas as pd\n","from tabulate import tabulate\n","import tracemalloc\n","from scipy.stats import qmc\n","\n","# ---------- Memory tracking functions ------------------------------\n","def start_memory_tracking():\n","    \"\"\"Start tracking memory allocations\"\"\"\n","    gc.collect()  # Force garbage collection\n","    tracemalloc.start()\n","    return tracemalloc.get_traced_memory()[0] / (1024 * 1024)  # Current memory in MB\n","\n","def get_peak_memory():\n","    \"\"\"Get peak memory usage in MB since tracking started\"\"\"\n","    current, peak = tracemalloc.get_traced_memory()\n","    return peak / (1024 * 1024)  # Peak memory in MB\n","\n","def stop_memory_tracking():\n","    \"\"\"Stop tracking memory allocations\"\"\"\n","    current, peak = tracemalloc.get_traced_memory()\n","    tracemalloc.stop()\n","    return peak / (1024 * 1024)  # Peak memory in MB\n","\n","def free_memory():\n","    \"\"\"Aggressively free memory\"\"\"\n","    gc.collect()  # Collect garbage\n","    if hasattr(plt, 'close'):\n","        plt.close('all')  # Close all matplotlib figures\n","\n","    # Try to clear large variables from memory\n","    for name in list(globals().keys()):\n","        if name.startswith('__'):\n","            continue\n","        obj = globals()[name]\n","        if isinstance(obj, np.ndarray) and obj.size > 1000000:\n","            del globals()[name]\n","\n","    gc.collect()  # Collect garbage again after deleting variables\n","\n","# ---------- Test functions -----------------------------------------\n","def neg_log(d):\n","    \"\"\"d-dimensional negative logarithm function: u(x) = -sum(log(x_i))\n","    Domain: x_i > 0\n","    LF transform: u*(s) = -d - sum(log(-s_i)) for s_i < 0\n","    \"\"\"\n","    def func(*args):\n","        # Handle domain constraints - inputs should be positive\n","        result = 0\n","        for x in args:\n","            x_safe = np.maximum(x, 1e-10)  # Avoid log(0)\n","            result -= np.log(x_safe)\n","        return result\n","    return func\n","\n","def neg_entropy(d):\n","    \"\"\"d-dimensional negative entropy function: u(x) = sum(x_i * log(x_i))\n","    Domain: x_i > 0\n","    LF transform: u*(s) = sum(exp(s_i - 1))\n","    \"\"\"\n","    def func(*args):\n","        result = 0\n","        for x in args:\n","            # Handle domain constraints and avoid singularities\n","            x_safe = np.maximum(x, 1e-10)\n","            x_log_x = x_safe * np.log(x_safe)\n","            # Set values to 0 for x close to 0 (lim x→0 x log(x) = 0)\n","            x_log_x = np.where(x_safe < 1e-8, 0, x_log_x)\n","            result += x_log_x\n","        return result\n","    return func\n","\n","# ---------- Analytical LF transforms for validation ----------------\n","def neg_log_transform_analytical(s_arrs):\n","    \"\"\"\n","    Analytical LF transform for negative log function:\n","    f*(s) = -d - sum(log(-s_i)) for s_i < 0\n","    \"\"\"\n","    d = len(s_arrs)\n","    S = np.meshgrid(*s_arrs, indexing='ij', sparse=False)\n","\n","    # Initialize with -d term\n","    result = np.full(tuple(len(s) for s in s_arrs), -d, dtype=float)\n","\n","    # Add log terms for each dimension\n","    for i in range(d):\n","        # Apply only where s < 0 (domain constraint)\n","        valid_mask = S[i] < 0\n","        # Set large negative values for invalid regions\n","        log_term = np.log(-S[i])\n","        result = np.where(valid_mask, result - log_term, -np.inf)\n","\n","    return result\n","\n","def neg_entropy_transform_analytical(s_arrs):\n","    \"\"\"\n","    Analytical LF transform for negative entropy function:\n","    f*(s) = sum(exp(s_i - 1))\n","    \"\"\"\n","    d = len(s_arrs)\n","    S = np.meshgrid(*s_arrs, indexing='ij', sparse=False)\n","\n","    # Initialize with zeros\n","    result = np.zeros(tuple(len(s) for s in s_arrs), dtype=float)\n","\n","    # Add exp terms for each dimension\n","    for i in range(d):\n","        result += np.exp(S[i] - 1)\n","\n","    return result\n","\n","# ---------- Monte Carlo LF Transform Methods ----------------------\n","def lf_transform_monte_carlo(x_ranges, f, s_arrs, eps=0.1, n_samples=100000, method='quasi'):\n","    \"\"\"\n","    Monte Carlo approximation of Legendre-Fenchel transform.\n","\n","    Parameters:\n","    x_ranges : list of tuples\n","        Range for each dimension of x (min, max)\n","    f : function\n","        The function to transform\n","    s_arrs : list of arrays\n","        Grid points in dual space (one array per dimension)\n","    eps : float\n","        Smoothing parameter\n","    n_samples : int\n","        Number of Monte Carlo samples\n","    method : str\n","        Monte Carlo method to use: 'basic', 'quasi', 'importance', or 'adaptive'\n","\n","    Returns:\n","    numpy.ndarray\n","        The approximate Legendre-Fenchel transform f*_eps(s)\n","    \"\"\"\n","    # Get dimension\n","    d = len(x_ranges)\n","\n","    # Create output array for the transform\n","    result = np.empty([len(s) for s in s_arrs])\n","\n","    # Create separate error estimates array if needed\n","    error_estimates = None\n","    if method in ['basic', 'quasi']:\n","        error_estimates = np.empty_like(result)\n","\n","    # Calculate domain volume\n","    domain_volume = np.prod([x_max - x_min for x_min, x_max in x_ranges])\n","\n","    # Generate samples based on the selected method\n","    if method == 'basic':\n","        # Simple uniform random sampling\n","        samples = np.array([np.random.uniform(x_min, x_max, n_samples)\n","                           for x_min, x_max in x_ranges]).T\n","\n","    elif method == 'quasi':\n","        # Quasi-Monte Carlo with Sobol sequences for better coverage\n","        # Initialize Sobol sequence generator\n","        sampler = qmc.Sobol(d=d, scramble=True)\n","\n","        # Generate samples in [0, 1]\n","        unit_samples = sampler.random(n_samples)\n","\n","        # Scale to the domain range\n","        samples = np.zeros((n_samples, d))\n","        for i in range(d):\n","            x_min, x_max = x_ranges[i]\n","            samples[:, i] = unit_samples[:, i] * (x_max - x_min) + x_min\n","\n","    elif method == 'importance' or method == 'adaptive':\n","        # For importance sampling, we need an initial set of samples to identify high-contribution regions\n","        # Start with uniform samples\n","        initial_samples = np.array([np.random.uniform(x_min, x_max, min(10000, n_samples // 10))\n","                                   for x_min, x_max in x_ranges]).T\n","\n","        # Pre-compute function values\n","        f_values_initial = np.array([f(*x) for x in initial_samples])\n","\n","        # For each output point, we'll generate specialized importance samples\n","        # This makes this method more computationally intensive but potentially more accurate\n","        samples_per_point = max(1000, n_samples // (np.prod([len(s) for s in s_arrs])))\n","\n","    else:\n","        raise ValueError(f\"Unknown Monte Carlo method: {method}\")\n","\n","    # For basic and quasi-Monte Carlo, pre-compute function values for all samples\n","    if method in ['basic', 'quasi']:\n","        f_values = np.array([f(*x) for x in samples])\n","\n","    # Iterate through the output grid\n","    it = np.nditer(result, flags=['multi_index'], op_flags=['writeonly'])\n","    while not it.finished:\n","        # Get current slope point\n","        s_point = [s_arrs[i][it.multi_index[i]] for i in range(d)]\n","\n","        if method in ['basic', 'quasi']:\n","            # Compute inner products for all samples\n","            inner_products = np.sum([s_point[i] * samples[:, i] for i in range(d)], axis=0)\n","\n","            # Compute exponents\n","            exponents = (inner_products - f_values) / eps\n","\n","            # Shift to prevent overflow\n","            max_exp = np.max(exponents)\n","            exponents -= max_exp\n","\n","            # Compute Monte Carlo approximation\n","            mc_sum = np.sum(np.exp(exponents))\n","            mc_avg = mc_sum / n_samples\n","\n","            # Apply scaling and logarithm\n","            it[0] = eps * (np.log(domain_volume * mc_avg) + max_exp)\n","\n","            # Calculate error estimate (for standard Monte Carlo)\n","            # Standard error = sigma / sqrt(N)\n","            if error_estimates is not None and method == 'basic':\n","                variance = np.var(np.exp(exponents)) * (domain_volume**2)\n","                std_error = np.sqrt(variance / n_samples)\n","                error_estimates[it.multi_index] = eps * std_error / (domain_volume * mc_avg)\n","\n","\n","        elif method == 'importance':\n","            # For importance sampling, we want to sample more heavily in regions where the integrand is large\n","            # Compute integrand values for initial samples for the current s_point\n","            inner_products_init = np.sum([s_point[i] * initial_samples[:, i] for i in range(d)], axis=0)\n","            integrand_values = np.exp((inner_products_init - f_values_initial) / eps)\n","\n","            # Normalize to create a probability distribution\n","            if np.sum(integrand_values) > 0:\n","                prob_dist = integrand_values / np.sum(integrand_values)\n","            else:\n","                # If all values are zero (underflow), use uniform distribution\n","                prob_dist = np.ones_like(integrand_values) / len(integrand_values)\n","\n","            # Generate importance samples for this specific s_point\n","            importance_indices = np.random.choice(\n","                range(len(initial_samples)),\n","                size=samples_per_point,\n","                replace=True,\n","                p=prob_dist\n","            )\n","            importance_samples = initial_samples[importance_indices]\n","\n","            # Compute function values for importance samples\n","            f_values_importance = f_values_initial[importance_indices]\n","\n","            # Compute integrand with importance sampling correction\n","            inner_products = np.sum([s_point[i] * importance_samples[:, i] for i in range(d)], axis=0)\n","            exponents = (inner_products - f_values_importance) / eps\n","\n","            # Apply importance sampling correction: divide by probability used for sampling\n","            importance_weights = 1.0 / (prob_dist[importance_indices] * len(prob_dist))\n","\n","            # Avoid overflow\n","            max_exp = np.max(exponents)\n","            exponents -= max_exp\n","            weighted_sum = np.sum(np.exp(exponents) * importance_weights)\n","\n","            # Compute final result\n","            it[0] = eps * (np.log(domain_volume * weighted_sum / samples_per_point) + max_exp)\n","\n","        elif method == 'adaptive':\n","            # Similar to importance sampling but with adaptive refinement\n","            # First pass with initial samples\n","            inner_products_init = np.sum([s_point[i] * initial_samples[:, i] for i in range(d)], axis=0)\n","            integrand_values = np.exp((inner_products_init - f_values_initial) / eps)\n","\n","            # Identify regions needing refinement (high integrand value or high variance)\n","            if len(integrand_values) > 0:\n","                threshold = np.percentile(integrand_values, 95)  # Focus on top 5%\n","                high_contribution_indices = np.where(integrand_values > threshold)[0]\n","\n","                if len(high_contribution_indices) > 0:\n","                    # Extract high-contribution samples\n","                    high_samples = initial_samples[high_contribution_indices]\n","\n","                    # Define a refined sampling region around these samples\n","                    refined_ranges = []\n","                    for dim in range(d):\n","                        min_val = max(x_ranges[dim][0], np.min(high_samples[:, dim]) - 0.1 * (x_ranges[dim][1] - x_ranges[dim][0]))\n","                        max_val = min(x_ranges[dim][1], np.max(high_samples[:, dim]) + 0.1 * (x_ranges[dim][1] - x_ranges[dim][0]))\n","                        refined_ranges.append((min_val, max_val))\n","\n","                    # Generate refined samples\n","                    refined_samples = np.array([np.random.uniform(r_min, r_max, samples_per_point)\n","                                              for r_min, r_max in refined_ranges]).T\n","\n","                    # Compute function values for refined samples\n","                    f_values_refined = np.array([f(*x) for x in refined_samples])\n","\n","                    # Compute integrand for refined samples\n","                    inner_products_refined = np.sum([s_point[i] * refined_samples[:, i] for i in range(d)], axis=0)\n","                    exponents_refined = (inner_products_refined - f_values_refined) / eps\n","\n","                    # Calculate refined volume\n","                    refined_volume = np.prod([r_max - r_min for r_min, r_max in refined_ranges])\n","                    ratio = refined_volume / domain_volume\n","\n","                    # Combine with initial estimate (weighted by volume ratio)\n","                    max_exp_init = np.max(inner_products_init - f_values_initial) / eps if len(inner_products_init) > 0 else 0\n","                    init_sum = np.sum(np.exp(((inner_products_init - f_values_initial) / eps) - max_exp_init))\n","\n","                    max_exp_refined = np.max(exponents_refined) if len(exponents_refined) > 0 else 0\n","                    refined_sum = np.sum(np.exp(exponents_refined - max_exp_refined))\n","\n","                    # Combine estimates with appropriate volume scaling\n","                    max_exp_combined = max(max_exp_init, max_exp_refined)\n","                    init_contribution = init_sum * np.exp(max_exp_init - max_exp_combined) * (1 - ratio)\n","                    refined_contribution = refined_sum * np.exp(max_exp_refined - max_exp_combined) * ratio\n","\n","                    combined_sum = init_contribution + refined_contribution\n","                    it[0] = eps * (np.log(domain_volume * combined_sum / samples_per_point) + max_exp_combined)\n","                else:\n","                    # Fall back to basic MC if no high-contribution regions found\n","                    max_exp = np.max((inner_products_init - f_values_initial) / eps)\n","                    mc_sum = np.sum(np.exp(((inner_products_init - f_values_initial) / eps) - max_exp))\n","                    mc_avg = mc_sum / len(initial_samples)\n","                    it[0] = eps * (np.log(domain_volume * mc_avg) + max_exp)\n","            else:\n","                # No valid samples, fall back to analytical approximation if possible\n","                it[0] = 0  # Default value\n","\n","        it.iternext()\n","\n","    # Return both result and error estimates if available\n","    if error_estimates is not None:\n","        return result, error_estimates\n","    return result\n","\n","# ---------- Error calculation functions ---------------------\n","def calculate_errors(U_mc, U_ana, validation_set=None):\n","    \"\"\"\n","    Calculate various error metrics between Monte Carlo approximation and analytical solution.\n","\n","    Parameters:\n","    U_mc : numpy.ndarray\n","        Monte Carlo approximation\n","    U_ana : numpy.ndarray\n","        Analytical solution\n","    validation_set : tuple, optional\n","        Tuple of slice objects defining a validation set region\n","\n","    Returns:\n","    dict\n","        Dictionary of error metrics\n","    \"\"\"\n","    # Get valid points (exclude -inf values)\n","    valid_mask = ~np.isinf(U_ana) & ~np.isinf(U_mc) & ~np.isnan(U_ana) & ~np.isnan(U_mc)\n","\n","    if np.sum(valid_mask) == 0:\n","        return {\n","            'max': np.nan,\n","            'mean': np.nan,\n","            'median': np.nan,\n","            'max_validation': np.nan,\n","            'rmse': np.nan,\n","            'valid_percentage': 0\n","        }\n","\n","    # Extract valid points\n","    U_mc_valid = U_mc[valid_mask]\n","    U_ana_valid = U_ana[valid_mask]\n","\n","    # Calculate absolute errors\n","    abs_errors = np.abs(U_mc_valid - U_ana_valid)\n","\n","    # Calculate max error over validation set if provided\n","    max_validation_error = np.nan\n","    if validation_set is not None:\n","        # Apply validation set slices\n","        validation_mask = valid_mask.copy()\n","        if isinstance(validation_set, tuple):\n","            validation_mask[validation_set] &= valid_mask[validation_set]\n","\n","            # Check if there are valid points in the validation set\n","            if np.any(validation_mask):\n","                # Calculate max error over validation set\n","                validation_errors = np.abs(U_mc[validation_mask] - U_ana[validation_mask])\n","                max_validation_error = np.max(validation_errors)\n","    else:\n","        # If no validation set provided, use a central region\n","        # (exclude 20% of points from each edge)\n","        shape = valid_mask.shape\n","        ndim = len(shape)\n","\n","        validation_mask = valid_mask.copy()\n","        for d in range(ndim):\n","            size = shape[d]\n","            margin = max(1, int(size * 0.2))\n","\n","            # Create a slice for this dimension\n","            idx = tuple(slice(margin, size - margin) if i == d else slice(None)\n","                        for i in range(ndim))\n","\n","            # Update validation mask\n","            subregion = np.zeros_like(valid_mask, dtype=bool)\n","            subregion[idx] = True\n","            validation_mask &= subregion\n","\n","        # Calculate max error over validation set\n","        if np.any(validation_mask):\n","            validation_errors = np.abs(U_mc[validation_mask] - U_ana[validation_mask])\n","            max_validation_error = np.max(validation_errors)\n","\n","    # Calculate RMSE\n","    squared_errors = (U_mc_valid - U_ana_valid)**2\n","    rmse = np.sqrt(np.mean(squared_errors))  # Root Mean Square Error\n","\n","    # Calculate percentage of valid points\n","    valid_percentage = 100.0 * np.sum(valid_mask) / valid_mask.size\n","\n","    # Free memory for large arrays\n","    del validation_mask, valid_mask\n","    gc.collect()\n","\n","    return {\n","        'max': np.max(abs_errors),\n","        'mean': np.mean(abs_errors),\n","        'median': np.median(abs_errors),\n","        'max_validation': max_validation_error,\n","        'rmse': rmse,\n","        'valid_percentage': valid_percentage\n","    }\n","\n","# ---------- Plot functions -----------------------------------------\n","def plot_1d_results(x_ranges, f, s_arrays, u_star_mc, u_star_analytical, eps, func_name, mc_method, n_samples):\n","    \"\"\"Plot results for 1D transform.\"\"\"\n","    plt.figure(figsize=(15, 5))\n","\n","    # Sample points to plot original function\n","    x = np.linspace(x_ranges[0][0], x_ranges[0][1], 100)\n","    u_x = np.array([f(xi) for xi in x])\n","\n","    # Plot original function\n","    plt.subplot(1, 3, 1)\n","    plt.plot(x, u_x)\n","    plt.xlabel('x')\n","    plt.ylabel('u(x)')\n","    plt.title(f'Original {func_name.capitalize()} Function')\n","    plt.grid(True)\n","\n","    # Plot transforms\n","    plt.subplot(1, 3, 2)\n","    s = s_arrays[0]\n","    plt.plot(s, u_star_mc, 'g-.', label=f'{mc_method.capitalize()} MC (ε={eps}, N={n_samples})')\n","    plt.plot(s, u_star_analytical, 'k-', label='Analytical')\n","\n","    plt.xlabel('s')\n","    plt.ylabel('u*(s)')\n","    plt.title('Legendre Transform Comparison')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Plot error\n","    plt.subplot(1, 3, 3)\n","    error = np.abs(u_star_mc - u_star_analytical)\n","    plt.semilogy(s, error, 'r-', label='|MC-Analytical|')\n","\n","    # Add reference line showing MC error scaling O(1/√N)\n","    ref_line = 1.0 / np.sqrt(n_samples) * np.ones_like(s)\n","    plt.semilogy(s, ref_line, 'k--', label=f'1/√N reference (N={n_samples})')\n","\n","    plt.xlabel('s')\n","    plt.ylabel('Absolute Error')\n","    plt.title('Error vs Analytical')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","\n","# ---------- Run comprehensive benchmark ---------------------------\n","def run_comprehensive_benchmark(func_name, dimensions=[1, 2, 3, 4, 5], eps_values=[0.001, 0.01, 0.1, 0.5],\n","                               verbose=True, columns_to_display=None, mc_method='quasi', n_samples=100000):\n","    \"\"\"\n","    Run comprehensive benchmark for the specified function with multiple dimensions and epsilon values.\n","\n","    Parameters:\n","    func_name : str\n","        Name of the function to test ('neg_log' or 'neg_entropy')\n","    dimensions : list of int\n","        List of dimensions to test\n","    eps_values : list of float\n","        List of epsilon values to test\n","    verbose : bool\n","        Whether to print progress information\n","    columns_to_display : list of str\n","        List of column names to display in the final table\n","    mc_method : str\n","        Monte Carlo method to use ('basic', 'quasi', 'importance', 'adaptive')\n","    n_samples : int\n","        Number of Monte Carlo samples to use\n","\n","    Returns:\n","    pandas.DataFrame\n","        DataFrame containing benchmark results\n","    \"\"\"\n","    # Select function based on name\n","    if func_name == 'neg_log':\n","        f_creator = neg_log\n","        analytical_transform = neg_log_transform_analytical\n","        # Domain constraints: x > 0, s < 0\n","        x_range = (0.1, 5.0)\n","        s_range = (-5.0, -0.1)\n","    elif func_name == 'neg_entropy':\n","        f_creator = neg_entropy\n","        analytical_transform = neg_entropy_transform_analytical\n","        # Domain constraints: x > 0, s can be any real number\n","        x_range = (0.1, 5.0)\n","        s_range = (-3.0, 3.0)\n","    else:\n","        raise ValueError(f\"Unknown function type: {func_name}\")\n","\n","    # Create results list to convert to DataFrame later\n","    results_list = []\n","\n","    # If no columns specified, use default set\n","    if columns_to_display is None:\n","        columns_to_display = ['d', 'eps', 'sample_size', 'time', 'memory',\n","                             'max_err', 'max_validation', 'mean_err', 'rmse', 'conv_rate']\n","\n","    # Print header\n","    if verbose:\n","        print(f\"\\n===== BENCHMARK FOR {func_name.upper()} (MONTE CARLO: {mc_method.upper()}, {n_samples} SAMPLES) =====\\n\")\n","        header = f\"{'d':>2} | {'eps':>6} | {'sample size':>12} | {'time(s)':>8} | {'MB':>6} | {'max err':>8} | {'max val':>8} | {'mean err':>8} | {'RMSE':>8} | {'conv rate':>9}\"\n","        print(header)\n","        print(\"-\" * len(header))\n","\n","    # Previous errors for convergence rate calculation\n","    prev_eps = {}\n","    prev_errors = {}\n","\n","    # Loop through dimensions\n","    for d in dimensions:\n","        # Create evaluation grids for output and analytical comparison\n","        n_eval_pts = min(20, 100 // d)  # Reduce grid size for high dimensions\n","        s_arrs = [np.linspace(s_range[0], s_range[1], n_eval_pts) for _ in range(d)]\n","        x_ranges = [(x_range[0], x_range[1]) for _ in range(d)]\n","        f = f_creator(d)\n","\n","        # Calculate analytical solution for comparison\n","        try:\n","            # Calculate analytical solution\n","            U_ana = analytical_transform(s_arrs)\n","\n","            # Add a horizontal rule before each dimension\n","            if d > dimensions[0] and verbose:\n","                print(\"-\" * len(header))\n","\n","            # Loop through epsilon values\n","            for eps in sorted(eps_values):\n","                try:\n","                    # Measure memory and time using tracemalloc\n","                    start_memory_tracking()\n","\n","                    t0 = time.perf_counter()\n","\n","                    # Use Monte Carlo method - handle both result and possible error estimates\n","                    result_data = lf_transform_monte_carlo(\n","                        x_ranges=x_ranges,\n","                        f=f,\n","                        s_arrs=s_arrs,\n","                        eps=eps,\n","                        n_samples=n_samples,\n","                        method=mc_method\n","                    )\n","\n","                    # Handle either single result or (result, error_estimates) tuple\n","                    if isinstance(result_data, tuple) and len(result_data) == 2:\n","                        U_mc, error_estimates = result_data\n","                    else:\n","                        U_mc = result_data\n","                        error_estimates = None\n","\n","                    t_mc = time.perf_counter() - t0\n","\n","                    # Get peak memory usage\n","                    peak_memory = get_peak_memory()\n","                    stop_memory_tracking()\n","\n","                    # Define validation set (central region)\n","                    validation_set = None  # Using default central region\n","\n","                    # Calculate errors\n","                    errors = calculate_errors(U_mc, U_ana, validation_set)\n","\n","                    # Calculate convergence rate if possible\n","                    conv_rate = np.nan\n","                    if d in prev_eps and d in prev_errors and prev_eps[d] is not None:\n","                        if prev_errors[d] > 0 and errors['max_validation'] > 0:  # Avoid division by zero or log(0)\n","                            conv_rate = np.log(errors['max_validation']/prev_errors[d]) / np.log(eps/prev_eps[d])\n","\n","                    # Store results in dictionary\n","                    size_info = f\"MC({n_samples})\"\n","\n","                    result_dict = {\n","                        'd': d,\n","                        'eps': eps,\n","                        'sample_size': size_info,\n","                        'time': t_mc,\n","                        'memory': peak_memory,\n","                        'max_err': errors['max'],\n","                        'max_validation': errors['max_validation'],\n","                        'mean_err': errors['mean'],\n","                        'rmse': errors['rmse'],\n","                        'conv_rate': conv_rate,\n","                        'valid_percentage': errors['valid_percentage']\n","                    }\n","\n","                    # Add error estimate info if available\n","                    if error_estimates is not None:\n","                        result_dict['mc_error_est'] = np.mean(error_estimates)\n","\n","                    # Add to results list\n","                    results_list.append(result_dict)\n","\n","                    # Print results\n","                    if verbose:\n","                        size_display = f\"MC({n_samples})\"\n","\n","                        print(f\"{d:2d} | {eps:6.3f} | {size_display:>12} | {t_mc:8.3f} | {peak_memory:6.2f} | \"\n","                              f\"{errors['max']:8.2e} | {errors['max_validation']:8.2e} | {errors['mean']:8.2e} | {errors['rmse']:8.2e} | \"\n","                              f\"{conv_rate if not np.isnan(conv_rate) else 'N/A':>9}\")\n","\n","                    # Save for convergence rate calculation in the next iteration\n","                    prev_eps[d] = eps\n","                    prev_errors[d] = errors['max_validation']\n","\n","                    # Create visualizations for 1D\n","                    if d == 1:\n","                        plot_1d_results(x_ranges, f, s_arrs, U_mc, U_ana, eps, func_name, mc_method, n_samples)\n","                        plt.savefig(f'{func_name}_1d_mc_{mc_method}_eps{eps}_N{n_samples}.png', dpi=300)\n","                        plt.close()\n","\n","                    # Free memory\n","                    del U_mc\n","                    if error_estimates is not None:\n","                        del error_estimates\n","                    gc.collect()\n","\n","                except Exception as e:\n","                    if verbose:\n","                        print(f\"{d:2d} | {eps:6.3f} | {'ERROR':>12} | {'ERROR':>8} | {'ERROR':>6} | \"\n","                              f\"{'ERROR':>8} | {'ERROR':>8} | {'ERROR':>8} | {'ERROR':>8} | {'ERROR':>9}\")\n","                        print(f\"  Error: {e}\")\n","\n","                    # Add error entry to results\n","                    results_list.append({\n","                        'd': d,\n","                        'eps': eps,\n","                        'sample_size': f\"MC({n_samples})\",\n","                        'time': np.nan,\n","                        'memory': np.nan,\n","                        'max_err': np.nan,\n","                        'max_validation': np.nan,\n","                        'mean_err': np.nan,\n","                        'rmse': np.nan,\n","                        'conv_rate': np.nan,\n","                        'valid_percentage': np.nan\n","                    })\n","\n","            # Free analytical solution to save memory\n","            del U_ana\n","            gc.collect()\n","\n","        except Exception as e:\n","            if verbose:\n","                print(f\"Error with dimension {d}: {e}\")\n","\n","    # Convert results list to DataFrame\n","    results_df = pd.DataFrame(results_list)\n","\n","    # Create a nice table for display\n","    if verbose:\n","        print(\"\\nResults Table:\")\n","        # Use only the requested columns for display\n","        display_df = results_df[columns_to_display].copy()\n","        print(tabulate(display_df, headers='keys', tablefmt='grid', showindex=False, floatfmt='.3e'))\n","\n","    # Free memory before returning\n","    free_memory()\n","\n","    return results_df\n","\n","# ---------- Main function -----------------------------------------\n","def main():\n","    # Define dimensions and epsilon values to test\n","    dimensions = [1, 2, 3, 4, 5, 6, 7, 8]  # Can go to higher dimensions with Monte Carlo\n","    eps_values = [0.001, 0.01, 0.1, 0.5]\n","\n","    # Dictionary to store all results\n","    all_results = {}\n","\n","    # Interactive selection of columns to display\n","    print(\"Available columns to display in result tables:\")\n","    all_columns = ['d', 'eps', 'sample_size', 'time', 'memory',\n","                   'max_err', 'max_validation', 'mean_err', 'rmse', 'conv_rate', 'valid_percentage']\n","\n","    for i, col in enumerate(all_columns):\n","        print(f\"{i+1}. {col}\")\n","\n","    try:\n","        selected_indices = input(\"\\nEnter column numbers to display (comma-separated, e.g., 1,2,3,4): \")\n","        selected_indices = [int(idx.strip()) - 1 for idx in selected_indices.split(',')]\n","        columns_to_display = [all_columns[idx] for idx in selected_indices if 0 <= idx < len(all_columns)]\n","\n","        # If no valid selections, use default\n","        if not columns_to_display:\n","            print(\"No valid columns selected, using default columns\")\n","            columns_to_display = ['d', 'eps', 'sample_size', 'time', 'memory',\n","                                 'max_err', 'max_validation', 'mean_err', 'rmse']\n","    except Exception as e:\n","        print(f\"Error parsing column selection: {e}\")\n","        print(\"Using default columns\")\n","        columns_to_display = ['d', 'eps', 'sample_size', 'time', 'memory',\n","                             'max_err', 'max_validation', 'mean_err', 'rmse']\n","\n","    print(f\"\\nSelected columns: {', '.join(columns_to_display)}\")\n","\n","    # Monte Carlo configuration\n","    print(\"\\nMonte Carlo configuration:\")\n","    print(\"1. basic - Standard uniform sampling\")\n","    print(\"2. quasi - Quasi-Monte Carlo with Sobol sequences (default)\")\n","    print(\"3. importance - Importance sampling based on integrand values\")\n","    print(\"4. adaptive - Adaptive refinement of sampling in high-contribution regions\")\n","\n","    try:\n","        mc_method_idx = input(\"\\nSelect Monte Carlo method (1-4, default=2): \").strip()\n","        mc_methods = ['basic', 'quasi', 'importance', 'adaptive']\n","\n","        if mc_method_idx == '':\n","            mc_method = 'quasi'  # Default\n","        else:\n","            try:\n","                mc_method = mc_methods[int(mc_method_idx) - 1]\n","            except (ValueError, IndexError):\n","                print(\"Invalid selection, using quasi-Monte Carlo\")\n","                mc_method = 'quasi'\n","    except Exception:\n","        mc_method = 'quasi'\n","        print(\"Invalid input, using quasi-Monte Carlo\")\n","\n","    try:\n","        n_samples_input = input(\"\\nNumber of Monte Carlo samples (default: 100000): \").strip()\n","        if n_samples_input == '':\n","            n_samples = 100000  # Default\n","        else:\n","            n_samples = int(n_samples_input)\n","            if n_samples <= 0:\n","                n_samples = 100000\n","                print(\"Invalid number of samples, using default: 100000\")\n","    except Exception:\n","        n_samples = 100000\n","        print(\"Invalid input, using default: 100000 samples\")\n","\n","    print(f\"\\nUsing {mc_method} Monte Carlo with {n_samples} samples\")\n","\n","    # Run benchmarks for both function types\n","    for func in [ 'neg_entropy']:\n","        try:\n","            print(f\"\\n=== Running benchmark for {func} ===\")\n","\n","            # Run comprehensive benchmark\n","            results_df = run_comprehensive_benchmark(\n","                func_name=func,\n","                dimensions=dimensions,\n","                eps_values=eps_values,\n","                columns_to_display=columns_to_display,\n","                mc_method=mc_method,\n","                n_samples=n_samples\n","            )\n","\n","            # Store results\n","            all_results[func] = results_df\n","\n","            # Save results to CSV\n","            results_df.to_csv(f'{func}_mc_{mc_method}_N{n_samples}_results.csv', index=False)\n","\n","            # Free memory\n","            free_memory()\n","\n","        except Exception as e:\n","            print(f\"Error running benchmark for {func}: {e}\")\n","\n","    # Create combined visualization for convergence rates\n","    try:\n","        plt.figure(figsize=(12, 8))\n","\n","        colors = {\n","            'neg_log': 'blue',\n","            'neg_entropy': 'red'\n","        }\n","\n","        markers = {\n","            1: 'o', 2: '^', 3: 's', 4: 'D', 5: 'p',\n","            6: '*', 7: 'X', 8: 'h', 9: '+', 10: 'x'\n","        }\n","\n","        for func, results_df in all_results.items():\n","            # Group by dimension\n","            for d, group in results_df.groupby('d'):\n","                if len(group) < 2:  # Skip if not enough points for a line\n","                    continue\n","\n","                # Extract values\n","                eps_values = group['eps'].values\n","                # Use max_validation instead of rmse for convergence plot\n","                err_values = group['max_validation'].values\n","\n","                # Skip if any error is NaN\n","                if np.any(np.isnan(err_values)):\n","                    continue\n","\n","                # Plot convergence\n","                label = f\"{func.capitalize()}, d={d}, {mc_method} MC\"\n","                plt.loglog(eps_values, err_values,\n","                           marker=markers.get(d, 'o'),\n","                           color=colors.get(func, 'black'),\n","                           label=label, alpha=0.8)\n","\n","        # Add reference slopes\n","        x_ref = np.array([min(eps_values), max(eps_values)])\n","        y_ref1 = 0.1 * x_ref  # O(ε)\n","        y_ref2 = 0.1 * x_ref**2  # O(ε²)\n","        y_ref_mc = 0.1 * np.ones_like(x_ref)  # O(1/√N), independent of ε\n","\n","        plt.loglog(x_ref, y_ref1, 'k--', label='O(ε)', alpha=0.5)\n","        plt.loglog(x_ref, y_ref2, 'k:', label='O(ε²)', alpha=0.5)\n","        plt.loglog(x_ref, y_ref_mc, 'k-.', label=f'O(1/√N), N={n_samples}', alpha=0.5)\n","\n","        plt.xlabel('ε (smoothing parameter)')\n","        plt.ylabel('Maximum Error (Validation Set)')\n","        plt.title(f'Convergence of Monte Carlo Approximation ({mc_method.capitalize()})')\n","        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","        plt.grid(True)\n","        plt.tight_layout()\n","\n","        plt.savefig(f'convergence_mc_{mc_method}_N{n_samples}.png', dpi=300)\n","        plt.close()\n","\n","    except Exception as e:\n","        print(f\"Error creating convergence visualization: {e}\")\n","\n","    # Free memory again\n","    free_memory()\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","metadata":{"id":"h3unxk6eBPSR"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1PlQbe8F12hKRVzn4xUkb1zN0DsbouOfc","timestamp":1747848610283}],"authorship_tag":"ABX9TyODRTexKyaUk6/1bkT3QRMZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}