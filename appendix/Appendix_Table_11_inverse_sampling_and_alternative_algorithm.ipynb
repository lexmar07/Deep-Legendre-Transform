{"cells":[{"cell_type":"markdown","source":["# Alternative NN Approaches to Computing Legendre Transform\n","\n","## Deep Legendre Transform (DLT) Method\n","\n","Our Deep Legendre Transform approximates the convex conjugate $f^*$ on $D = \\nabla f(C)$ by leveraging the exact Fenchel-Young identity. For any $x \\in C$ with $y = \\nabla f(x)$, we have $f^*(y) = \\langle x,y\\rangle - f(x)$ exactly.\n","\n","**DLT Loss Function:**\n","$$\\left[g_\\theta(\\nabla f(x)) + f(x) - \\langle x,\\nabla f(x)\\rangle\\right]^2$$\n","\n","This loss equals $(g_\\theta(y) - f^*(y))^2$ for points on the manifold, providing an immediate accuracy certificate.\n","\n","## Proxy Method\n","\n","The proxy approach uses an approximate inverse gradient:\n","$$f^*(y) \\approx \\langle \\Psi(y), y \\rangle - f(\\Psi(y)), \\quad \\text{where } \\Psi \\approx (\\nabla f)^{-1}$$\n","\n","This introduces bias: even with perfect optimization, the approximation error includes $t_\\Psi(y)-f^{*}(y)$, which is zero only when $\\Psi=(\\nabla f)^{-1}$.\n","\n","## Test Functions\n","\n","We evaluate both methods on three convex functions:\n","\n","**1. Quadratic Function**\n","- Expression: $f(x) = \\frac{1}{2} \\sum x_i^2$\n","- Domain: $C \\sim \\mathcal{N}(0,1)^d$\n","\n","**2. Negative Log Function**\n","- Expression: $f(x) = -\\sum \\log(x_i)$\n","- Domain: $C = \\exp(U[-2.3,2.3])^d$\n","\n","**3. Negative Entropy Function**\n","- Expression: $f(x) = \\sum x_i \\log x_i$\n","- Domain: $C = \\exp(U[-2.3,2.3])^d$\n","\n","Where $U[-2.3,2.3]$ denotes uniform distribution on $[-2.3, 2.3]$.\n","\n","## Key Advantages of DLT\n","\n","1. **Guaranteed convexity** with ICNN architecture\n","2. **No accuracy bottleneck** from intermediate approximations  \n","3. **Efficient sampling** using exact gradient mapping\n","4. **Lower training overhead** compared to proxy methods\n","\n","## Experimental Results\n","\n","**Table: DLT vs Proxy Method Comparison**\n","\n","| Function | $d$ | Method | RMSE | Training (s) |\n","|----------|-----|--------|------|--------------|\n","| Quadratic | 5 | DLT | 8.29e-03 | 113.95 |\n","| | | Proxy | 1.49e-02 | 104.60 |\n","| Quadratic | 10 | DLT | 2.02e-02 | 222.05 |\n","| | | Proxy | 2.10e-02 | 209.22 |\n","| Neg-Log | 5 | DLT | 3.98e-02 | 107.96 |\n","| | | Proxy | 4.21e-02 | 104.28 |\n","| Neg-Log | 10 | DLT | 1.28e-01 | 209.41 |\n","| | | Proxy | 1.45e-01 | 204.96 |\n","| Neg-Entropy | 5 | DLT | 3.27e-02 | 110.70 |\n","| | | Proxy | 3.25e-02 | 106.79 |\n","| Neg-Entropy | 10 | DLT | 3.53e-02 | 221.21 |\n","| | | Proxy | 3.64e-02 | 212.85 |\n","\n","Both methods use the same pre-trained inverse mapping $\\Psi$ (4-8 minutes pre-training). DLT consistently achieves comparable or better accuracy (RMSE) with similar training times, validating our theoretical advantages."],"metadata":{"id":"9IOAXzBrxD_E"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0E94EUZcpsB","executionInfo":{"status":"ok","timestamp":1761039736168,"user_tz":-120,"elapsed":4124094,"user":{"displayName":"Alexey Minabutdinov","userId":"01550160032560633467"}},"outputId":"f67e23f5-56cd-404a-aaef-34150a670543"},"outputs":[{"output_type":"stream","name":"stdout","text":["[region-check] quadratic d=5: P(Z<x_lo)=0.000000, P(Z>x_hi)=0.000000, minZ=-2.999965, maxZ=2.999935, C=[-3.000000,3.000000]\n","[region-check] quadratic d=10: P(Z<x_lo)=0.000000, P(Z>x_hi)=0.000000, minZ=-2.999968, maxZ=2.999967, C=[-3.000000,3.000000]\n","[region-check] neg_log d=5: P(Z<x_lo)=0.000000, P(Z>x_hi)=0.000000, minZ=0.100001, maxZ=4.997336, C=[0.100000,5.000000]\n","[region-check] neg_log d=10: P(Z<x_lo)=0.000000, P(Z>x_hi)=0.000000, minZ=0.100001, maxZ=4.998646, C=[0.100000,5.000000]\n","[region-check] neg_entropy d=5: P(Z<x_lo)=0.000000, P(Z>x_hi)=0.000000, minZ=0.100262, maxZ=9.973684, C=[0.100259,9.974182]\n","[region-check] neg_entropy d=10: P(Z<x_lo)=0.000000, P(Z>x_hi)=0.000000, minZ=0.100261, maxZ=9.973927, C=[0.100259,9.974182]\n","Function        d | method                 | model          | hidden       |  Nstep |   steps |  tPreInv |   tSolve |   tEval |  MB(act) |    train_MSE |   cert_MSE |  cert_RMSE |  f*_relL2\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["[PreInv-AE] quadratic d=5:  90%|█████████ | 180104/200000 [04:47<00:31, 625.81it/s, cyc=4.116e-06, dec=4.116e-06, tot=8.232e-06]\n"]},{"output_type":"stream","name":"stdout","text":["[stream-info] quadratic d=5 | baseN=600 | pre_batch=606 | DLT N=594, steps=30304, total=18000576 | Inv N=594, steps=30304, total=18000576\n","Quadratic       5 | DLT(stream+AE-inv)     | RESNET_SCALAR  | 128,128      |    594 |   30304 |   287.80 |   113.95 |    3.29 |      1.6 |    9.609e-05 |   6.87e-05 |   8.29e-03 |  9.89e-04\n","Quadratic       5 | InvGradProxy(stream)   | RESNET_SCALAR  | 128,128      |    594 |   30304 |   287.80 |   104.60 |    0.03 |      1.6 |    2.727e-04 |   2.21e-04 |   1.49e-02 |  1.90e-03\n"]},{"output_type":"stream","name":"stderr","text":["[PreInv-AE] quadratic d=10: 100%|██████████| 300000/300000 [07:48<00:00, 640.49it/s, cyc=3.702e-05, dec=3.702e-05, tot=7.405e-05]\n"]},{"output_type":"stream","name":"stdout","text":["[stream-info] quadratic d=10 | baseN=640 | pre_batch=647 | DLT N=633, steps=60607, total=38364231 | Inv N=633, steps=60607, total=38364231\n","Quadratic      10 | DLT(stream+AE-inv)     | RESNET_SCALAR  | 128,128      |    633 |   60607 |   468.39 |   222.05 |    0.64 |      1.7 |    4.101e-04 |   4.10e-04 |   2.02e-02 |  1.32e-03\n","Quadratic      10 | InvGradProxy(stream)   | RESNET_SCALAR  | 128,128      |    633 |   60607 |   468.39 |   209.22 |    0.03 |      1.7 |    3.871e-04 |   4.41e-04 |   2.10e-02 |  1.35e-03\n"]},{"output_type":"stream","name":"stderr","text":["[PreInv-AE] neg_log d=5:  70%|███████   | 140002/200000 [03:49<01:38, 609.44it/s, cyc=2.896e-06, dec=2.896e-06, tot=5.792e-06]\n"]},{"output_type":"stream","name":"stdout","text":["[stream-info] neg_log d=5 | baseN=600 | pre_batch=606 | DLT N=594, steps=30304, total=18000576 | Inv N=594, steps=30304, total=18000576\n","Neg. Log        5 | DLT(stream+AE-inv)     | RESNET_SCALAR  | 128,128      |    594 |   30304 |   229.72 |   107.96 |    0.03 |      1.6 |    1.053e-04 |   1.58e-03 |   3.98e-02 |  8.92e-04\n","Neg. Log        5 | InvGradProxy(stream)   | RESNET_SCALAR  | 128,128      |    594 |   30304 |   229.72 |   104.28 |    0.03 |      1.6 |    1.686e-04 |   1.77e-03 |   4.21e-02 |  8.28e-04\n"]},{"output_type":"stream","name":"stderr","text":["[PreInv-AE] neg_log d=10:  80%|████████  | 241062/300000 [06:30<01:35, 617.81it/s, cyc=4.013e-05, dec=4.013e-05, tot=8.025e-05]\n"]},{"output_type":"stream","name":"stdout","text":["[stream-info] neg_log d=10 | baseN=640 | pre_batch=647 | DLT N=633, steps=60607, total=38364231 | Inv N=633, steps=60607, total=38364231\n","Neg. Log       10 | DLT(stream+AE-inv)     | RESNET_SCALAR  | 128,128      |    633 |   60607 |   390.19 |   209.41 |    0.03 |      1.7 |    2.280e-04 |   1.63e-02 |   1.28e-01 |  6.94e-04\n","Neg. Log       10 | InvGradProxy(stream)   | RESNET_SCALAR  | 128,128      |    633 |   60607 |   390.19 |   204.96 |    0.03 |      1.7 |    2.591e-04 |   2.11e-02 |   1.45e-01 |  7.32e-04\n"]},{"output_type":"stream","name":"stderr","text":["[PreInv-AE] neg_entropy d=5: 100%|██████████| 200000/200000 [05:26<00:00, 612.66it/s, cyc=1.637e-05, dec=1.637e-05, tot=3.274e-05]\n"]},{"output_type":"stream","name":"stdout","text":["[stream-info] neg_entropy d=5 | baseN=600 | pre_batch=606 | DLT N=594, steps=30304, total=18000576 | Inv N=594, steps=30304, total=18000576\n","Neg. Entropy    5 | DLT(stream+AE-inv)     | RESNET_SCALAR  | 128,128      |    594 |   30304 |   326.45 |   110.70 |    0.03 |      1.6 |    9.870e-04 |   1.07e-03 |   3.27e-02 |  2.78e-03\n","Neg. Entropy    5 | InvGradProxy(stream)   | RESNET_SCALAR  | 128,128      |    594 |   30304 |   326.45 |   106.79 |    0.03 |      1.6 |    4.760e-04 |   1.06e-03 |   3.25e-02 |  2.72e-03\n"]},{"output_type":"stream","name":"stderr","text":["[PreInv-AE] neg_entropy d=10:  97%|█████████▋| 292387/300000 [07:54<00:12, 615.61it/s, cyc=9.413e-05, dec=9.413e-05, tot=1.883e-04]\n"]},{"output_type":"stream","name":"stdout","text":["[stream-info] neg_entropy d=10 | baseN=640 | pre_batch=647 | DLT N=633, steps=60607, total=38364231 | Inv N=633, steps=60607, total=38364231\n","Neg. Entropy   10 | DLT(stream+AE-inv)     | RESNET_SCALAR  | 128,128      |    633 |   60607 |   474.96 |   221.21 |    0.03 |      1.7 |    1.294e-03 |   1.24e-03 |   3.53e-02 |  1.57e-03\n","Neg. Entropy   10 | InvGradProxy(stream)   | RESNET_SCALAR  | 128,128      |    633 |   60607 |   474.96 |   212.85 |    0.03 |      1.7 |    1.036e-03 |   1.32e-03 |   3.64e-02 |  1.59e-03\n","\n","Wrote CSV: results_stream.csv  (12 rows)\n","Saved figures to: figs_stream\n"]}],"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Streaming DLT vs InvGrad-Proxy with approximate inverse Ψ_samp (AE-style).\n","- Fresh y~Unif(D) each step for both methods.\n","- Correct g(y) per test function, *domain-aware* (tight, interior-clipped).\n","- Safe AE cycle: project decoded x back to C before ∇f / g∘∇f (STE in pretrain).\n","- OOB handling in z=g(y)-space: drop (default) or penalty.\n","- Pretrain inverse on a *larger* set; DLT/InvGrad use 10% smaller streams by default.\n","- Optional step-compensation so total samples stay constant despite stream scaling.\n","- Region validator to confirm Z=g(Y) lies inside C (no silent projection distortions).\n","\n","No placeholders. Ready to run in Colab/Notebook or as a script.\n","\"\"\"\n","\n","import os, sys, time, math, argparse, subprocess\n","import numpy as np\n","from typing import Callable, Tuple, Dict, Sequence, List\n","from functools import partial\n","\n","# ---------- lightweight dependency bootstrap ----------\n","def _ensure(pkgs: List[str]):\n","    import importlib\n","    miss=[]\n","    for p in pkgs:\n","        try: importlib.import_module(p)\n","        except Exception: miss.append(p)\n","    if miss:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + miss)\n","\n","_ensure([\"flax\",\"optax\",\"pandas\",\"matplotlib\",\"tqdm\"])\n","\n","import jax, jax.numpy as jnp, optax\n","from jax import random\n","from flax import linen as nn\n","from flax.training import train_state\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import trange\n","\n","# ====================== 1) Test functions, domains, conjugates =================\n","def f_quad(x):    return 0.5*jnp.sum(x**2, -1)\n","def grad_quad(x): return x\n","def fst_quad(y):  return 0.5*jnp.sum(y**2, -1)\n","\n","def f_nlog(x):    return -jnp.sum(jnp.log(x), -1)                 # dom x>0\n","def grad_nlog(x): return -1.0/x\n","def fst_nlog(y):  return -jnp.sum(jnp.log(-y), -1) - y.shape[-1]  # dom y<0\n","\n","def f_nent(x):    return jnp.sum(x*jnp.log(x), -1)                # dom x>0\n","def grad_nent(x): return jnp.log(x)+1.0\n","def fst_nent(y):  return jnp.sum(jnp.exp(y-1.0), -1)\n","\n","FUNS = {\n","    \"quadratic\":   dict(f=f_quad,    g=grad_quad, fst=fst_quad,   printable=\"Quadratic\"),\n","    \"neg_log\":     dict(f=f_nlog,    g=grad_nlog, fst=fst_nlog,   printable=\"Neg. Log\"),\n","    \"neg_entropy\": dict(f=f_nent,    g=grad_nent, fst=fst_nent,   printable=\"Neg. Entropy\"),\n","}\n","\n","# IMPORTANT: D must be consistent with C via g(y)\n","DOMAINS_PAPER = {\n","    \"quadratic\":   dict(C=(-3.0,  3.0), D=(-3.0,  3.0)),\n","    \"neg_log\":     dict(C=( 0.1,  5.0), D=(-10.0, -0.2)),   # finite D upper bound near 0-\n","    \"neg_entropy\": dict(C=(math.exp(-2.3), math.exp(2.3)), D=(-1.3, 3.3)),  # y=log x + 1\n","}\n","\n","# ====================== 2) Domain-aware g(y) ==================================\n","def g_map(fn_key: str, domain, eps_rel: float = 1e-6) -> Callable[[jnp.ndarray], jnp.ndarray]:\n","    \"\"\"\n","    Returns g(y) consistent with the declared domain D for the function.\n","    We clip y to a tiny interior of D so z = g(y) lands inside C without\n","    projection-induced distortions.\n","    \"\"\"\n","    (y_lo, y_hi) = domain[fn_key][\"D\"]\n","    span = float(y_hi - y_lo)\n","    y_lo_i = float(y_lo) + eps_rel * span\n","    y_hi_i = float(y_hi) - eps_rel * span\n","\n","    if fn_key == \"quadratic\":\n","        def g_quad(y):\n","            y_safe = jnp.clip(y, y_lo_i, y_hi_i)\n","            return y_safe\n","        return g_quad\n","\n","    if fn_key == \"neg_log\":\n","        # g(y) = -1/y, with y strictly negative; we clip to declared D interior\n","        def g_neglog(y):\n","            y_safe = jnp.clip(y, y_lo_i, y_hi_i)  # e.g. [-10, -0.2] interior\n","            return -1.0 / y_safe\n","        return g_neglog\n","\n","    if fn_key == \"neg_entropy\":\n","        # g(y) = exp(y-1), with y = log x + 1; clip y to interior of declared D\n","        def g_nent(y):\n","            y_safe = jnp.clip(y, y_lo_i, y_hi_i)\n","            return jnp.exp(y_safe - 1.0)\n","        return g_nent\n","\n","    raise ValueError(f\"Unknown fn_key {fn_key}\")\n","\n","# ====================== 3) Models =============================================\n","def _act(name: str) -> Callable:\n","    n=name.lower()\n","    if n==\"relu\": return nn.relu\n","    if n==\"gelu\": return jax.nn.gelu\n","    if n==\"softplus\": return jax.nn.softplus\n","    raise ValueError(f\"Unknown activation {name}\")\n","\n","class ResBlock(nn.Module):\n","    features:int\n","    act: Callable\n","    @nn.compact\n","    def __call__(self, x):\n","        h = self.act(nn.Dense(self.features)(x))\n","        h = nn.Dense(self.features)(h)\n","        if x.shape[-1] != self.features:\n","            x = nn.Dense(self.features, use_bias=False)(x)\n","        return self.act(h + x)\n","\n","class ResNetScalar(nn.Module):\n","    hidden:Sequence[int]\n","    act: Callable = jax.nn.gelu\n","    @nn.compact\n","    def __call__(self, y):\n","        assert len(self.hidden) >= 1\n","        h = self.act(nn.Dense(self.hidden[0])(y))\n","        for w in self.hidden:\n","            h = ResBlock(w, act=self.act)(h)\n","        return jnp.squeeze(nn.Dense(1)(h), -1)\n","\n","class ResNetVec(nn.Module):\n","    hidden:Sequence[int]\n","    act: Callable = jax.nn.gelu\n","    out_dim:int = 1\n","    out_bias_init: float = 0.0  # initialize outputs near mid-C to avoid early clipping\n","    @nn.compact\n","    def __call__(self, z):\n","        assert len(self.hidden) >= 1\n","        h = self.act(nn.Dense(self.hidden[0])(z))\n","        for w in self.hidden:\n","            h = ResBlock(w, act=self.act)(h)\n","        return nn.Dense(\n","            self.out_dim,\n","            bias_init=nn.initializers.constant(self.out_bias_init)\n","        )(h)\n","\n","# ====================== 4) Training utilities =================================\n","class State(train_state.TrainState): ...\n","def schedule(lr): return optax.exponential_decay(lr, 20_000, 0.5, staircase=True)\n","\n","def new_scalar_state(rng, model, d, lr, wd=1e-6):\n","    params = model.init(rng, jnp.zeros((1,d), jnp.float32))[\"params\"]\n","    tx = optax.adamw(learning_rate=schedule(lr), weight_decay=wd)\n","    return State.create(apply_fn=model.apply, params=params, tx=tx)\n","\n","def new_vector_state(rng, model, d, lr, wd=1e-6):\n","    params = model.init(rng, jnp.zeros((1,d), jnp.float32))[\"params\"]\n","    tx = optax.adamw(learning_rate=schedule(lr), weight_decay=wd)\n","    return State.create(apply_fn=model.apply, params=params, tx=tx)\n","\n","def count_param_bytes(params):\n","    leaves = jax.tree_util.tree_leaves(params)\n","    return sum(np.prod(l.shape) * np.dtype(l.dtype).itemsize for l in leaves)\n","\n","def estimate_active_mem_mb(params, batch_elems: int, act_footprint: int):\n","    pbytes = count_param_bytes(params); adam=2*pbytes; grads=pbytes\n","    act_bytes = batch_elems * act_footprint * 4  # float32\n","    return (pbytes + adam + grads + act_bytes) / (1024**2)\n","\n","class Stopper:\n","    def __init__(self, pat:int, tol:float=1e-6):\n","        self.best=float(\"inf\"); self.pat=int(pat); self.tol=float(tol)\n","        self.cnt=0; self.bp=None\n","    def update(self, loss, params):\n","        lv=float(loss)\n","        if lv + self.tol < self.best:\n","            self.best, self.cnt, self.bp = lv, 0, params\n","        else:\n","            self.cnt += 1\n","        return self.cnt >= self.pat or self.best < self.tol\n","    def res(self): return self.best, self.bp\n","\n","def parse_hidden(s: str) -> Tuple[int,...]:\n","    return tuple(int(v) for v in s.split(\",\") if v)\n","\n","# ====================== 5) Samplers, projections, eval sets ====================\n","def make_samplers(domain, fn_key, x_loguniform: bool, eps_rel: float = 1e-6):\n","    (x_lo,x_hi) = domain[fn_key][\"C\"]\n","    (y_lo,y_hi) = domain[fn_key][\"D\"]\n","\n","    if fn_key in (\"neg_log\", \"neg_entropy\") and x_loguniform:\n","        if not (x_lo > 0 and x_hi > 0):\n","            raise ValueError(f\"{fn_key} requires x>0\")\n","        log_x_lo = float(math.log(x_lo)); log_x_hi = float(math.log(x_hi))\n","        def samp_x(key, sh):  # log-uniform in C (helps make y ~ uniform)\n","            return jnp.exp(random.uniform(key, sh, minval=log_x_lo, maxval=log_x_hi, dtype=jnp.float32))\n","    else:\n","        def samp_x(key, sh):  # uniform in C\n","            return random.uniform(key, sh, minval=float(x_lo), maxval=float(x_hi), dtype=jnp.float32)\n","\n","    # y ~ Unif(D) with relative interior margin, aligned with g_map\n","    span = float(y_hi - y_lo)\n","    lo_i = float(y_lo) + eps_rel * span\n","    hi_i = float(y_hi) - eps_rel * span\n","    def samp_y(key, sh):\n","        return random.uniform(key, sh, minval=lo_i, maxval=hi_i, dtype=jnp.float32)\n","\n","    return samp_x, samp_y\n","\n","def projector_C(fn_key, domain, ste: bool = False):\n","    \"\"\"\n","    Projection to the primal box C. If ste=True, use a Straight-Through Estimator:\n","      forward pass = clip to [lo, hi],\n","      backward pass = identity (non-zero gradient through saturation).\n","    \"\"\"\n","    lo, hi = domain[fn_key][\"C\"]\n","    lo = jnp.float32(lo); hi = jnp.float32(hi)\n","\n","    def proj_clip(x):\n","        return jnp.clip(x, lo, hi)\n","\n","    if not ste:\n","        return proj_clip\n","\n","    def proj_ste(x):\n","        x_clip = jnp.clip(x, lo, hi)\n","        return x + jax.lax.stop_gradient(x_clip - x)\n","\n","    return proj_ste\n","\n","def make_eval_Y(fn_key, d, n_eval, domain, eps_rel: float = 1e-6):\n","    (y_lo,y_hi)=domain[fn_key][\"D\"]\n","    span = float(y_hi - y_lo)\n","    lo = float(y_lo) + eps_rel*span\n","    hi = float(y_hi) - eps_rel*span\n","    return np.array(random.uniform(random.PRNGKey(1234+d), (n_eval, d), minval=lo, maxval=hi, dtype=jnp.float32))\n","\n","def make_cert_pairs(fn_key, d, n_cert, domain, samp_x, rng_seed: int):\n","    g = FUNS[fn_key][\"g\"]\n","    key = random.PRNGKey(rng_seed + d)\n","    X = samp_x(key, (int(n_cert), d))\n","    Y = g(X)\n","    return X, Y\n","\n","# ====================== 6) OOB handling in z=g(y) space =======================\n","def z_box_for_fn(fn_key: str, domain):\n","    x_lo, x_hi = domain[fn_key][\"C\"]\n","    return jnp.float32(x_lo), jnp.float32(x_hi)\n","\n","def compute_oob_mask_and_weights(fn_key: str, Y: jnp.ndarray, domain, eps_rel: float = 1e-6):\n","    g_of_y = g_map(fn_key, domain=domain, eps_rel=eps_rel)\n","    Z = g_of_y(Y)  # should be inside C almost always if D is consistent\n","    z_lo, z_hi = z_box_for_fn(fn_key, domain)\n","    in_lo = Z >= z_lo\n","    in_hi = Z <= z_hi\n","    valid = jnp.logical_and(jnp.all(in_lo, axis=-1), jnp.all(in_hi, axis=-1))\n","    w = valid.astype(jnp.float32)  # 1 for in-range, 0 otherwise\n","    oob = 1.0 - w\n","    return Z, w, oob\n","\n","# ====================== 7) AE pretraining (decoder) ===========================\n","class ResNetVecWithBias(ResNetVec):\n","    pass  # clarity alias\n","\n","def build_decoder(widths: Tuple[int,...], act_name: str, d: int, fn_key: str, domain):\n","    x_lo, x_hi = domain[fn_key][\"C\"]\n","    out_bias = float(0.5 * (x_lo + x_hi))  # start inside the box\n","    return ResNetVecWithBias(hidden=widths, act=_act(act_name), out_dim=d, out_bias_init=out_bias)\n","\n","def ae_losses(fn_key: str, f: Callable, gradf: Callable, g_of_y: Callable, projC: Callable):\n","    def loss_fn(params, apply_fn, x):\n","        z        = g_of_y(gradf(x))                # z = g(∇f(x))\n","        x_hat    = apply_fn({\"params\": params}, z) # decode (unconstrained)\n","        x_hat_pr = projC(x_hat)                    # project to C (STE during AE pretrain)\n","        z_hat    = g_of_y(gradf(x_hat_pr))         # cycle\n","        dec = jnp.mean(jnp.sum((x_hat_pr - x)**2, axis=-1))\n","        cyc = jnp.mean(jnp.sum((z_hat    - z)**2, axis=-1))\n","        return dec, cyc, dec + cyc\n","    return loss_fn\n","\n","@partial(jax.jit, static_argnums=(2,))\n","def ae_step(state, x_mb, loss_fn):\n","    def _loss(p):\n","        dec,cyc,tot = loss_fn(p, state.apply_fn, x_mb)\n","        return tot, (dec,cyc)\n","    (tot,(dec,cyc)), grads = jax.value_and_grad(_loss, has_aux=True)(state.params)\n","    return state.apply_gradients(grads=grads), tot, dec, cyc\n","\n","def pretrain_inverse_autoG(fn_key, d, widths_vec, act_name, samp_x, f, gradf,\n","                           steps: int, lr: float, batch_mb: int, patience, seed=11_001, progress=True, wd=1e-6, domain=None):\n","    g_of_y = g_map(fn_key, domain=domain)\n","    model  = build_decoder(widths_vec, act_name, d, fn_key, domain)\n","    key0   = random.PRNGKey(seed + d)\n","    state  = new_vector_state(key0, model, d, lr, wd=wd)\n","\n","    # STE projection to avoid dead gradients during AE training\n","    loss_fn = ae_losses(fn_key, f, gradf, g_of_y, projC=projector_C(fn_key, domain, ste=True))\n","    steps_num = int(steps)\n","    B = int(batch_mb)\n","    pat = int(patience) if patience != \"auto\" else max(5000, steps_num//5)\n","    stopper = Stopper(pat)\n","\n","    t0 = time.perf_counter()\n","    it = trange(steps_num, desc=f\"[PreInv-AE] {fn_key} d={d}\", disable=(not progress), mininterval=0.1)\n","    last_tot = 0.0\n","    for i in it:\n","        key_i = random.fold_in(key0, i)\n","        x_mb  = samp_x(key_i, (B, d))\n","        state, tot, dec, cyc = ae_step(state, x_mb, loss_fn)\n","        last_tot = float(tot)\n","        if progress and (i % 50 == 0):\n","            it.set_postfix(tot=f\"{float(tot):.3e}\", dec=f\"{float(dec):.3e}\", cyc=f\"{float(cyc):.3e}\")\n","        if stopper.update(tot, state.params): break\n","\n","    best_loss, best_params = stopper.res()\n","    if best_params is None: best_params = state.params\n","    t = time.perf_counter() - t0\n","\n","    mem_MB = estimate_active_mem_mb(best_params, batch_elems=B, act_footprint=(d + sum(widths_vec) + d))\n","    return dict(params=best_params, time=t, model=model, mem_MB=mem_MB, last_loss=float(last_tot))\n","\n","def make_preinv_apply(fn_key: str, preinv, domain):\n","    g_of_y = g_map(fn_key, domain=domain)\n","    proj   = projector_C(fn_key, domain, ste=False)  # hard clip at inference/use time\n","    @jax.jit\n","    def _apply(Y):\n","        Z = g_of_y(Y)\n","        Z = proj(Z)  # keep decoder input z in training range\n","        X = preinv[\"model\"].apply({\"params\": preinv[\"params\"]}, Z)\n","        return proj(X)  # decoded x clipped to C\n","    return _apply\n","\n","# ====================== 8) Weighted steps (drop / penalty) ====================\n","@jax.jit\n","def step_weighted(st, y_all, t_all, w):\n","    def _loss(p):\n","        pred  = st.apply_fn({\"params\": p}, y_all)\n","        err   = pred - t_all\n","        denom = jnp.maximum(jnp.sum(w), 1.0)\n","        return jnp.sum(w * err * err) / denom\n","    l, gr = jax.value_and_grad(_loss)(st.params)\n","    return st.apply_gradients(grads=gr), l\n","\n","@jax.jit\n","def step_weighted_penalty(st, y_all, t_all, w, oob_mask, lam):\n","    def _loss(p):\n","        pred  = st.apply_fn({\"params\": p}, y_all)\n","        err   = pred - t_all\n","        denom = jnp.maximum(jnp.sum(w), 1.0)\n","        mse = jnp.sum(w * err * err) / denom\n","        pen = lam * jnp.mean(oob_mask * (pred ** 2))\n","        return mse + pen\n","    l, gr = jax.value_and_grad(_loss)(st.params)\n","    return st.apply_gradients(grads=gr), l\n","\n","# ====================== 9) Streaming trainers (masked) ========================\n","def train_dlt_stream(d, widths, act_name, lr, steps, patience, N_stream,\n","                     samp_y, preinv_apply, f, gradf, seed: int, wd=1e-6,\n","                     early_stop=False, fn_key: str = \"\", domain=None,\n","                     oob_policy: str = \"drop\", oob_lambda: float = 1e-3, eps_rel: float = 1e-6):\n","    assert fn_key and domain is not None\n","    model = ResNetScalar(hidden=widths, act=_act(act_name))\n","    key0  = random.PRNGKey(seed + d)\n","    st    = new_scalar_state(key0, model, d, lr, wd=wd)\n","    stop  = Stopper(patience) if early_stop else None\n","\n","    last_loss = None\n","    t0 = time.perf_counter()\n","    for _ in range(int(steps)):\n","        key0, k = random.split(key0)\n","        y_unif  = samp_y(k, (N_stream, d))\n","        _, w, oob = compute_oob_mask_and_weights(fn_key, y_unif, domain, eps_rel=eps_rel)\n","        x_hat   = preinv_apply(y_unif)        # x in C\n","        y_true  = gradf(x_hat)\n","        target  = jnp.sum(x_hat * y_true, -1) - f(x_hat)\n","        if oob_policy == \"penalty\":\n","            st, loss = step_weighted_penalty(st, y_true, target, w, oob, jnp.float32(oob_lambda))\n","        else:\n","            st, loss = step_weighted(st, y_true, target, w)\n","        last_loss = float(loss)\n","        if stop and stop.update(loss, st.params): break\n","\n","    params = st.params if (not stop or stop.bp is None) else stop.bp\n","    tsolve = time.perf_counter() - t0\n","    mem_MB = estimate_active_mem_mb(params, batch_elems=N_stream, act_footprint=(d + sum(widths) + 1))\n","    return dict(params=params, model=model, train_loss=float(last_loss),\n","                tsolve=tsolve, mem_MB=mem_MB, batch_used=N_stream)\n","\n","def train_invproxy_stream(d, widths, act_name, lr, steps, patience, N_stream,\n","                          samp_y, preinv_apply, f, seed: int, wd=1e-6,\n","                          early_stop=False, fn_key: str = \"\", domain=None,\n","                          oob_policy: str = \"drop\", oob_lambda: float = 1e-3, eps_rel: float = 1e-6):\n","    assert fn_key and domain is not None\n","    model = ResNetScalar(hidden=widths, act=_act(act_name))\n","    key0  = random.PRNGKey(seed + 10_000 + d)\n","    st    = new_scalar_state(key0, model, d, lr, wd=wd)\n","    stop  = Stopper(patience) if early_stop else None\n","\n","    last_loss = None\n","    t0 = time.perf_counter()\n","    for _ in range(int(steps)):\n","        key0, k = random.split(key0)\n","        y_unif  = samp_y(k, (N_stream, d))\n","        _, w, oob = compute_oob_mask_and_weights(fn_key, y_unif, domain, eps_rel=eps_rel)\n","        x_hat   = preinv_apply(y_unif)\n","        target  = jnp.sum(x_hat * y_unif, -1) - f(x_hat)\n","        if oob_policy == \"penalty\":\n","            st, loss = step_weighted_penalty(st, y_unif, target, w, oob, jnp.float32(oob_lambda))\n","        else:\n","            st, loss = step_weighted(st, y_unif, target, w)\n","        last_loss = float(loss)\n","        if stop and stop.update(loss, st.params): break\n","\n","    params = st.params if (not stop or stop.bp is None) else stop.bp\n","    tsolve = time.perf_counter() - t0\n","    mem_MB = estimate_active_mem_mb(params, batch_elems=N_stream, act_footprint=(d + sum(widths) + 1))\n","    return dict(params=params, model=model, train_loss=float(last_loss),\n","                tsolve=tsolve, mem_MB=mem_MB, batch_used=N_stream)\n","\n","# ====================== 10) Metrics, N/steps rules, plotting ==================\n","def relative_l2(pred: np.ndarray, true: np.ndarray) -> float:\n","    denom = float(np.linalg.norm(true))\n","    if denom == 0.0: return float(\"nan\")\n","    return float(np.linalg.norm(pred - true) / denom)\n","\n","def cert_stats_from_g(g_vals: np.ndarray, x: np.ndarray, y: np.ndarray, f: Callable) -> Dict[str,float]:\n","    resid = g_vals + np.array(f(x)) - np.array(np.sum(x*y, axis=-1))\n","    mse = float(np.mean(resid**2)); rmse = float(np.sqrt(mse)); mxa = float(np.max(np.abs(resid)))\n","    return {\"cert_MSE\": mse, \"cert_RMSE\": rmse, \"cert_MAX\": mxa}\n","\n","def parse_dim_map(tokens: List[str]) -> Dict[int,int]:\n","    m: Dict[int,int] = {}\n","    for tok in tokens or []:\n","        d_str, v_str = tok.split(\":\")\n","        m[int(d_str)] = int(v_str)\n","    return m\n","\n","def resolve_steps(d: int, fallback_steps: str, steps_map_tokens: List[str]) -> int:\n","    m = parse_dim_map(steps_map_tokens)\n","    if d in m: return int(m[d])\n","    if fallback_steps == \"auto\":\n","        return 20000 if d <= 8 else 60000\n","    return int(fallback_steps)\n","\n","def compute_N_stream(d: int, args) -> int:\n","    if args.stream_N_mode == \"explicit\" and args.stream_N_explicit:\n","        m = parse_dim_map(args.stream_N_explicit)\n","        if d in m: return int(m[d])\n","    if args.stream_N_mode == \"linear\":\n","        return int(max(d, args.stream_N_linear_k * d))\n","    if d <= args.stream_N_switch_dim:\n","        return int(args.stream_N_small_mult * d)   # e.g., 5 -> 600\n","    else:\n","        return int(args.stream_N_large_mult * d)   # e.g., 20 -> 1280\n","\n","def resolve_pre_steps(d: int, pre_fallback: str, pre_steps_map_tokens: List[str]) -> int:\n","    m = parse_dim_map(pre_steps_map_tokens)\n","    if d in m: return int(m[d])\n","    if pre_fallback == \"auto\":\n","        return 100000 if d <= 8 else 300000\n","    return int(pre_fallback)\n","\n","def compute_stream_Ns(d: int, args):\n","    \"\"\"\n","    Returns:\n","      base_N: baseline stream size from compute_N_stream\n","      N_dlt:  scaled stream size for DLT\n","      N_inv:  scaled stream size for InvGradProxy\n","      pre_batch_eff: effective AE pretrain batch size (scaled from base_N if enabled)\n","    \"\"\"\n","    base = compute_N_stream(d, args)\n","    N_dlt = max(1, int(math.floor(base * float(args.dlt_stream_scale))))\n","    N_inv = max(1, int(math.floor(base * float(args.inv_stream_scale))))\n","    if args.pre_batch_from_stream and float(args.pre_batch_scale) > 0:\n","        pre_batch_eff = max(1, int(math.ceil(base * float(args.pre_batch_scale))))\n","    else:\n","        pre_batch_eff = int(args.pre_batch)\n","    return base, N_dlt, N_inv, pre_batch_eff\n","\n","def make_plots(df: pd.DataFrame, outdir: str):\n","    os.makedirs(outdir, exist_ok=True)\n","    if df.empty: return\n","    for (fn_key, d), g in df.groupby([\"fn_key\",\"d\"]):\n","        g = g.dropna(subset=[\"cert_RMSE\",\"t_solve\"])\n","        if g.empty: continue\n","        plt.figure()\n","        total_time = g[\"t_preinv\"] + g[\"t_solve\"]\n","        for meth in g[\"method\"].unique():\n","            mask = g[\"method\"]==meth\n","            plt.scatter(total_time[mask], g[\"cert_RMSE\"][mask], label=meth)\n","        plt.xscale(\"log\"); plt.yscale(\"log\")\n","        plt.xlabel(\"Total train time (s)  [preinv + train]\")\n","        plt.ylabel(\"DLT certificate RMSE\")\n","        plt.title(f\"{fn_key} (d={d}) — CERT_RMSE vs time (stream)\")\n","        plt.legend(); plt.tight_layout()\n","        plt.savefig(os.path.join(outdir, f\"{fn_key}_d{d}_CERT_RMSE_vs_time.png\"), dpi=150)\n","        plt.close()\n","\n","# ====================== 11) Region consistency validator ======================\n","def validate_region_consistency(fn_key: str, d: int, domain, n: int = 20000, seed: int = 12345, eps_rel: float = 1e-6):\n","    \"\"\"\n","    Samples Y ~ Unif(D_interior), computes Z = g(Y), and checks Z ∈ C bounds.\n","    Prints violation rates and extreme values for quick inspection.\n","    \"\"\"\n","    key = random.PRNGKey(seed + d)\n","    (y_lo, y_hi) = domain[fn_key][\"D\"]\n","    (x_lo, x_hi) = domain[fn_key][\"C\"]\n","    span = float(y_hi - y_lo)\n","    lo_i = float(y_lo) + eps_rel * span\n","    hi_i = float(y_hi) - eps_rel * span\n","\n","    Y = random.uniform(key, (n, d), minval=lo_i, maxval=hi_i, dtype=jnp.float32)\n","    g = g_map(fn_key, domain=domain, eps_rel=eps_rel)\n","    Z = g(Y)\n","\n","    below = jnp.mean((Z < x_lo).astype(jnp.float32))\n","    above = jnp.mean((Z > x_hi).astype(jnp.float32))\n","    minz = float(jnp.min(Z))\n","    maxz = float(jnp.max(Z))\n","\n","    print(f\"[region-check] {fn_key} d={d}: \"\n","          f\"P(Z<x_lo)={float(below):.6f}, P(Z>x_hi)={float(above):.6f}, \"\n","          f\"minZ={minz:.6f}, maxZ={maxz:.6f}, C=[{float(x_lo):.6f},{float(x_hi):.6f}]\")\n","\n","# ====================== 12) CLI & Orchestration ===============================\n","def build_parser():\n","    P = argparse.ArgumentParser(add_help=True)\n","    P.add_argument(\"--dims\", nargs=\"+\", type=int, default=[5,20])\n","    P.add_argument(\"--n_eval\", type=int, default=5000)\n","    P.add_argument(\"--n_cert\", type=int, default=5000)\n","    P.add_argument(\"--domain_profile\", choices=[\"paper\"], default=\"paper\")\n","    P.add_argument(\"--eps_rel\", type=float, default=1e-6, help=\"Relative interior margin for D and samplers.\")\n","\n","    # Architectures\n","    P.add_argument(\"--resnet_scalar\", default=\"256,256\")\n","    P.add_argument(\"--resnet_vector\", default=\"256,256\")\n","    P.add_argument(\"--act_scalar\", default=\"gelu\", choices=[\"relu\",\"gelu\",\"softplus\"])\n","    P.add_argument(\"--act_vector\", default=\"gelu\", choices=[\"relu\",\"gelu\",\"softplus\"])\n","\n","    # Optim & regularization\n","    P.add_argument(\"--lr\", type=float, default=1e-3)\n","    P.add_argument(\"--wd\", type=float, default=1e-6)\n","    P.add_argument(\"--early_stop\", action=\"store_true\", default=False)\n","\n","    # Steps (per-dim maps)\n","    P.add_argument(\"--dlt_steps\", default=\"auto\")\n","    P.add_argument(\"--inv_steps\", default=\"auto\")\n","    P.add_argument(\"--steps_map\", nargs=\"*\", default=[\"5:30000\",\"20:100000\"])\n","\n","    P.add_argument(\"--pre_steps\", default=\"auto\")\n","    P.add_argument(\"--pre_steps_map\", nargs=\"*\", default=[\"5:200000\",\"20:400000\"])\n","    P.add_argument(\"--pre_batch\", type=int, default=64)\n","\n","    # Stream size per step (baseline)\n","    P.add_argument(\"--stream_N_mode\", choices=[\"piecewise\",\"linear\",\"explicit\"], default=\"piecewise\")\n","    P.add_argument(\"--stream_N_small_mult\", type=int, default=120)\n","    P.add_argument(\"--stream_N_large_mult\", type=int, default=64)\n","    P.add_argument(\"--stream_N_switch_dim\", type=int, default=8)\n","    P.add_argument(\"--stream_N_linear_k\",   type=int, default=120)\n","    P.add_argument(\"--stream_N_explicit\",   nargs=\"*\", default=[])\n","\n","    # Per-method scaling of stream sizes and pretrain batch\n","    P.add_argument(\"--dlt_stream_scale\", type=float, default=0.90,\n","                   help=\"Multiply baseline stream N for DLT (e.g., 0.90 = 10% smaller).\")\n","    P.add_argument(\"--inv_stream_scale\", type=float, default=0.90,\n","                   help=\"Multiply baseline stream N for InvGradProxy (e.g., 0.90 = 10% smaller).\")\n","    P.add_argument(\"--pre_batch_from_stream\", dest=\"pre_batch_from_stream\",\n","                   action=\"store_true\", default=True,\n","                   help=\"If True, set AE pretrain batch = ceil(pre_batch_scale * baseline stream N).\")\n","    P.add_argument(\"--no_pre_batch_from_stream\", dest=\"pre_batch_from_stream\",\n","                   action=\"store_false\",\n","                   help=\"If provided, use --pre_batch as a fixed batch size.\")\n","    P.add_argument(\"--pre_batch_scale\", type=float, default=1.10,\n","                   help=\"Scale factor for AE pretrain batch vs. baseline stream N (if pre_batch_from_stream=True).\")\n","\n","    # Keep total samples constant by compensating steps for stream scaling\n","    P.add_argument(\"--compensate_steps_for_scale\", dest=\"compensate_steps_for_scale\",\n","                   action=\"store_true\", default=True,\n","                   help=\"Increase steps by 1/scale so steps*N_stream stays constant.\")\n","    P.add_argument(\"--no_compensate_steps_for_scale\", dest=\"compensate_steps_for_scale\",\n","                   action=\"store_false\",\n","                   help=\"Disable step compensation (total samples decrease with scale).\")\n","\n","    # Sampling helpers (RESTORED FLAGS)\n","    P.add_argument(\"--x_loguniform\", dest=\"x_loguniform\", action=\"store_true\", default=True,\n","                   help=\"Sample x log-uniformly in C for positive-domain functions.\")\n","    P.add_argument(\"--no_x_loguniform\", dest=\"x_loguniform\", action=\"store_false\",\n","                   help=\"Disable log-uniform x sampling.\")\n","\n","    # OOB policy in z=g(y) space\n","    P.add_argument(\"--oob_policy\", choices=[\"drop\",\"penalty\"], default=\"drop\",\n","                   help=\"Disregard (drop) or penalize OOB z=g(y) samples.\")\n","    P.add_argument(\"--oob_penalty_weight\", type=float, default=1e-3,\n","                   help=\"λ for penalty term if --oob_policy=penalty\")\n","\n","    # Region validator\n","    P.add_argument(\"--validate_regions\", action=\"store_true\", default=False,\n","                   help=\"Run region consistency checks before training.\")\n","\n","    # Output\n","    P.add_argument(\"--csv\", default=\"results_stream.csv\")\n","    P.add_argument(\"--outdir\", default=\"figs_stream\")\n","    return P\n","\n","def main(argv=None):\n","    parser = build_parser()\n","    if argv is None: argv = sys.argv[1:]\n","    args, _ = parser.parse_known_args(argv)\n","\n","    domain = DOMAINS_PAPER\n","    widths_scalar = parse_hidden(args.resnet_scalar)\n","    widths_vector = parse_hidden(args.resnet_vector)\n","\n","    if args.validate_regions:\n","        for fn_key in [\"quadratic\",\"neg_log\",\"neg_entropy\"]:\n","            for d in args.dims:\n","                validate_region_consistency(fn_key, d, domain, n=20000, seed=2025, eps_rel=args.eps_rel)\n","\n","    rows=[]\n","    hdr = (\n","        f\"{'Function':<12} {'d':>4} | {'method':<22} | {'model':<14} | {'hidden':<12} | \"\n","        f\"{'Nstep':>6} | {'steps':>7} | {'tPreInv':>8} | {'tSolve':>8} | {'tEval':>7} | \"\n","        f\"{'MB(act)':>8} | {'train_MSE':>12} | {'cert_MSE':>10} | {'cert_RMSE':>10} | {'f*_relL2':>9}\"\n","    )\n","    print(hdr); print(\"-\"*len(hdr))\n","\n","    for fn_key in [\"quadratic\",\"neg_log\",\"neg_entropy\"]:\n","        f   = FUNS[fn_key][\"f\"]; gradf = FUNS[fn_key][\"g\"]; fst = FUNS[fn_key][\"fst\"]\n","        samp_x, samp_y = make_samplers(domain, fn_key, args.x_loguniform, eps_rel=args.eps_rel)\n","\n","        for d in args.dims:\n","            # Per-dim sizes (baseline + scaled per method)\n","            base_N, N_dlt, N_inv, pre_batch_eff = compute_stream_Ns(d, args)\n","\n","            # Resolve steps and optionally compensate by 1/scale\n","            steps_dlt = resolve_steps(d, args.dlt_steps, args.steps_map)\n","            steps_inv = resolve_steps(d, args.inv_steps, args.steps_map)\n","            if args.compensate_steps_for_scale:\n","                steps_dlt = int(math.ceil(steps_dlt / max(float(args.dlt_stream_scale), 1e-9)))\n","                steps_inv = int(math.ceil(steps_inv / max(float(args.inv_stream_scale), 1e-9)))\n","\n","            # ---- Stage 0: pretrain Ψ_samp with AE-style inverse (correct g) ----\n","            pre_steps = resolve_pre_steps(d, args.pre_steps, args.pre_steps_map)\n","            preinv = pretrain_inverse_autoG(\n","                fn_key, d, widths_vector, args.act_vector,\n","                samp_x, f, gradf, steps=pre_steps, lr=args.lr, batch_mb=pre_batch_eff,\n","                patience=\"auto\", seed=10_001 + hash(fn_key)%999, progress=True, wd=args.wd, domain=domain\n","            )\n","            preinv_apply = make_preinv_apply(fn_key, preinv, domain)\n","\n","            # Print stream/steps summary so you can verify effective samples\n","            total_samples_dlt = N_dlt * steps_dlt\n","            total_samples_inv = N_inv * steps_inv\n","            print(f\"[stream-info] {fn_key} d={d} | baseN={base_N} | pre_batch={pre_batch_eff} | \"\n","                  f\"DLT N={N_dlt}, steps={steps_dlt}, total={total_samples_dlt} | \"\n","                  f\"Inv N={N_inv}, steps={steps_inv}, total={total_samples_inv}\")\n","\n","            # Shared eval/cert sets\n","            Y_eval = make_eval_Y(fn_key, d, args.n_eval, domain, eps_rel=args.eps_rel)\n","            X_cert, Y_cert = make_cert_pairs(fn_key, d, args.n_cert, domain, samp_x, rng_seed=9_000 + hash(fn_key)%9991)\n","\n","            # ---- DLT (streaming, exact identity) ----\n","            try:\n","                patience = 10**12 if not args.early_stop else max(5000, steps_dlt//5)\n","\n","                fit = train_dlt_stream(\n","                    d, widths_scalar, args.act_scalar, args.lr, steps_dlt, patience, N_dlt,\n","                    samp_y, preinv_apply, f, gradf, seed=20001, wd=args.wd, early_stop=args.early_stop,\n","                    fn_key=fn_key, domain=domain, oob_policy=args.oob_policy, oob_lambda=args.oob_penalty_weight,\n","                    eps_rel=args.eps_rel\n","                )\n","\n","                y_eval = jnp.asarray(Y_eval, jnp.float32)\n","                t0_eval = time.perf_counter()\n","                g_pred  = np.array(fit[\"model\"].apply({\"params\":fit[\"params\"]}, y_eval))\n","                t_eval  = time.perf_counter() - t0_eval\n","                fstar_true = np.array(fst(Y_eval))\n","                relL2 = relative_l2(g_pred, fstar_true)\n","\n","                g_on_cert = np.array(fit[\"model\"].apply({\"params\":fit[\"params\"]}, jnp.asarray(Y_cert, jnp.float32)))\n","                cert = cert_stats_from_g(g_on_cert, X_cert, Y_cert, f)\n","\n","                r_out = dict(\n","                    Function=FUNS[fn_key][\"printable\"], fn_key=fn_key, d=d,\n","                    method=\"DLT(stream+AE-inv)\", model=\"RESNET_SCALAR\", hidden=\",\".join(map(str, widths_scalar)),\n","                    N_stream=N_dlt, steps=int(steps_dlt),\n","                    t_preinv=float(preinv[\"time\"]), t_solve=float(fit[\"tsolve\"]), t_eval=float(t_eval),\n","                    mem_MB=float(fit[\"mem_MB\"]), train_MSE=float(fit[\"train_loss\"]),\n","                    cert_MSE=cert[\"cert_MSE\"], cert_RMSE=cert[\"cert_RMSE\"], fstar_relL2=relL2\n","                )\n","                rows.append(r_out)\n","                print(f\"{r_out['Function']:<12} {r_out['d']:>4} | {r_out['method']:<22} | {r_out['model']:<14} | \"\n","                      f\"{r_out['hidden']:<12} | {r_out['N_stream']:>6} | {r_out['steps']:>7} | \"\n","                      f\"{r_out['t_preinv']:>8.2f} | {r_out['t_solve']:>8.2f} | {r_out['t_eval']:>7.2f} | \"\n","                      f\"{r_out['mem_MB']:>8.1f} | {r_out['train_MSE']:>12.3e} | \"\n","                      f\"{r_out['cert_MSE']:>10.2e} | {r_out['cert_RMSE']:>10.2e} | {r_out['fstar_relL2']:>9.2e}\")\n","            except Exception as e:\n","                print(f\"[skip DLT(stream) {fn_key} d={d}] {e}\")\n","\n","            # ---- InvGrad-Proxy (streaming) ----\n","            try:\n","                patience = 10**12 if not args.early_stop else max(5000, steps_inv//5)\n","\n","                fit = train_invproxy_stream(\n","                    d, widths_scalar, args.act_scalar, args.lr, steps_inv, patience, N_inv,\n","                    samp_y, preinv_apply, f, seed=30001, wd=args.wd, early_stop=args.early_stop,\n","                    fn_key=fn_key, domain=domain, oob_policy=args.oob_policy, oob_lambda=args.oob_penalty_weight,\n","                    eps_rel=args.eps_rel\n","                )\n","\n","                y_eval = jnp.asarray(Y_eval, jnp.float32)\n","                t0_eval = time.perf_counter()\n","                g_pred  = np.array(fit[\"model\"].apply({\"params\":fit[\"params\"]}, y_eval))\n","                t_eval  = time.perf_counter() - t0_eval\n","                fstar_true = np.array(fst(Y_eval))\n","                relL2 = relative_l2(g_pred, fstar_true)\n","\n","                g_on_cert = np.array(fit[\"model\"].apply({\"params\":fit[\"params\"]}, jnp.asarray(Y_cert, jnp.float32)))\n","                cert = cert_stats_from_g(g_on_cert, X_cert, Y_cert, f)\n","\n","                r_out = dict(\n","                    Function=FUNS[fn_key][\"printable\"], fn_key=fn_key, d=d,\n","                    method=\"InvGradProxy(stream)\", model=\"RESNET_SCALAR\", hidden=\",\".join(map(str, widths_scalar)),\n","                    N_stream=N_inv, steps=int(steps_inv),\n","                    t_preinv=float(preinv[\"time\"]), t_solve=float(fit[\"tsolve\"]), t_eval=float(t_eval),\n","                    mem_MB=float(fit[\"mem_MB\"]), train_MSE=float(fit[\"train_loss\"]),\n","                    cert_MSE=cert[\"cert_MSE\"], cert_RMSE=cert[\"cert_RMSE\"], fstar_relL2=relL2\n","                )\n","                rows.append(r_out)\n","                print(f\"{r_out['Function']:<12} {r_out['d']:>4} | {r_out['method']:<22} | {r_out['model']:<14} | \"\n","                      f\"{r_out['hidden']:<12} | {r_out['N_stream']:>6} | {r_out['steps']:>7} | \"\n","                      f\"{r_out['t_preinv']:>8.2f} | {r_out['t_solve']:>8.2f} | {r_out['t_eval']:>7.2f} | \"\n","                      f\"{r_out['mem_MB']:>8.1f} | {r_out['train_MSE']:>12.3e} | \"\n","                      f\"{r_out['cert_MSE']:>10.2e} | {r_out['cert_RMSE']:>10.2e} | {r_out['fstar_relL2']:>9.2e}\")\n","            except Exception as e:\n","                print(f\"[skip InvGradProxy(stream) {fn_key} d={d}] {e}\")\n","\n","    df = pd.DataFrame(rows)\n","    df.to_csv(args.csv, index=False)\n","    print(f\"\\nWrote CSV: {args.csv}  ({len(df)} rows)\")\n","    try:\n","        os.makedirs(args.outdir, exist_ok=True)\n","        make_plots(df, args.outdir)\n","        print(f\"Saved figures to: {args.outdir}\")\n","    except Exception as e:\n","        print(f\"[plotting skipped] {e}\")\n","\n","if __name__==\"__main__\":\n","    if \"ipykernel\" in sys.modules or \"google.colab\" in sys.modules:\n","        main([\n","            \"--dims\",\"5\",\"10\",\n","            \"--steps_map\",\"5:30000\",\"20:100000\",\n","            \"--pre_steps_map\",\"5:200000\",\"20:400000\",\n","            \"--stream_N_mode\",\"explicit\",\"--stream_N_explicit\",\"5:600\",\"20:1280\",\n","            \"--domain_profile\",\"paper\",\n","            \"--resnet_scalar\",\"128,128\",\n","            \"--resnet_vector\",\"128,128\",\n","            \"--act_scalar\",\"gelu\",\"--act_vector\",\"gelu\",\n","            \"--lr\",\"1e-3\",\"--wd\",\"1e-6\",\n","            \"--oob_policy\",\"penalty\",\n","            \"--oob_penalty_weight\",\"1e-3\",\n","            \"--n_eval\",\"5000\",\"--n_cert\",\"5000\",\n","            \"--csv\",\"results_stream.csv\",\"--outdir\",\"figs_stream\",\n","            # Larger pretrain, smaller downstream streams\n","            \"--dlt_stream_scale\",\"0.99\",\n","            \"--inv_stream_scale\",\"0.99\",\n","            \"--pre_batch_from_stream\",\n","            \"--pre_batch_scale\",\"1.01\",\n","            # Keep total samples constant despite smaller streams\n","            \"--compensate_steps_for_scale\",\n","            # Region checks + tight interior clipping\n","            \"--validate_regions\",\n","            \"--eps_rel\",\"1e-6\",\n","            # RESTORED: log-uniform x sampling flags usable from CLI\n","            \"--x_loguniform\"\n","        ])\n","    else:\n","        main(None)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_PlX-FFdJq_"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"authorship_tag":"ABX9TyNrYryjxY3CfPbyGDZAgLQQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}