{"cells":[{"cell_type":"markdown","metadata":{"id":"cI9MKtoFWn5B"},"source":["# Experiments with Non-separable Functions\n","\n","## Performance on Non-separable Functions\n","\n","We evaluate DLT on challenging non-separable convex functions where classical methods struggle. Unlike separable functions where $f(x) = \\sum_i f_i(x_i)$, these functions have coupled variables requiring the network to learn complex interactions.\n","\n","## Test Functions and Domains\n","\n","### 1. Quadratic with Random SPD Matrix\n","- **Function**: $f(x) = \\frac{1}{2}x^\\top Q x$ where $Q$ is random SPD with $\\kappa \\approx 10^2$\n","- **Domain $C$**: $\\mathcal{N}(0,1)^d$ (sampled as $\\approx[-3,3]^d$)\n","- **Dual Domain $D$**: $\\approx[-3,3]^d$\n","\n","### 2. Exponential-minus-Linear  \n","- **Function**: $f(x) = e^{\\langle a,x\\rangle} - \\langle b,x\\rangle$ where $\\|a\\|=\\|b\\|=1$\n","- **Domain $C$**: $\\mathcal{N}(0,1)^d$ (sampled as $\\approx[-3,3]^d$)\n","- **Dual Domain $D$**: $\\approx[e^{-3\\sqrt{d}}, e^{3\\sqrt{d}}]^d$\n","\n","### 3. Pre-trained ICNN\n","- **Function**: $f(x) = \\text{ICNN}_{2\\text{-layer}}(x)$ with 128 hidden units\n","- **Domain $C$**: $\\approx[-3s,3s]^d$ where $s \\in [0.1,10]$ (auto-scaled)\n","- **Dual Domain $D$**: $\\approx[-3,3]^d$\n","\n","### 4. Coupled Soft-plus\n","- **Function**: $f(x) = \\sum_{i<j}\\log(1+e^{x_i+x_j})$ with $O(d^2)$ interactions\n","- **Domain $C$**: $[-1.5,1.5]^d$ (uniform)\n","- **Dual Domain $D$**: $\\approx[0, d-1]^d$\n","\n","### Function Details:\n","\n","1. **Quadratic SPD**: Non-separable through random positive definite matrix $Q$ with controlled condition number\n","2. **Exp-Minus-Linear**: Sampled unit vectors $a, b \\sim \\mathcal{N}(0, I_d)$ normalized to unit sphere\n","3. **Pre-trained ICNN**: 2-layer network (128 units/layer) frozen after training on quadratic target\n","4. **Coupled Soft-plus**: Explicitly couples all variable pairs with $\\binom{d}{2}$ interaction terms\n","\n","## Experimental Results\n","\n","**Table: DLT Performance on Non-separable Functions (ResNet, 5 trials)**\n","\n","| Function | $d=20$ RMSE | $d=20$ Time (s) | $d=50$ RMSE | $d=50$ Time (s) |\n","|----------|-------------|-----------------|-------------|-----------------|\n","| Quadratic SPD | 7.37e-3 ± 7.0e-3 | 516 ± 1 | 4.04e-3 ± 2.5e-3 | 1336 ± 0.1 |\n","| Exp-minus-Linear | 1.01e-1 ± 9.6e-2 | 530 ± 2 | 3.01e-2 ± 1.6e-2 | 1333 ± 0.4 |\n","| 2-layer ICNN | 1.30e-3 ± 9.0e-4 | 609 ± 2 | 2.69e-4 ± 1.0e-5 | 1361 ± 0.5 |\n","| Coupled Soft-plus | 8.19e-3 ± 7.3e-3 | 1126 ± 5 | 2.37e-1 ± 2.2e-2 | 2857 ± 3 |\n","\n","**Architecture**: ResNet with two blocks of size 128\n","\n","**Key Finding**: DLT maintains consistent accuracy across all non-separable function types, successfully learning complex variable interactions that classical methods cannot handle."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDqr7hgFOR3T","executionInfo":{"status":"ok","timestamp":1761030289507,"user_tz":-120,"elapsed":3939556,"user":{"displayName":"Alexey Minabutdinov","userId":"01550160032560633467"}},"outputId":"56b46e65-7d1e-449d-bc41-83a725ff0a6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda, torch 2.8.0+cu126\n","\n","============================ TRIAL 1/5 (seed=42) ============================\n","\n","================= quad_spd — d=10 =================\n","Q stats: ||Q||₂≈1.010, κ=69.94, trace=3.0 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=1.864e-04 | eval RMSE=2.007e-02 (rel=2.016e-02, tstd=9.951e-01) *best\n","[ 10.0%] train loss=3.698e-05 | eval RMSE=8.777e-03 (rel=8.918e-03, tstd=9.842e-01) *best\n","[ 15.0%] train loss=1.130e-04 | eval RMSE=8.545e-03 (rel=8.804e-03, tstd=9.706e-01) *best\n","[ 20.0%] train loss=4.545e-05 | eval RMSE=1.386e-02 (rel=1.427e-02, tstd=9.714e-01)\n","[ 25.0%] train loss=6.986e-05 | eval RMSE=1.356e-02 (rel=1.348e-02, tstd=1.006e+00)\n","[ 30.0%] train loss=2.578e-05 | eval RMSE=4.236e-03 (rel=4.452e-03, tstd=9.513e-01) *best\n","[ 35.0%] train loss=4.257e-05 | eval RMSE=6.179e-03 (rel=6.274e-03, tstd=9.848e-01)\n","[ 40.0%] train loss=5.833e-05 | eval RMSE=4.711e-03 (rel=4.821e-03, tstd=9.771e-01)\n","[ 45.0%] train loss=5.142e-06 | eval RMSE=2.713e-03 (rel=2.798e-03, tstd=9.696e-01) *best\n","[ 50.0%] train loss=8.283e-06 | eval RMSE=2.325e-03 (rel=2.355e-03, tstd=9.876e-01) *best\n","[ 55.0%] train loss=1.391e-05 | eval RMSE=5.917e-03 (rel=6.244e-03, tstd=9.476e-01)\n","[ 60.0%] train loss=4.894e-06 | eval RMSE=4.258e-03 (rel=4.374e-03, tstd=9.735e-01)\n","[ 65.0%] train loss=2.408e-05 | eval RMSE=4.845e-03 (rel=5.031e-03, tstd=9.630e-01)\n","[ 70.0%] train loss=7.857e-07 | eval RMSE=1.649e-03 (rel=1.721e-03, tstd=9.580e-01) *best\n","[ 75.0%] train loss=6.063e-06 | eval RMSE=2.698e-03 (rel=2.679e-03, tstd=1.007e+00)\n","[ 80.0%] train loss=3.604e-06 | eval RMSE=8.258e-04 (rel=8.346e-04, tstd=9.894e-01) *best\n","[ 85.0%] train loss=8.481e-06 | eval RMSE=3.491e-03 (rel=3.533e-03, tstd=9.879e-01)\n","[ 90.0%] train loss=2.366e-06 | eval RMSE=2.378e-03 (rel=2.493e-03, tstd=9.536e-01)\n","[ 95.0%] train loss=3.070e-06 | eval RMSE=2.518e-03 (rel=2.562e-03, tstd=9.827e-01)\n","[100.0%] train loss=1.388e-05 | eval RMSE=4.520e-03 (rel=4.558e-03, tstd=9.918e-01)\n","Final: RMSE=1.345e-02 (rel=1.381e-02) | time=296.7s\n","\n","================= quad_spd — d=20 =================\n","Q stats: ||Q||₂≈1.010, κ=93.90, trace=6.1 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=3.122e-04 | eval RMSE=3.011e-02 (rel=2.190e-02, tstd=1.375e+00) *best\n","[ 10.0%] train loss=9.132e-05 | eval RMSE=1.852e-02 (rel=1.323e-02, tstd=1.400e+00) *best\n","[ 15.0%] train loss=2.086e-05 | eval RMSE=4.818e-03 (rel=3.458e-03, tstd=1.393e+00) *best\n","[ 20.0%] train loss=1.354e-05 | eval RMSE=6.143e-03 (rel=4.442e-03, tstd=1.383e+00)\n","[ 25.0%] train loss=1.420e-05 | eval RMSE=7.287e-03 (rel=5.198e-03, tstd=1.402e+00)\n","[ 30.0%] train loss=2.499e-05 | eval RMSE=4.027e-03 (rel=2.925e-03, tstd=1.377e+00) *best\n","[ 35.0%] train loss=1.053e-05 | eval RMSE=8.237e-03 (rel=5.954e-03, tstd=1.383e+00)\n","[ 40.0%] train loss=1.023e-05 | eval RMSE=9.556e-03 (rel=6.776e-03, tstd=1.410e+00)\n","[ 45.0%] train loss=1.564e-06 | eval RMSE=3.878e-03 (rel=2.785e-03, tstd=1.392e+00) *best\n","[ 50.0%] train loss=7.266e-06 | eval RMSE=3.690e-03 (rel=2.699e-03, tstd=1.367e+00) *best\n","[ 55.0%] train loss=1.017e-06 | eval RMSE=1.885e-03 (rel=1.341e-03, tstd=1.405e+00) *best\n","[ 60.0%] train loss=1.128e-06 | eval RMSE=3.184e-03 (rel=2.252e-03, tstd=1.414e+00)\n","[ 65.0%] train loss=1.088e-05 | eval RMSE=6.888e-03 (rel=4.977e-03, tstd=1.384e+00)\n","[ 70.0%] train loss=4.930e-07 | eval RMSE=4.560e-03 (rel=3.264e-03, tstd=1.397e+00)\n","[ 75.0%] train loss=4.023e-07 | eval RMSE=1.324e-03 (rel=9.584e-04, tstd=1.381e+00) *best\n","[ 80.0%] train loss=5.911e-07 | eval RMSE=1.286e-03 (rel=9.159e-04, tstd=1.405e+00) *best\n","[ 85.0%] train loss=1.662e-06 | eval RMSE=9.898e-04 (rel=7.135e-04, tstd=1.387e+00) *best\n","[ 90.0%] train loss=1.935e-06 | eval RMSE=2.147e-03 (rel=1.544e-03, tstd=1.391e+00)\n","[ 95.0%] train loss=3.824e-06 | eval RMSE=2.927e-03 (rel=2.122e-03, tstd=1.379e+00)\n","[100.0%] train loss=3.868e-07 | eval RMSE=1.343e-03 (rel=9.549e-04, tstd=1.406e+00)\n","Final: RMSE=5.813e-03 (rel=4.190e-03) | time=515.8s\n","\n","================= quad_spd — d=50 =================\n","Q stats: ||Q||₂≈1.010, κ=100.11, trace=14.8 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=9.434e-05 | eval RMSE=3.581e-02 (rel=1.754e-02, tstd=2.042e+00) *best\n","[ 10.0%] train loss=7.606e-06 | eval RMSE=1.624e-02 (rel=8.027e-03, tstd=2.024e+00) *best\n","[ 15.0%] train loss=1.488e-04 | eval RMSE=3.974e-02 (rel=1.969e-02, tstd=2.018e+00)\n","[ 20.0%] train loss=1.173e-04 | eval RMSE=6.857e-02 (rel=3.344e-02, tstd=2.050e+00)\n","[ 25.0%] train loss=6.439e-06 | eval RMSE=9.719e-03 (rel=4.758e-03, tstd=2.043e+00) *best\n","[ 30.0%] train loss=9.258e-06 | eval RMSE=1.078e-02 (rel=5.272e-03, tstd=2.045e+00)\n","[ 35.0%] train loss=5.075e-06 | eval RMSE=4.332e-03 (rel=2.100e-03, tstd=2.063e+00) *best\n","[ 40.0%] train loss=1.565e-06 | eval RMSE=3.632e-03 (rel=1.782e-03, tstd=2.038e+00) *best\n","[ 45.0%] train loss=4.366e-06 | eval RMSE=2.418e-03 (rel=1.198e-03, tstd=2.018e+00) *best\n","[ 50.0%] train loss=9.643e-06 | eval RMSE=3.086e-03 (rel=1.511e-03, tstd=2.043e+00)\n","[ 55.0%] train loss=1.517e-06 | eval RMSE=1.799e-03 (rel=8.967e-04, tstd=2.006e+00) *best\n","[ 60.0%] train loss=1.670e-06 | eval RMSE=2.621e-03 (rel=1.286e-03, tstd=2.039e+00)\n","[ 65.0%] train loss=9.598e-06 | eval RMSE=2.298e-03 (rel=1.130e-03, tstd=2.034e+00)\n","[ 70.0%] train loss=7.687e-07 | eval RMSE=3.627e-03 (rel=1.777e-03, tstd=2.041e+00)\n","[ 75.0%] train loss=4.322e-07 | eval RMSE=1.616e-03 (rel=8.000e-04, tstd=2.020e+00) *best\n","[ 80.0%] train loss=7.242e-07 | eval RMSE=2.690e-03 (rel=1.323e-03, tstd=2.034e+00)\n","[ 85.0%] train loss=1.609e-06 | eval RMSE=1.601e-03 (rel=7.866e-04, tstd=2.036e+00) *best\n","[ 90.0%] train loss=4.654e-07 | eval RMSE=2.548e-03 (rel=1.269e-03, tstd=2.008e+00)\n","[ 95.0%] train loss=7.668e-07 | eval RMSE=2.430e-03 (rel=1.201e-03, tstd=2.022e+00)\n","[100.0%] train loss=2.506e-06 | eval RMSE=4.476e-03 (rel=2.223e-03, tstd=2.013e+00)\n","Final: RMSE=5.021e-03 (rel=2.474e-03) | time=1336.3s\n","\n","================= exp_minus_lin — d=10 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=384, depth=4), params=1.19M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=3.777e-04 | eval RMSE=1.263e+00 (rel=2.776e-01, tstd=4.550e+00) *best\n","[ 10.0%] train loss=1.200e-03 | eval RMSE=1.426e+00 (rel=2.576e-01, tstd=5.534e+00)\n","[ 15.0%] train loss=8.920e-04 | eval RMSE=2.244e-01 (rel=6.032e-02, tstd=3.719e+00) *best\n","[ 20.0%] train loss=5.526e-05 | eval RMSE=3.489e-01 (rel=8.937e-02, tstd=3.905e+00)\n","[ 25.0%] train loss=1.426e-05 | eval RMSE=4.149e-02 (rel=1.085e-02, tstd=3.824e+00) *best\n","[ 30.0%] train loss=1.298e-05 | eval RMSE=2.544e-02 (rel=7.358e-03, tstd=3.457e+00) *best\n","[ 35.0%] train loss=3.264e-04 | eval RMSE=1.065e-01 (rel=2.747e-02, tstd=3.878e+00)\n","[ 40.0%] train loss=8.194e-04 | eval RMSE=1.591e-01 (rel=4.348e-02, tstd=3.659e+00)\n","[ 45.0%] train loss=1.552e-04 | eval RMSE=9.431e-02 (rel=2.721e-02, tstd=3.466e+00)\n","[ 50.0%] train loss=6.354e-04 | eval RMSE=1.469e-01 (rel=3.764e-02, tstd=3.902e+00)\n","[ 55.0%] train loss=2.239e-04 | eval RMSE=1.081e-01 (rel=3.002e-02, tstd=3.601e+00)\n","[ 60.0%] train loss=2.490e-04 | eval RMSE=9.118e-02 (rel=2.097e-02, tstd=4.347e+00)\n","[ 65.0%] train loss=1.364e-04 | eval RMSE=5.815e-02 (rel=1.537e-02, tstd=3.783e+00)\n","[ 70.0%] train loss=7.011e-05 | eval RMSE=4.152e-02 (rel=1.193e-02, tstd=3.482e+00)\n","[ 75.0%] train loss=4.068e-03 | eval RMSE=2.068e-01 (rel=4.553e-02, tstd=4.543e+00)\n","[ 80.0%] train loss=1.273e-05 | eval RMSE=1.007e-01 (rel=2.200e-02, tstd=4.576e+00)\n","[ 85.0%] train loss=9.208e-05 | eval RMSE=6.390e-02 (rel=1.443e-02, tstd=4.429e+00)\n","[ 90.0%] train loss=3.161e-04 | eval RMSE=6.294e-02 (rel=1.451e-02, tstd=4.339e+00)\n","[ 95.0%] train loss=2.510e-05 | eval RMSE=3.700e-02 (rel=1.108e-02, tstd=3.339e+00)\n","[100.0%] train loss=5.849e-05 | eval RMSE=5.240e-02 (rel=1.361e-02, tstd=3.849e+00)\n","Final: RMSE=1.576e-01 (rel=4.358e-02) | time=307.7s\n","\n","================= exp_minus_lin — d=20 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=512, depth=5), params=2.64M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=1.149e-04 | eval RMSE=3.702e-02 (rel=1.150e-02, tstd=3.218e+00) *best\n","[ 10.0%] train loss=7.640e-05 | eval RMSE=9.832e-02 (rel=2.373e-02, tstd=4.144e+00)\n","[ 15.0%] train loss=7.970e-06 | eval RMSE=3.289e-02 (rel=8.095e-03, tstd=4.063e+00) *best\n","[ 20.0%] train loss=4.680e-04 | eval RMSE=5.713e-02 (rel=1.663e-02, tstd=3.435e+00)\n","[ 25.0%] train loss=9.119e-06 | eval RMSE=6.603e-02 (rel=1.571e-02, tstd=4.203e+00)\n","[ 30.0%] train loss=2.828e-04 | eval RMSE=8.419e-02 (rel=2.344e-02, tstd=3.592e+00)\n","[ 35.0%] train loss=1.216e-05 | eval RMSE=1.515e-02 (rel=4.587e-03, tstd=3.302e+00) *best\n","[ 40.0%] train loss=7.999e-05 | eval RMSE=6.964e-02 (rel=1.870e-02, tstd=3.724e+00)\n","[ 45.0%] train loss=2.920e-04 | eval RMSE=8.822e-02 (rel=2.390e-02, tstd=3.691e+00)\n","[ 50.0%] train loss=3.219e-04 | eval RMSE=8.069e-02 (rel=2.175e-02, tstd=3.710e+00)\n","[ 55.0%] train loss=4.814e-05 | eval RMSE=3.812e-02 (rel=1.068e-02, tstd=3.570e+00)\n","[ 60.0%] train loss=1.791e-04 | eval RMSE=6.304e-02 (rel=1.915e-02, tstd=3.292e+00)\n","[ 65.0%] train loss=8.664e-05 | eval RMSE=4.606e-02 (rel=1.136e-02, tstd=4.054e+00)\n","[ 70.0%] train loss=6.227e-05 | eval RMSE=5.368e-02 (rel=1.470e-02, tstd=3.652e+00)\n","[ 75.0%] train loss=6.342e-04 | eval RMSE=1.653e-01 (rel=4.263e-02, tstd=3.878e+00)\n","[ 80.0%] train loss=1.418e-05 | eval RMSE=2.157e-02 (rel=5.912e-03, tstd=3.649e+00)\n","[ 85.0%] train loss=5.694e-06 | eval RMSE=1.337e-02 (rel=3.932e-03, tstd=3.400e+00) *best\n","[ 90.0%] train loss=2.241e-05 | eval RMSE=2.909e-02 (rel=6.874e-03, tstd=4.231e+00)\n","[ 95.0%] train loss=5.164e-04 | eval RMSE=1.180e-01 (rel=3.422e-02, tstd=3.448e+00)\n","[100.0%] train loss=2.554e-04 | eval RMSE=6.980e-02 (rel=2.135e-02, tstd=3.269e+00)\n","Final: RMSE=1.824e-02 (rel=5.710e-03) | time=531.3s\n","\n","================= exp_minus_lin — d=50 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=768, depth=6), params=7.11M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=1.884e-04 | eval RMSE=8.787e-02 (rel=2.033e-02, tstd=4.322e+00) *best\n","[ 10.0%] train loss=8.447e-05 | eval RMSE=5.345e-02 (rel=1.363e-02, tstd=3.922e+00) *best\n","[ 15.0%] train loss=1.153e-04 | eval RMSE=5.868e-02 (rel=1.760e-02, tstd=3.334e+00)\n","[ 20.0%] train loss=3.720e-05 | eval RMSE=4.598e-02 (rel=1.433e-02, tstd=3.208e+00) *best\n","[ 25.0%] train loss=1.797e-05 | eval RMSE=2.503e-02 (rel=7.413e-03, tstd=3.377e+00) *best\n","[ 30.0%] train loss=1.046e-05 | eval RMSE=2.662e-02 (rel=6.526e-03, tstd=4.079e+00)\n","[ 35.0%] train loss=8.931e-05 | eval RMSE=5.785e-02 (rel=1.832e-02, tstd=3.157e+00)\n","[ 40.0%] train loss=1.262e-04 | eval RMSE=5.450e-02 (rel=1.403e-02, tstd=3.885e+00)\n","[ 45.0%] train loss=2.184e-04 | eval RMSE=4.200e-02 (rel=1.272e-02, tstd=3.301e+00)\n","[ 50.0%] train loss=6.633e-06 | eval RMSE=1.153e-02 (rel=3.440e-03, tstd=3.350e+00) *best\n","[ 55.0%] train loss=1.538e-05 | eval RMSE=2.387e-02 (rel=6.779e-03, tstd=3.521e+00)\n","[ 60.0%] train loss=1.018e-05 | eval RMSE=2.082e-02 (rel=5.640e-03, tstd=3.692e+00)\n","[ 65.0%] train loss=5.924e-04 | eval RMSE=9.151e-02 (rel=2.810e-02, tstd=3.256e+00)\n","[ 70.0%] train loss=4.568e-04 | eval RMSE=8.924e-02 (rel=3.020e-02, tstd=2.955e+00)\n","[ 75.0%] train loss=1.968e-05 | eval RMSE=3.371e-02 (rel=9.403e-03, tstd=3.585e+00)\n","[ 80.0%] train loss=2.150e-05 | eval RMSE=2.575e-02 (rel=7.062e-03, tstd=3.646e+00)\n","[ 85.0%] train loss=1.575e-04 | eval RMSE=5.735e-02 (rel=1.855e-02, tstd=3.092e+00)\n","[ 90.0%] train loss=9.163e-05 | eval RMSE=5.282e-02 (rel=1.375e-02, tstd=3.842e+00)\n","[ 95.0%] train loss=1.724e-05 | eval RMSE=2.215e-02 (rel=6.719e-03, tstd=3.296e+00)\n","[100.0%] train loss=1.620e-05 | eval RMSE=2.325e-02 (rel=7.255e-03, tstd=3.205e+00)\n","Final: RMSE=4.432e-02 (rel=1.172e-02) | time=1333.5s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=84.47, ridge=0.01\n","[pre  20.0%] train MSE=3.013e+07 | eval RMSE=5.476e+03\n","[pre  40.0%] train MSE=2.010e+07 | eval RMSE=4.471e+03\n","[pre  60.0%] train MSE=1.552e+07 | eval RMSE=3.941e+03\n","[pre  80.0%] train MSE=1.390e+07 | eval RMSE=3.726e+03\n","[pre 100.0%] train MSE=1.361e+07 | eval RMSE=3.687e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=10 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=122.86 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=2.385e-04 | eval RMSE=1.855e-02 (rel=2.254e-02, tstd=8.230e-01) *best\n","[ 10.0%] train loss=4.104e-05 | eval RMSE=7.475e-03 (rel=9.204e-03, tstd=8.121e-01) *best\n","[ 15.0%] train loss=2.997e-05 | eval RMSE=7.771e-03 (rel=9.312e-03, tstd=8.345e-01)\n","[ 20.0%] train loss=1.260e-05 | eval RMSE=6.293e-03 (rel=7.587e-03, tstd=8.295e-01) *best\n","[ 25.0%] train loss=1.384e-05 | eval RMSE=2.729e-03 (rel=3.334e-03, tstd=8.186e-01) *best\n","[ 30.0%] train loss=6.512e-06 | eval RMSE=5.352e-03 (rel=6.513e-03, tstd=8.218e-01)\n","[ 35.0%] train loss=5.062e-06 | eval RMSE=2.412e-03 (rel=2.938e-03, tstd=8.207e-01) *best\n","[ 40.0%] train loss=4.596e-05 | eval RMSE=7.670e-03 (rel=9.324e-03, tstd=8.226e-01)\n","[ 45.0%] train loss=7.737e-06 | eval RMSE=2.780e-03 (rel=3.330e-03, tstd=8.350e-01)\n","[ 50.0%] train loss=9.509e-06 | eval RMSE=1.911e-03 (rel=2.316e-03, tstd=8.252e-01) *best\n","[ 55.0%] train loss=2.581e-06 | eval RMSE=2.507e-03 (rel=3.102e-03, tstd=8.084e-01)\n","[ 60.0%] train loss=2.126e-05 | eval RMSE=3.171e-03 (rel=3.860e-03, tstd=8.214e-01)\n","[ 65.0%] train loss=5.416e-06 | eval RMSE=1.845e-03 (rel=2.230e-03, tstd=8.273e-01) *best\n","[ 70.0%] train loss=1.137e-05 | eval RMSE=2.928e-03 (rel=3.607e-03, tstd=8.119e-01)\n","[ 75.0%] train loss=3.629e-06 | eval RMSE=2.301e-03 (rel=2.802e-03, tstd=8.212e-01)\n","[ 80.0%] train loss=1.516e-06 | eval RMSE=1.908e-03 (rel=2.283e-03, tstd=8.357e-01)\n","[ 85.0%] train loss=2.815e-06 | eval RMSE=2.440e-03 (rel=2.974e-03, tstd=8.202e-01)\n","[ 90.0%] train loss=1.383e-06 | eval RMSE=1.642e-03 (rel=2.010e-03, tstd=8.170e-01) *best\n","[ 95.0%] train loss=1.034e-05 | eval RMSE=3.295e-03 (rel=4.035e-03, tstd=8.165e-01)\n","[100.0%] train loss=8.571e-06 | eval RMSE=3.152e-03 (rel=3.905e-03, tstd=8.072e-01)\n","Final: RMSE=4.550e-03 (rel=5.560e-03) | time=356.6s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=100.98, ridge=0.01\n","[pre  20.0%] train MSE=2.486e+07 | eval RMSE=4.971e+03\n","[pre  40.0%] train MSE=1.632e+07 | eval RMSE=4.034e+03\n","[pre  60.0%] train MSE=1.258e+07 | eval RMSE=3.544e+03\n","[pre  80.0%] train MSE=1.123e+07 | eval RMSE=3.354e+03\n","[pre 100.0%] train MSE=1.101e+07 | eval RMSE=3.316e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=20 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=58.46 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=4.851e-05 | eval RMSE=2.885e-03 (rel=8.296e-03, tstd=3.477e-01) *best\n","[ 10.0%] train loss=1.814e-04 | eval RMSE=7.571e-03 (rel=2.183e-02, tstd=3.468e-01)\n","[ 15.0%] train loss=4.362e-06 | eval RMSE=1.126e-03 (rel=3.266e-03, tstd=3.449e-01) *best\n","[ 20.0%] train loss=6.207e-06 | eval RMSE=1.993e-03 (rel=5.842e-03, tstd=3.412e-01)\n","[ 25.0%] train loss=1.375e-05 | eval RMSE=2.147e-03 (rel=6.262e-03, tstd=3.428e-01)\n","[ 30.0%] train loss=3.520e-05 | eval RMSE=4.267e-03 (rel=1.217e-02, tstd=3.507e-01)\n","[ 35.0%] train loss=3.779e-05 | eval RMSE=2.462e-03 (rel=7.166e-03, tstd=3.436e-01)\n","[ 40.0%] train loss=1.276e-05 | eval RMSE=1.976e-03 (rel=5.795e-03, tstd=3.409e-01)\n","[ 45.0%] train loss=2.018e-05 | eval RMSE=1.503e-03 (rel=4.303e-03, tstd=3.494e-01)\n","[ 50.0%] train loss=1.640e-06 | eval RMSE=8.962e-04 (rel=2.599e-03, tstd=3.449e-01) *best\n","[ 55.0%] train loss=1.410e-06 | eval RMSE=1.281e-03 (rel=3.689e-03, tstd=3.473e-01)\n","[ 60.0%] train loss=7.374e-07 | eval RMSE=7.626e-04 (rel=2.195e-03, tstd=3.475e-01) *best\n","[ 65.0%] train loss=2.168e-06 | eval RMSE=7.260e-04 (rel=2.114e-03, tstd=3.435e-01) *best\n","[ 70.0%] train loss=2.334e-06 | eval RMSE=9.979e-04 (rel=2.891e-03, tstd=3.452e-01)\n","[ 75.0%] train loss=1.657e-06 | eval RMSE=4.493e-04 (rel=1.297e-03, tstd=3.464e-01) *best\n","[ 80.0%] train loss=4.553e-07 | eval RMSE=7.414e-04 (rel=2.141e-03, tstd=3.463e-01)\n","[ 85.0%] train loss=2.090e-06 | eval RMSE=7.255e-04 (rel=2.075e-03, tstd=3.497e-01)\n","[ 90.0%] train loss=6.576e-07 | eval RMSE=5.438e-04 (rel=1.561e-03, tstd=3.483e-01)\n","[ 95.0%] train loss=3.156e-07 | eval RMSE=3.731e-04 (rel=1.060e-03, tstd=3.521e-01) *best\n","[100.0%] train loss=1.174e-06 | eval RMSE=5.381e-04 (rel=1.570e-03, tstd=3.428e-01)\n","Final: RMSE=1.054e-03 (rel=3.052e-03) | time=609.9s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=100.85, ridge=0.01\n","[pre  20.0%] train MSE=2.643e+07 | eval RMSE=5.128e+03\n","[pre  40.0%] train MSE=1.739e+07 | eval RMSE=4.163e+03\n","[pre  60.0%] train MSE=1.354e+07 | eval RMSE=3.675e+03\n","[pre  80.0%] train MSE=1.216e+07 | eval RMSE=3.486e+03\n","[pre 100.0%] train MSE=1.190e+07 | eval RMSE=3.451e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=50 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=28.56 → scale=0.11); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=1.655e-04 | eval RMSE=1.346e-03 (rel=1.434e-02, tstd=9.382e-02) *best\n","[ 10.0%] train loss=1.895e-05 | eval RMSE=1.119e-03 (rel=1.205e-02, tstd=9.282e-02) *best\n","[ 15.0%] train loss=5.434e-05 | eval RMSE=7.966e-04 (rel=8.438e-03, tstd=9.440e-02) *best\n","[ 20.0%] train loss=1.672e-05 | eval RMSE=9.517e-04 (rel=1.007e-02, tstd=9.450e-02)\n","[ 25.0%] train loss=3.454e-05 | eval RMSE=1.392e-03 (rel=1.504e-02, tstd=9.257e-02)\n","[ 30.0%] train loss=2.574e-05 | eval RMSE=1.137e-03 (rel=1.216e-02, tstd=9.353e-02)\n","[ 35.0%] train loss=3.021e-05 | eval RMSE=4.144e-04 (rel=4.435e-03, tstd=9.343e-02) *best\n","[ 40.0%] train loss=1.459e-05 | eval RMSE=4.871e-04 (rel=5.256e-03, tstd=9.266e-02)\n","[ 45.0%] train loss=2.468e-05 | eval RMSE=5.166e-04 (rel=5.566e-03, tstd=9.282e-02)\n","[ 50.0%] train loss=2.238e-05 | eval RMSE=3.935e-04 (rel=4.251e-03, tstd=9.255e-02) *best\n","[ 55.0%] train loss=5.248e-06 | eval RMSE=4.585e-04 (rel=4.830e-03, tstd=9.493e-02)\n","[ 60.0%] train loss=1.592e-05 | eval RMSE=2.889e-04 (rel=3.088e-03, tstd=9.355e-02) *best\n","[ 65.0%] train loss=4.229e-06 | eval RMSE=2.684e-04 (rel=2.863e-03, tstd=9.373e-02) *best\n","[ 70.0%] train loss=4.839e-06 | eval RMSE=3.393e-04 (rel=3.674e-03, tstd=9.235e-02)\n","[ 75.0%] train loss=4.150e-06 | eval RMSE=2.825e-04 (rel=3.046e-03, tstd=9.274e-02)\n","[ 80.0%] train loss=3.714e-06 | eval RMSE=2.988e-04 (rel=3.240e-03, tstd=9.223e-02)\n","[ 85.0%] train loss=5.505e-06 | eval RMSE=3.209e-04 (rel=3.469e-03, tstd=9.251e-02)\n","[ 90.0%] train loss=6.202e-06 | eval RMSE=3.363e-04 (rel=3.640e-03, tstd=9.240e-02)\n","[ 95.0%] train loss=4.681e-06 | eval RMSE=2.704e-04 (rel=2.906e-03, tstd=9.302e-02)\n","[100.0%] train loss=4.202e-06 | eval RMSE=2.784e-04 (rel=3.024e-03, tstd=9.205e-02)\n","Final: RMSE=2.823e-04 (rel=2.998e-03) | time=1361.0s\n","\n","================= softplus_pairs — d=10 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=2.392e-03 | eval RMSE=1.935e-01 (rel=1.990e-01, tstd=9.727e-01) *best\n","[ 10.0%] train loss=2.119e-04 | eval RMSE=2.020e-02 (rel=2.094e-02, tstd=9.644e-01) *best\n","[ 15.0%] train loss=6.222e-05 | eval RMSE=8.588e-03 (rel=8.777e-03, tstd=9.785e-01) *best\n","[ 20.0%] train loss=3.155e-05 | eval RMSE=8.929e-03 (rel=9.185e-03, tstd=9.721e-01)\n","[ 25.0%] train loss=2.068e-05 | eval RMSE=5.575e-03 (rel=5.638e-03, tstd=9.887e-01) *best\n","[ 30.0%] train loss=3.272e-05 | eval RMSE=8.012e-03 (rel=8.222e-03, tstd=9.745e-01)\n","[ 35.0%] train loss=3.749e-05 | eval RMSE=8.306e-03 (rel=8.499e-03, tstd=9.772e-01)\n","[ 40.0%] train loss=2.400e-05 | eval RMSE=4.447e-03 (rel=4.524e-03, tstd=9.832e-01) *best\n","[ 45.0%] train loss=1.027e-05 | eval RMSE=5.298e-03 (rel=5.469e-03, tstd=9.687e-01)\n","[ 50.0%] train loss=3.314e-05 | eval RMSE=5.919e-03 (rel=6.080e-03, tstd=9.735e-01)\n","[ 55.0%] train loss=1.066e-05 | eval RMSE=2.494e-03 (rel=2.600e-03, tstd=9.593e-01) *best\n","[ 60.0%] train loss=3.249e-06 | eval RMSE=2.503e-03 (rel=2.584e-03, tstd=9.686e-01)\n","[ 65.0%] train loss=3.985e-06 | eval RMSE=3.585e-03 (rel=3.705e-03, tstd=9.676e-01)\n","[ 70.0%] train loss=2.520e-06 | eval RMSE=2.962e-03 (rel=2.997e-03, tstd=9.882e-01)\n","[ 75.0%] train loss=8.510e-07 | eval RMSE=1.379e-03 (rel=1.408e-03, tstd=9.796e-01) *best\n","[ 80.0%] train loss=2.198e-06 | eval RMSE=1.001e-03 (rel=1.029e-03, tstd=9.727e-01) *best\n","[ 85.0%] train loss=1.985e-06 | eval RMSE=1.805e-03 (rel=1.863e-03, tstd=9.688e-01)\n","[ 90.0%] train loss=8.393e-07 | eval RMSE=1.451e-03 (rel=1.480e-03, tstd=9.803e-01)\n","[ 95.0%] train loss=4.243e-06 | eval RMSE=2.042e-03 (rel=2.092e-03, tstd=9.763e-01)\n","[100.0%] train loss=8.261e-06 | eval RMSE=4.039e-03 (rel=4.152e-03, tstd=9.728e-01)\n","Final: RMSE=6.289e-03 (rel=6.394e-03) | time=493.3s\n","\n","================= softplus_pairs — d=20 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=1.547e-04 | eval RMSE=5.161e-02 (rel=1.825e-02, tstd=2.829e+00) *best\n","[ 10.0%] train loss=8.695e-05 | eval RMSE=3.433e-02 (rel=1.221e-02, tstd=2.811e+00) *best\n","[ 15.0%] train loss=4.811e-05 | eval RMSE=2.875e-02 (rel=1.011e-02, tstd=2.845e+00) *best\n","[ 20.0%] train loss=1.177e-05 | eval RMSE=1.237e-02 (rel=4.413e-03, tstd=2.803e+00) *best\n","[ 25.0%] train loss=1.131e-05 | eval RMSE=1.233e-02 (rel=4.417e-03, tstd=2.793e+00) *best\n","[ 30.0%] train loss=2.721e-05 | eval RMSE=2.458e-02 (rel=8.643e-03, tstd=2.844e+00)\n","[ 35.0%] train loss=2.250e-05 | eval RMSE=2.341e-02 (rel=8.292e-03, tstd=2.823e+00)\n","[ 40.0%] train loss=1.379e-05 | eval RMSE=1.179e-02 (rel=4.165e-03, tstd=2.831e+00) *best\n","[ 45.0%] train loss=8.278e-06 | eval RMSE=1.437e-02 (rel=5.079e-03, tstd=2.829e+00)\n","[ 50.0%] train loss=1.578e-05 | eval RMSE=8.662e-03 (rel=3.103e-03, tstd=2.792e+00) *best\n","[ 55.0%] train loss=5.482e-05 | eval RMSE=1.438e-02 (rel=5.086e-03, tstd=2.827e+00)\n","[ 60.0%] train loss=5.933e-06 | eval RMSE=6.782e-03 (rel=2.420e-03, tstd=2.802e+00) *best\n","[ 65.0%] train loss=4.512e-06 | eval RMSE=5.686e-03 (rel=2.042e-03, tstd=2.785e+00) *best\n","[ 70.0%] train loss=1.316e-06 | eval RMSE=5.149e-03 (rel=1.857e-03, tstd=2.774e+00) *best\n","[ 75.0%] train loss=1.498e-06 | eval RMSE=6.088e-03 (rel=2.169e-03, tstd=2.807e+00)\n","[ 80.0%] train loss=5.426e-07 | eval RMSE=2.376e-03 (rel=8.522e-04, tstd=2.788e+00) *best\n","[ 85.0%] train loss=1.236e-06 | eval RMSE=3.332e-03 (rel=1.174e-03, tstd=2.837e+00)\n","[ 90.0%] train loss=2.234e-06 | eval RMSE=6.806e-03 (rel=2.428e-03, tstd=2.803e+00)\n","[ 95.0%] train loss=2.571e-06 | eval RMSE=6.700e-03 (rel=2.383e-03, tstd=2.812e+00)\n","[100.0%] train loss=2.315e-06 | eval RMSE=5.413e-03 (rel=1.924e-03, tstd=2.813e+00)\n","Final: RMSE=5.961e-03 (rel=2.121e-03) | time=1123.4s\n","\n","================= softplus_pairs — d=50 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=1.075e-02 | eval RMSE=5.447e-01 (rel=4.845e-02, tstd=1.124e+01) *best\n","[ 10.0%] train loss=3.552e-04 | eval RMSE=2.867e-01 (rel=2.527e-02, tstd=1.134e+01) *best\n","[ 15.0%] train loss=4.234e-04 | eval RMSE=3.011e-01 (rel=2.641e-02, tstd=1.140e+01)\n","[ 20.0%] train loss=3.596e-04 | eval RMSE=2.922e-01 (rel=2.547e-02, tstd=1.147e+01)\n","[ 25.0%] train loss=5.278e-04 | eval RMSE=3.398e-01 (rel=2.992e-02, tstd=1.136e+01)\n","[ 30.0%] train loss=2.910e-04 | eval RMSE=2.926e-01 (rel=2.585e-02, tstd=1.132e+01)\n","[ 35.0%] train loss=3.039e-04 | eval RMSE=2.945e-01 (rel=2.649e-02, tstd=1.112e+01)\n","[ 40.0%] train loss=3.226e-04 | eval RMSE=2.835e-01 (rel=2.531e-02, tstd=1.120e+01) *best\n","[ 45.0%] train loss=2.997e-04 | eval RMSE=2.831e-01 (rel=2.475e-02, tstd=1.144e+01) *best\n","[ 50.0%] train loss=2.999e-04 | eval RMSE=2.750e-01 (rel=2.377e-02, tstd=1.157e+01) *best\n","[ 55.0%] train loss=2.642e-04 | eval RMSE=2.646e-01 (rel=2.319e-02, tstd=1.141e+01) *best\n","[ 60.0%] train loss=2.647e-04 | eval RMSE=2.673e-01 (rel=2.359e-02, tstd=1.133e+01)\n","[ 65.0%] train loss=2.429e-04 | eval RMSE=2.616e-01 (rel=2.279e-02, tstd=1.148e+01) *best\n","[ 70.0%] train loss=2.579e-04 | eval RMSE=2.453e-01 (rel=2.181e-02, tstd=1.125e+01) *best\n","[ 75.0%] train loss=2.395e-04 | eval RMSE=2.410e-01 (rel=2.131e-02, tstd=1.131e+01) *best\n","[ 80.0%] train loss=2.229e-04 | eval RMSE=2.483e-01 (rel=2.198e-02, tstd=1.130e+01)\n","[ 85.0%] train loss=2.222e-04 | eval RMSE=2.406e-01 (rel=2.109e-02, tstd=1.141e+01) *best\n","[ 90.0%] train loss=2.062e-04 | eval RMSE=2.324e-01 (rel=2.025e-02, tstd=1.147e+01) *best\n","[ 95.0%] train loss=2.270e-04 | eval RMSE=2.348e-01 (rel=2.071e-02, tstd=1.134e+01)\n","[100.0%] train loss=2.395e-04 | eval RMSE=2.436e-01 (rel=2.134e-02, tstd=1.141e+01)\n","Final: RMSE=2.455e-01 (rel=2.169e-02) | time=2856.0s\n","\n","============================ TRIAL 2/5 (seed=1000045) ============================\n","\n","================= quad_spd — d=10 =================\n","Q stats: ||Q||₂≈1.010, κ=90.15, trace=3.6 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=5.867e-05 | eval RMSE=1.140e-02 (rel=9.754e-03, tstd=1.169e+00) *best\n","[ 10.0%] train loss=1.110e-03 | eval RMSE=6.529e-02 (rel=5.610e-02, tstd=1.164e+00)\n","[ 15.0%] train loss=9.881e-05 | eval RMSE=1.611e-02 (rel=1.389e-02, tstd=1.160e+00)\n","[ 20.0%] train loss=1.019e-04 | eval RMSE=1.189e-02 (rel=1.030e-02, tstd=1.154e+00)\n","[ 25.0%] train loss=7.258e-05 | eval RMSE=1.594e-02 (rel=1.389e-02, tstd=1.148e+00)\n","[ 30.0%] train loss=7.787e-06 | eval RMSE=4.244e-03 (rel=3.664e-03, tstd=1.158e+00) *best\n","[ 35.0%] train loss=7.767e-06 | eval RMSE=1.545e-02 (rel=1.348e-02, tstd=1.147e+00)\n","[ 40.0%] train loss=8.337e-05 | eval RMSE=1.447e-02 (rel=1.250e-02, tstd=1.158e+00)\n","[ 45.0%] train loss=2.426e-05 | eval RMSE=2.568e-03 (rel=2.211e-03, tstd=1.161e+00) *best\n","[ 50.0%] train loss=4.560e-06 | eval RMSE=5.288e-03 (rel=4.581e-03, tstd=1.154e+00)\n","[ 55.0%] train loss=1.323e-06 | eval RMSE=5.502e-03 (rel=4.715e-03, tstd=1.167e+00)\n","[ 60.0%] train loss=2.258e-06 | eval RMSE=3.953e-03 (rel=3.304e-03, tstd=1.197e+00)\n","[ 65.0%] train loss=1.811e-06 | eval RMSE=5.447e-03 (rel=4.683e-03, tstd=1.163e+00)\n","[ 70.0%] train loss=1.557e-05 | eval RMSE=2.657e-03 (rel=2.324e-03, tstd=1.144e+00)\n","[ 75.0%] train loss=4.359e-06 | eval RMSE=1.434e-03 (rel=1.209e-03, tstd=1.185e+00) *best\n","[ 80.0%] train loss=7.468e-06 | eval RMSE=5.418e-03 (rel=4.636e-03, tstd=1.169e+00)\n","[ 85.0%] train loss=9.745e-06 | eval RMSE=6.046e-03 (rel=5.277e-03, tstd=1.146e+00)\n","[ 90.0%] train loss=6.980e-06 | eval RMSE=2.886e-03 (rel=2.475e-03, tstd=1.166e+00)\n","[ 95.0%] train loss=5.854e-06 | eval RMSE=3.932e-03 (rel=3.345e-03, tstd=1.175e+00)\n","[100.0%] train loss=6.424e-06 | eval RMSE=3.937e-03 (rel=3.349e-03, tstd=1.176e+00)\n","Final: RMSE=9.141e-03 (rel=7.966e-03) | time=298.4s\n","\n","================= quad_spd — d=20 =================\n","Q stats: ||Q||₂≈1.010, κ=99.31, trace=5.4 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=1.378e-03 | eval RMSE=5.951e-02 (rel=4.877e-02, tstd=1.220e+00) *best\n","[ 10.0%] train loss=4.262e-05 | eval RMSE=7.635e-03 (rel=6.342e-03, tstd=1.204e+00) *best\n","[ 15.0%] train loss=6.177e-05 | eval RMSE=7.259e-03 (rel=6.085e-03, tstd=1.193e+00) *best\n","[ 20.0%] train loss=2.701e-04 | eval RMSE=4.054e-02 (rel=3.351e-02, tstd=1.210e+00)\n","[ 25.0%] train loss=2.821e-05 | eval RMSE=5.992e-03 (rel=4.982e-03, tstd=1.203e+00) *best\n","[ 30.0%] train loss=5.862e-06 | eval RMSE=3.606e-03 (rel=2.996e-03, tstd=1.203e+00) *best\n","[ 35.0%] train loss=1.510e-05 | eval RMSE=5.286e-03 (rel=4.447e-03, tstd=1.189e+00)\n","[ 40.0%] train loss=2.184e-06 | eval RMSE=3.793e-03 (rel=3.195e-03, tstd=1.187e+00)\n","[ 45.0%] train loss=2.132e-06 | eval RMSE=5.643e-03 (rel=4.715e-03, tstd=1.197e+00)\n","[ 50.0%] train loss=1.148e-05 | eval RMSE=2.237e-03 (rel=1.888e-03, tstd=1.185e+00) *best\n","[ 55.0%] train loss=1.604e-05 | eval RMSE=2.729e-03 (rel=2.224e-03, tstd=1.227e+00)\n","[ 60.0%] train loss=1.409e-06 | eval RMSE=5.864e-03 (rel=4.864e-03, tstd=1.206e+00)\n","[ 65.0%] train loss=2.999e-06 | eval RMSE=2.280e-03 (rel=1.866e-03, tstd=1.222e+00)\n","[ 70.0%] train loss=7.443e-07 | eval RMSE=2.708e-03 (rel=2.220e-03, tstd=1.220e+00)\n","[ 75.0%] train loss=2.176e-06 | eval RMSE=2.519e-03 (rel=2.076e-03, tstd=1.213e+00)\n","[ 80.0%] train loss=8.223e-07 | eval RMSE=8.443e-04 (rel=6.834e-04, tstd=1.236e+00) *best\n","[ 85.0%] train loss=2.909e-06 | eval RMSE=8.878e-04 (rel=7.379e-04, tstd=1.203e+00)\n","[ 90.0%] train loss=5.540e-07 | eval RMSE=6.643e-04 (rel=5.540e-04, tstd=1.199e+00) *best\n","[ 95.0%] train loss=5.524e-07 | eval RMSE=1.415e-03 (rel=1.149e-03, tstd=1.232e+00)\n","[100.0%] train loss=8.140e-07 | eval RMSE=1.629e-03 (rel=1.343e-03, tstd=1.213e+00)\n","Final: RMSE=1.090e-02 (rel=9.215e-03) | time=516.4s\n","\n","================= quad_spd — d=50 =================\n","Q stats: ||Q||₂≈1.010, κ=100.94, trace=14.6 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=2.494e-04 | eval RMSE=3.993e-02 (rel=1.935e-02, tstd=2.063e+00) *best\n","[ 10.0%] train loss=3.548e-05 | eval RMSE=2.214e-02 (rel=1.088e-02, tstd=2.036e+00) *best\n","[ 15.0%] train loss=3.614e-06 | eval RMSE=4.323e-03 (rel=2.121e-03, tstd=2.038e+00) *best\n","[ 20.0%] train loss=1.711e-05 | eval RMSE=1.610e-02 (rel=7.935e-03, tstd=2.029e+00)\n","[ 25.0%] train loss=5.024e-06 | eval RMSE=7.448e-03 (rel=3.609e-03, tstd=2.064e+00)\n","[ 30.0%] train loss=4.141e-06 | eval RMSE=6.236e-03 (rel=3.050e-03, tstd=2.045e+00)\n","[ 35.0%] train loss=2.407e-05 | eval RMSE=1.694e-02 (rel=8.247e-03, tstd=2.054e+00)\n","[ 40.0%] train loss=3.415e-06 | eval RMSE=3.168e-03 (rel=1.566e-03, tstd=2.022e+00) *best\n","[ 45.0%] train loss=2.480e-06 | eval RMSE=4.098e-03 (rel=2.015e-03, tstd=2.033e+00)\n","[ 50.0%] train loss=1.398e-05 | eval RMSE=1.164e-02 (rel=5.712e-03, tstd=2.039e+00)\n","[ 55.0%] train loss=2.761e-06 | eval RMSE=7.466e-03 (rel=3.602e-03, tstd=2.073e+00)\n","[ 60.0%] train loss=6.540e-07 | eval RMSE=1.769e-03 (rel=8.668e-04, tstd=2.041e+00) *best\n","[ 65.0%] train loss=2.899e-06 | eval RMSE=2.941e-03 (rel=1.453e-03, tstd=2.024e+00)\n","[ 70.0%] train loss=2.357e-06 | eval RMSE=3.131e-03 (rel=1.524e-03, tstd=2.055e+00)\n","[ 75.0%] train loss=3.350e-06 | eval RMSE=3.141e-03 (rel=1.521e-03, tstd=2.065e+00)\n","[ 80.0%] train loss=1.466e-06 | eval RMSE=2.786e-03 (rel=1.370e-03, tstd=2.033e+00)\n","[ 85.0%] train loss=2.092e-06 | eval RMSE=1.386e-03 (rel=6.766e-04, tstd=2.049e+00) *best\n","[ 90.0%] train loss=2.440e-06 | eval RMSE=4.600e-03 (rel=2.241e-03, tstd=2.052e+00)\n","[ 95.0%] train loss=6.678e-07 | eval RMSE=1.813e-03 (rel=8.799e-04, tstd=2.061e+00)\n","[100.0%] train loss=7.623e-07 | eval RMSE=2.294e-03 (rel=1.107e-03, tstd=2.072e+00)\n","Final: RMSE=2.084e-03 (rel=1.026e-03) | time=1336.0s\n","\n","================= exp_minus_lin — d=10 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=384, depth=4), params=1.19M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=2.130e-04 | eval RMSE=1.622e-01 (rel=3.980e-02, tstd=4.075e+00) *best\n","[ 10.0%] train loss=1.741e-04 | eval RMSE=6.844e-02 (rel=1.805e-02, tstd=3.792e+00) *best\n","[ 15.0%] train loss=3.403e-04 | eval RMSE=3.601e-02 (rel=1.226e-02, tstd=2.938e+00) *best\n","[ 20.0%] train loss=9.359e-05 | eval RMSE=3.578e-02 (rel=9.775e-03, tstd=3.660e+00) *best\n","[ 25.0%] train loss=6.884e-05 | eval RMSE=4.621e-02 (rel=1.266e-02, tstd=3.650e+00)\n","[ 30.0%] train loss=1.055e-04 | eval RMSE=7.833e-01 (rel=1.534e-01, tstd=5.107e+00)\n","[ 35.0%] train loss=2.309e-05 | eval RMSE=1.788e-02 (rel=5.657e-03, tstd=3.161e+00) *best\n","[ 40.0%] train loss=1.381e-05 | eval RMSE=8.123e-02 (rel=1.914e-02, tstd=4.244e+00)\n","[ 45.0%] train loss=1.231e-04 | eval RMSE=4.928e-02 (rel=1.462e-02, tstd=3.371e+00)\n","[ 50.0%] train loss=1.487e-04 | eval RMSE=6.000e-02 (rel=1.619e-02, tstd=3.705e+00)\n","[ 55.0%] train loss=6.646e-05 | eval RMSE=2.267e-02 (rel=6.068e-03, tstd=3.735e+00)\n","[ 60.0%] train loss=1.328e-04 | eval RMSE=6.512e-02 (rel=1.502e-02, tstd=4.334e+00)\n","[ 65.0%] train loss=7.482e-06 | eval RMSE=2.427e-02 (rel=7.649e-03, tstd=3.172e+00)\n","[ 70.0%] train loss=1.349e-05 | eval RMSE=4.478e-02 (rel=1.001e-02, tstd=4.472e+00)\n","[ 75.0%] train loss=1.350e-06 | eval RMSE=4.503e-02 (rel=1.024e-02, tstd=4.399e+00)\n","[ 80.0%] train loss=6.959e-04 | eval RMSE=1.138e-01 (rel=3.891e-02, tstd=2.924e+00)\n","[ 85.0%] train loss=7.952e-05 | eval RMSE=6.422e-02 (rel=1.848e-02, tstd=3.475e+00)\n","[ 90.0%] train loss=6.806e-05 | eval RMSE=3.331e-02 (rel=9.992e-03, tstd=3.333e+00)\n","[ 95.0%] train loss=4.594e-04 | eval RMSE=1.564e-01 (rel=4.146e-02, tstd=3.773e+00)\n","[100.0%] train loss=1.157e-04 | eval RMSE=5.286e-02 (rel=1.737e-02, tstd=3.044e+00)\n","Final: RMSE=2.281e-01 (rel=5.931e-02) | time=305.7s\n","\n","================= exp_minus_lin — d=20 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=512, depth=5), params=2.64M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=6.972e-04 | eval RMSE=2.110e-01 (rel=4.363e-02, tstd=4.836e+00) *best\n","[ 10.0%] train loss=1.013e-04 | eval RMSE=3.955e-02 (rel=1.004e-02, tstd=3.941e+00) *best\n","[ 15.0%] train loss=3.235e-05 | eval RMSE=2.536e-02 (rel=4.694e-03, tstd=5.402e+00) *best\n","[ 20.0%] train loss=1.554e-04 | eval RMSE=5.351e-02 (rel=1.403e-02, tstd=3.813e+00)\n","[ 25.0%] train loss=3.509e-04 | eval RMSE=2.212e-02 (rel=6.110e-03, tstd=3.621e+00) *best\n","[ 30.0%] train loss=8.484e-05 | eval RMSE=4.105e-02 (rel=1.270e-02, tstd=3.232e+00)\n","[ 35.0%] train loss=1.720e-05 | eval RMSE=4.419e-02 (rel=1.085e-02, tstd=4.073e+00)\n","[ 40.0%] train loss=1.397e-03 | eval RMSE=1.275e-01 (rel=3.104e-02, tstd=4.106e+00)\n","[ 45.0%] train loss=1.172e-04 | eval RMSE=8.604e-02 (rel=1.994e-02, tstd=4.315e+00)\n","[ 50.0%] train loss=4.010e-05 | eval RMSE=2.680e-02 (rel=7.509e-03, tstd=3.569e+00)\n","[ 55.0%] train loss=7.058e-06 | eval RMSE=1.906e-02 (rel=5.372e-03, tstd=3.548e+00) *best\n","[ 60.0%] train loss=7.924e-06 | eval RMSE=1.401e-02 (rel=3.860e-03, tstd=3.629e+00) *best\n","[ 65.0%] train loss=1.854e-04 | eval RMSE=6.869e-02 (rel=1.841e-02, tstd=3.731e+00)\n","[ 70.0%] train loss=2.223e-04 | eval RMSE=9.527e-02 (rel=2.805e-02, tstd=3.397e+00)\n","[ 75.0%] train loss=8.916e-06 | eval RMSE=1.724e-02 (rel=5.508e-03, tstd=3.131e+00)\n","[ 80.0%] train loss=1.388e-05 | eval RMSE=2.221e-02 (rel=5.768e-03, tstd=3.851e+00)\n","[ 85.0%] train loss=1.494e-04 | eval RMSE=9.644e-02 (rel=2.932e-02, tstd=3.289e+00)\n","[ 90.0%] train loss=9.219e-04 | eval RMSE=2.058e-01 (rel=5.947e-02, tstd=3.460e+00)\n","[ 95.0%] train loss=6.349e-05 | eval RMSE=6.856e-02 (rel=1.880e-02, tstd=3.646e+00)\n","[100.0%] train loss=3.114e-04 | eval RMSE=1.692e-01 (rel=4.285e-02, tstd=3.949e+00)\n","Final: RMSE=2.481e-01 (rel=6.858e-02) | time=528.5s\n","\n","================= exp_minus_lin — d=50 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=768, depth=6), params=7.11M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=3.611e-04 | eval RMSE=7.138e-02 (rel=2.352e-02, tstd=3.035e+00) *best\n","[ 10.0%] train loss=1.850e-04 | eval RMSE=2.433e-01 (rel=4.641e-02, tstd=5.244e+00)\n","[ 15.0%] train loss=4.604e-04 | eval RMSE=9.483e-02 (rel=2.351e-02, tstd=4.033e+00)\n","[ 20.0%] train loss=2.080e-05 | eval RMSE=2.401e-02 (rel=7.170e-03, tstd=3.349e+00) *best\n","[ 25.0%] train loss=1.532e-04 | eval RMSE=4.037e-02 (rel=1.074e-02, tstd=3.759e+00)\n","[ 30.0%] train loss=2.911e-04 | eval RMSE=1.058e-01 (rel=3.099e-02, tstd=3.415e+00)\n","[ 35.0%] train loss=2.763e-05 | eval RMSE=2.212e-02 (rel=6.190e-03, tstd=3.574e+00) *best\n","[ 40.0%] train loss=2.742e-04 | eval RMSE=1.099e-01 (rel=2.850e-02, tstd=3.854e+00)\n","[ 45.0%] train loss=1.167e-05 | eval RMSE=2.451e-02 (rel=6.427e-03, tstd=3.813e+00)\n","[ 50.0%] train loss=2.829e-04 | eval RMSE=8.222e-02 (rel=2.086e-02, tstd=3.942e+00)\n","[ 55.0%] train loss=2.684e-05 | eval RMSE=3.311e-02 (rel=1.074e-02, tstd=3.082e+00)\n","[ 60.0%] train loss=4.691e-05 | eval RMSE=3.332e-02 (rel=8.717e-03, tstd=3.822e+00)\n","[ 65.0%] train loss=1.288e-05 | eval RMSE=1.828e-02 (rel=5.204e-03, tstd=3.513e+00) *best\n","[ 70.0%] train loss=1.134e-04 | eval RMSE=4.837e-02 (rel=1.097e-02, tstd=4.411e+00)\n","[ 75.0%] train loss=5.421e-05 | eval RMSE=3.890e-02 (rel=1.006e-02, tstd=3.865e+00)\n","[ 80.0%] train loss=8.923e-06 | eval RMSE=2.241e-02 (rel=5.902e-03, tstd=3.796e+00)\n","[ 85.0%] train loss=1.502e-05 | eval RMSE=1.882e-02 (rel=5.476e-03, tstd=3.438e+00)\n","[ 90.0%] train loss=2.298e-04 | eval RMSE=5.443e-02 (rel=1.707e-02, tstd=3.189e+00)\n","[ 95.0%] train loss=6.803e-05 | eval RMSE=1.433e-01 (rel=2.165e-02, tstd=6.620e+00)\n","[100.0%] train loss=9.466e-05 | eval RMSE=4.318e-02 (rel=1.154e-02, tstd=3.743e+00)\n","Final: RMSE=1.939e-02 (rel=5.398e-03) | time=1333.5s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=96.59, ridge=0.01\n","[pre  20.0%] train MSE=3.071e+07 | eval RMSE=5.513e+03\n","[pre  40.0%] train MSE=2.035e+07 | eval RMSE=4.500e+03\n","[pre  60.0%] train MSE=1.569e+07 | eval RMSE=3.958e+03\n","[pre  80.0%] train MSE=1.399e+07 | eval RMSE=3.741e+03\n","[pre 100.0%] train MSE=1.369e+07 | eval RMSE=3.702e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=10 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=126.61 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=5.073e-04 | eval RMSE=3.318e-02 (rel=3.481e-02, tstd=9.531e-01) *best\n","[ 10.0%] train loss=8.131e-05 | eval RMSE=1.434e-02 (rel=1.481e-02, tstd=9.682e-01) *best\n","[ 15.0%] train loss=2.284e-05 | eval RMSE=9.795e-03 (rel=9.978e-03, tstd=9.816e-01) *best\n","[ 20.0%] train loss=6.470e-05 | eval RMSE=7.611e-03 (rel=8.007e-03, tstd=9.505e-01) *best\n","[ 25.0%] train loss=4.903e-05 | eval RMSE=9.566e-03 (rel=1.007e-02, tstd=9.496e-01)\n","[ 30.0%] train loss=2.472e-05 | eval RMSE=4.182e-03 (rel=4.365e-03, tstd=9.580e-01) *best\n","[ 35.0%] train loss=3.922e-06 | eval RMSE=2.475e-03 (rel=2.562e-03, tstd=9.661e-01) *best\n","[ 40.0%] train loss=1.104e-05 | eval RMSE=2.065e-03 (rel=2.153e-03, tstd=9.587e-01) *best\n","[ 45.0%] train loss=5.642e-06 | eval RMSE=3.890e-03 (rel=4.045e-03, tstd=9.617e-01)\n","[ 50.0%] train loss=3.471e-05 | eval RMSE=3.053e-03 (rel=3.213e-03, tstd=9.500e-01)\n","[ 55.0%] train loss=1.777e-05 | eval RMSE=1.957e-03 (rel=2.052e-03, tstd=9.535e-01) *best\n","[ 60.0%] train loss=1.051e-06 | eval RMSE=1.905e-03 (rel=2.013e-03, tstd=9.465e-01) *best\n","[ 65.0%] train loss=1.698e-05 | eval RMSE=7.453e-03 (rel=7.683e-03, tstd=9.701e-01)\n","[ 70.0%] train loss=2.479e-06 | eval RMSE=1.737e-03 (rel=1.819e-03, tstd=9.551e-01) *best\n","[ 75.0%] train loss=7.753e-06 | eval RMSE=1.377e-03 (rel=1.433e-03, tstd=9.613e-01) *best\n","[ 80.0%] train loss=4.433e-06 | eval RMSE=3.094e-03 (rel=3.167e-03, tstd=9.767e-01)\n","[ 85.0%] train loss=1.679e-07 | eval RMSE=5.835e-04 (rel=5.993e-04, tstd=9.737e-01) *best\n","[ 90.0%] train loss=2.676e-06 | eval RMSE=2.214e-03 (rel=2.277e-03, tstd=9.727e-01)\n","[ 95.0%] train loss=8.926e-06 | eval RMSE=3.714e-03 (rel=3.811e-03, tstd=9.747e-01)\n","[100.0%] train loss=6.330e-07 | eval RMSE=1.087e-03 (rel=1.114e-03, tstd=9.760e-01)\n","Final: RMSE=9.155e-03 (rel=9.485e-03) | time=358.0s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=99.28, ridge=0.01\n","[pre  20.0%] train MSE=2.907e+07 | eval RMSE=5.383e+03\n","[pre  40.0%] train MSE=1.922e+07 | eval RMSE=4.378e+03\n","[pre  60.0%] train MSE=1.486e+07 | eval RMSE=3.852e+03\n","[pre  80.0%] train MSE=1.332e+07 | eval RMSE=3.647e+03\n","[pre 100.0%] train MSE=1.303e+07 | eval RMSE=3.609e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=20 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=54.67 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=7.288e-04 | eval RMSE=1.922e-02 (rel=5.222e-02, tstd=3.681e-01) *best\n","[ 10.0%] train loss=6.181e-05 | eval RMSE=3.982e-03 (rel=1.064e-02, tstd=3.742e-01) *best\n","[ 15.0%] train loss=3.314e-05 | eval RMSE=2.900e-03 (rel=7.853e-03, tstd=3.693e-01) *best\n","[ 20.0%] train loss=6.420e-05 | eval RMSE=4.349e-03 (rel=1.187e-02, tstd=3.664e-01)\n","[ 25.0%] train loss=4.178e-06 | eval RMSE=1.236e-03 (rel=3.362e-03, tstd=3.675e-01) *best\n","[ 30.0%] train loss=8.832e-05 | eval RMSE=4.601e-03 (rel=1.244e-02, tstd=3.699e-01)\n","[ 35.0%] train loss=7.278e-05 | eval RMSE=3.195e-03 (rel=8.667e-03, tstd=3.687e-01)\n","[ 40.0%] train loss=3.952e-05 | eval RMSE=3.988e-03 (rel=1.087e-02, tstd=3.668e-01)\n","[ 45.0%] train loss=2.875e-06 | eval RMSE=5.996e-04 (rel=1.639e-03, tstd=3.659e-01) *best\n","[ 50.0%] train loss=6.415e-06 | eval RMSE=2.296e-03 (rel=6.184e-03, tstd=3.713e-01)\n","[ 55.0%] train loss=3.488e-06 | eval RMSE=9.769e-04 (rel=2.659e-03, tstd=3.675e-01)\n","[ 60.0%] train loss=6.968e-06 | eval RMSE=6.011e-04 (rel=1.643e-03, tstd=3.658e-01)\n","[ 65.0%] train loss=6.565e-07 | eval RMSE=7.081e-04 (rel=1.940e-03, tstd=3.650e-01)\n","[ 70.0%] train loss=4.370e-06 | eval RMSE=9.970e-04 (rel=2.717e-03, tstd=3.669e-01)\n","[ 75.0%] train loss=2.101e-06 | eval RMSE=1.151e-03 (rel=3.090e-03, tstd=3.723e-01)\n","[ 80.0%] train loss=4.115e-06 | eval RMSE=5.064e-04 (rel=1.390e-03, tstd=3.643e-01) *best\n","[ 85.0%] train loss=1.097e-06 | eval RMSE=9.145e-04 (rel=2.499e-03, tstd=3.659e-01)\n","[ 90.0%] train loss=5.399e-06 | eval RMSE=1.109e-03 (rel=2.969e-03, tstd=3.734e-01)\n","[ 95.0%] train loss=4.206e-07 | eval RMSE=3.785e-04 (rel=1.027e-03, tstd=3.685e-01) *best\n","[100.0%] train loss=3.699e-06 | eval RMSE=1.034e-03 (rel=2.819e-03, tstd=3.667e-01)\n","Final: RMSE=2.013e-03 (rel=5.523e-03) | time=607.9s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=99.19, ridge=0.01\n","[pre  20.0%] train MSE=2.636e+07 | eval RMSE=5.122e+03\n","[pre  40.0%] train MSE=1.733e+07 | eval RMSE=4.158e+03\n","[pre  60.0%] train MSE=1.349e+07 | eval RMSE=3.668e+03\n","[pre  80.0%] train MSE=1.211e+07 | eval RMSE=3.480e+03\n","[pre 100.0%] train MSE=1.187e+07 | eval RMSE=3.445e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=50 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=29.83 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=1.223e-05 | eval RMSE=4.766e-04 (rel=5.657e-03, tstd=8.424e-02) *best\n","[ 10.0%] train loss=3.448e-04 | eval RMSE=9.177e-04 (rel=1.078e-02, tstd=8.515e-02)\n","[ 15.0%] train loss=2.622e-04 | eval RMSE=1.246e-03 (rel=1.475e-02, tstd=8.443e-02)\n","[ 20.0%] train loss=1.464e-05 | eval RMSE=4.075e-04 (rel=4.845e-03, tstd=8.410e-02) *best\n","[ 25.0%] train loss=7.647e-06 | eval RMSE=3.578e-04 (rel=4.227e-03, tstd=8.465e-02) *best\n","[ 30.0%] train loss=1.049e-05 | eval RMSE=3.696e-04 (rel=4.377e-03, tstd=8.445e-02)\n","[ 35.0%] train loss=3.041e-05 | eval RMSE=4.421e-04 (rel=5.199e-03, tstd=8.503e-02)\n","[ 40.0%] train loss=7.349e-06 | eval RMSE=5.032e-04 (rel=5.937e-03, tstd=8.475e-02)\n","[ 45.0%] train loss=7.153e-06 | eval RMSE=3.163e-04 (rel=3.750e-03, tstd=8.436e-02) *best\n","[ 50.0%] train loss=1.550e-05 | eval RMSE=7.285e-04 (rel=8.714e-03, tstd=8.360e-02)\n","[ 55.0%] train loss=1.580e-05 | eval RMSE=4.262e-04 (rel=5.045e-03, tstd=8.446e-02)\n","[ 60.0%] train loss=5.229e-06 | eval RMSE=3.061e-04 (rel=3.645e-03, tstd=8.398e-02) *best\n","[ 65.0%] train loss=5.216e-06 | eval RMSE=3.179e-04 (rel=3.807e-03, tstd=8.350e-02)\n","[ 70.0%] train loss=5.227e-06 | eval RMSE=2.962e-04 (rel=3.522e-03, tstd=8.410e-02) *best\n","[ 75.0%] train loss=7.325e-06 | eval RMSE=3.802e-04 (rel=4.437e-03, tstd=8.569e-02)\n","[ 80.0%] train loss=5.058e-06 | eval RMSE=3.845e-04 (rel=4.565e-03, tstd=8.423e-02)\n","[ 85.0%] train loss=4.906e-06 | eval RMSE=2.655e-04 (rel=3.143e-03, tstd=8.446e-02) *best\n","[ 90.0%] train loss=4.399e-06 | eval RMSE=2.653e-04 (rel=3.168e-03, tstd=8.374e-02) *best\n","[ 95.0%] train loss=4.776e-06 | eval RMSE=2.684e-04 (rel=3.174e-03, tstd=8.456e-02)\n","[100.0%] train loss=4.269e-06 | eval RMSE=2.599e-04 (rel=3.048e-03, tstd=8.526e-02) *best\n","Final: RMSE=2.602e-04 (rel=3.093e-03) | time=1361.0s\n","\n","================= softplus_pairs — d=10 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=1.032e-04 | eval RMSE=1.824e-02 (rel=1.873e-02, tstd=9.737e-01) *best\n","[ 10.0%] train loss=7.375e-04 | eval RMSE=3.444e-02 (rel=3.590e-02, tstd=9.592e-01)\n","[ 15.0%] train loss=5.008e-04 | eval RMSE=2.720e-02 (rel=2.724e-02, tstd=9.987e-01)\n","[ 20.0%] train loss=6.979e-05 | eval RMSE=7.264e-03 (rel=7.397e-03, tstd=9.820e-01) *best\n","[ 25.0%] train loss=1.704e-05 | eval RMSE=5.605e-03 (rel=5.766e-03, tstd=9.721e-01) *best\n","[ 30.0%] train loss=4.497e-05 | eval RMSE=6.030e-03 (rel=6.078e-03, tstd=9.922e-01)\n","[ 35.0%] train loss=7.782e-06 | eval RMSE=4.183e-03 (rel=4.241e-03, tstd=9.863e-01) *best\n","[ 40.0%] train loss=8.727e-06 | eval RMSE=5.207e-03 (rel=5.295e-03, tstd=9.834e-01)\n","[ 45.0%] train loss=4.273e-06 | eval RMSE=3.015e-03 (rel=3.081e-03, tstd=9.787e-01) *best\n","[ 50.0%] train loss=3.548e-05 | eval RMSE=3.782e-03 (rel=3.913e-03, tstd=9.663e-01)\n","[ 55.0%] train loss=1.032e-05 | eval RMSE=3.389e-03 (rel=3.480e-03, tstd=9.737e-01)\n","[ 60.0%] train loss=1.766e-05 | eval RMSE=5.013e-03 (rel=5.100e-03, tstd=9.829e-01)\n","[ 65.0%] train loss=8.074e-06 | eval RMSE=4.924e-03 (rel=5.056e-03, tstd=9.740e-01)\n","[ 70.0%] train loss=1.437e-06 | eval RMSE=2.737e-03 (rel=2.812e-03, tstd=9.735e-01) *best\n","[ 75.0%] train loss=3.612e-06 | eval RMSE=3.197e-03 (rel=3.256e-03, tstd=9.818e-01)\n","[ 80.0%] train loss=5.143e-06 | eval RMSE=3.330e-03 (rel=3.405e-03, tstd=9.779e-01)\n","[ 85.0%] train loss=8.935e-07 | eval RMSE=2.280e-03 (rel=2.327e-03, tstd=9.795e-01) *best\n","[ 90.0%] train loss=3.377e-06 | eval RMSE=1.822e-03 (rel=1.891e-03, tstd=9.634e-01) *best\n","[ 95.0%] train loss=1.971e-05 | eval RMSE=5.410e-03 (rel=5.629e-03, tstd=9.610e-01)\n","[100.0%] train loss=9.462e-06 | eval RMSE=4.043e-03 (rel=4.192e-03, tstd=9.644e-01)\n","Final: RMSE=1.129e-02 (rel=1.178e-02) | time=496.4s\n","\n","================= softplus_pairs — d=20 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=2.715e-04 | eval RMSE=7.525e-02 (rel=2.654e-02, tstd=2.835e+00) *best\n","[ 10.0%] train loss=9.076e-05 | eval RMSE=4.296e-02 (rel=1.515e-02, tstd=2.836e+00) *best\n","[ 15.0%] train loss=2.873e-05 | eval RMSE=1.604e-02 (rel=5.641e-03, tstd=2.843e+00) *best\n","[ 20.0%] train loss=2.136e-05 | eval RMSE=1.502e-02 (rel=5.325e-03, tstd=2.820e+00) *best\n","[ 25.0%] train loss=3.441e-05 | eval RMSE=2.257e-02 (rel=7.972e-03, tstd=2.831e+00)\n","[ 30.0%] train loss=1.353e-05 | eval RMSE=1.558e-02 (rel=5.433e-03, tstd=2.867e+00)\n","[ 35.0%] train loss=1.484e-05 | eval RMSE=1.395e-02 (rel=4.961e-03, tstd=2.812e+00) *best\n","[ 40.0%] train loss=3.447e-05 | eval RMSE=2.459e-02 (rel=8.588e-03, tstd=2.863e+00)\n","[ 45.0%] train loss=5.114e-06 | eval RMSE=8.650e-03 (rel=3.054e-03, tstd=2.833e+00) *best\n","[ 50.0%] train loss=3.270e-06 | eval RMSE=7.576e-03 (rel=2.691e-03, tstd=2.815e+00) *best\n","[ 55.0%] train loss=1.898e-05 | eval RMSE=8.168e-03 (rel=2.840e-03, tstd=2.875e+00)\n","[ 60.0%] train loss=3.732e-06 | eval RMSE=7.217e-03 (rel=2.606e-03, tstd=2.769e+00) *best\n","[ 65.0%] train loss=2.807e-06 | eval RMSE=6.053e-03 (rel=2.137e-03, tstd=2.832e+00) *best\n","[ 70.0%] train loss=7.752e-07 | eval RMSE=3.776e-03 (rel=1.312e-03, tstd=2.878e+00) *best\n","[ 75.0%] train loss=2.366e-06 | eval RMSE=5.317e-03 (rel=1.876e-03, tstd=2.834e+00)\n","[ 80.0%] train loss=2.801e-06 | eval RMSE=4.833e-03 (rel=1.711e-03, tstd=2.825e+00)\n","[ 85.0%] train loss=6.631e-07 | eval RMSE=5.439e-03 (rel=1.919e-03, tstd=2.834e+00)\n","[ 90.0%] train loss=7.677e-07 | eval RMSE=3.807e-03 (rel=1.342e-03, tstd=2.836e+00)\n","[ 95.0%] train loss=9.836e-07 | eval RMSE=3.613e-03 (rel=1.281e-03, tstd=2.820e+00) *best\n","[100.0%] train loss=1.051e-06 | eval RMSE=3.637e-03 (rel=1.291e-03, tstd=2.816e+00)\n","Final: RMSE=1.047e-02 (rel=3.716e-03) | time=1130.7s\n","\n","================= softplus_pairs — d=50 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=3.211e-04 | eval RMSE=2.832e-01 (rel=2.521e-02, tstd=1.124e+01) *best\n","[ 10.0%] train loss=4.022e-04 | eval RMSE=3.033e-01 (rel=2.709e-02, tstd=1.120e+01)\n","[ 15.0%] train loss=3.350e-04 | eval RMSE=2.957e-01 (rel=2.577e-02, tstd=1.148e+01)\n","[ 20.0%] train loss=3.216e-04 | eval RMSE=2.872e-01 (rel=2.522e-02, tstd=1.138e+01)\n","[ 25.0%] train loss=3.271e-04 | eval RMSE=2.776e-01 (rel=2.456e-02, tstd=1.130e+01) *best\n","[ 30.0%] train loss=3.252e-04 | eval RMSE=2.702e-01 (rel=2.382e-02, tstd=1.135e+01) *best\n","[ 35.0%] train loss=2.955e-04 | eval RMSE=2.762e-01 (rel=2.448e-02, tstd=1.129e+01)\n","[ 40.0%] train loss=3.880e-04 | eval RMSE=3.016e-01 (rel=2.649e-02, tstd=1.139e+01)\n","[ 45.0%] train loss=2.971e-04 | eval RMSE=2.607e-01 (rel=2.286e-02, tstd=1.141e+01) *best\n","[ 50.0%] train loss=2.592e-04 | eval RMSE=2.645e-01 (rel=2.326e-02, tstd=1.137e+01)\n","[ 55.0%] train loss=2.693e-04 | eval RMSE=2.635e-01 (rel=2.335e-02, tstd=1.128e+01)\n","[ 60.0%] train loss=2.484e-04 | eval RMSE=2.560e-01 (rel=2.275e-02, tstd=1.125e+01) *best\n","[ 65.0%] train loss=2.398e-04 | eval RMSE=2.586e-01 (rel=2.256e-02, tstd=1.146e+01)\n","[ 70.0%] train loss=2.471e-04 | eval RMSE=2.552e-01 (rel=2.238e-02, tstd=1.140e+01) *best\n","[ 75.0%] train loss=2.677e-04 | eval RMSE=2.565e-01 (rel=2.243e-02, tstd=1.143e+01)\n","[ 80.0%] train loss=2.526e-04 | eval RMSE=2.591e-01 (rel=2.278e-02, tstd=1.138e+01)\n","[ 85.0%] train loss=2.402e-04 | eval RMSE=2.455e-01 (rel=2.159e-02, tstd=1.137e+01) *best\n","[ 90.0%] train loss=2.284e-04 | eval RMSE=2.487e-01 (rel=2.183e-02, tstd=1.139e+01)\n","[ 95.0%] train loss=2.370e-04 | eval RMSE=2.482e-01 (rel=2.171e-02, tstd=1.143e+01)\n","[100.0%] train loss=2.471e-04 | eval RMSE=2.499e-01 (rel=2.179e-02, tstd=1.147e+01)\n","Final: RMSE=2.481e-01 (rel=2.195e-02) | time=2858.9s\n","\n","============================ TRIAL 3/5 (seed=2000048) ============================\n","\n","================= quad_spd — d=10 =================\n","Q stats: ||Q||₂≈1.010, κ=97.22, trace=2.7 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=1.525e-04 | eval RMSE=2.401e-02 (rel=2.698e-02, tstd=8.897e-01) *best\n","[ 10.0%] train loss=6.647e-05 | eval RMSE=1.652e-02 (rel=1.860e-02, tstd=8.880e-01) *best\n","[ 15.0%] train loss=7.554e-05 | eval RMSE=1.943e-02 (rel=2.109e-02, tstd=9.212e-01)\n","[ 20.0%] train loss=1.151e-04 | eval RMSE=1.201e-02 (rel=1.351e-02, tstd=8.887e-01) *best\n","[ 25.0%] train loss=2.353e-05 | eval RMSE=5.782e-03 (rel=6.600e-03, tstd=8.761e-01) *best\n","[ 30.0%] train loss=2.082e-05 | eval RMSE=3.506e-03 (rel=3.948e-03, tstd=8.881e-01) *best\n","[ 35.0%] train loss=3.594e-06 | eval RMSE=3.625e-03 (rel=4.101e-03, tstd=8.840e-01)\n","[ 40.0%] train loss=1.304e-05 | eval RMSE=5.478e-03 (rel=6.107e-03, tstd=8.969e-01)\n","[ 45.0%] train loss=2.882e-05 | eval RMSE=2.989e-03 (rel=3.425e-03, tstd=8.729e-01) *best\n","[ 50.0%] train loss=6.510e-06 | eval RMSE=3.649e-03 (rel=4.224e-03, tstd=8.640e-01)\n","[ 55.0%] train loss=6.602e-06 | eval RMSE=4.221e-03 (rel=4.828e-03, tstd=8.742e-01)\n","[ 60.0%] train loss=2.095e-05 | eval RMSE=3.987e-03 (rel=4.474e-03, tstd=8.911e-01)\n","[ 65.0%] train loss=6.334e-06 | eval RMSE=3.727e-03 (rel=4.154e-03, tstd=8.972e-01)\n","[ 70.0%] train loss=1.597e-06 | eval RMSE=3.122e-03 (rel=3.542e-03, tstd=8.812e-01)\n","[ 75.0%] train loss=2.144e-06 | eval RMSE=2.581e-03 (rel=2.927e-03, tstd=8.819e-01) *best\n","[ 80.0%] train loss=4.488e-06 | eval RMSE=1.708e-03 (rel=1.866e-03, tstd=9.153e-01) *best\n","[ 85.0%] train loss=2.107e-06 | eval RMSE=1.518e-03 (rel=1.690e-03, tstd=8.986e-01) *best\n","[ 90.0%] train loss=1.361e-05 | eval RMSE=4.365e-03 (rel=4.876e-03, tstd=8.952e-01)\n","[ 95.0%] train loss=6.102e-06 | eval RMSE=2.731e-03 (rel=3.086e-03, tstd=8.849e-01)\n","[100.0%] train loss=2.087e-06 | eval RMSE=1.882e-03 (rel=2.099e-03, tstd=8.963e-01)\n","Final: RMSE=7.508e-03 (rel=8.454e-03) | time=296.2s\n","\n","================= quad_spd — d=20 =================\n","Q stats: ||Q||₂≈1.010, κ=94.76, trace=6.3 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=4.965e-04 | eval RMSE=3.474e-02 (rel=2.505e-02, tstd=1.386e+00) *best\n","[ 10.0%] train loss=3.450e-05 | eval RMSE=1.057e-02 (rel=7.593e-03, tstd=1.392e+00) *best\n","[ 15.0%] train loss=2.493e-05 | eval RMSE=7.561e-03 (rel=5.474e-03, tstd=1.381e+00) *best\n","[ 20.0%] train loss=7.242e-06 | eval RMSE=6.112e-03 (rel=4.378e-03, tstd=1.396e+00) *best\n","[ 25.0%] train loss=2.128e-05 | eval RMSE=3.962e-03 (rel=2.884e-03, tstd=1.374e+00) *best\n","[ 30.0%] train loss=6.922e-06 | eval RMSE=2.678e-03 (rel=1.934e-03, tstd=1.385e+00) *best\n","[ 35.0%] train loss=6.952e-06 | eval RMSE=9.588e-03 (rel=7.005e-03, tstd=1.369e+00)\n","[ 40.0%] train loss=7.354e-06 | eval RMSE=5.650e-03 (rel=4.158e-03, tstd=1.359e+00)\n","[ 45.0%] train loss=2.555e-06 | eval RMSE=5.996e-03 (rel=4.387e-03, tstd=1.367e+00)\n","[ 50.0%] train loss=2.535e-06 | eval RMSE=3.772e-03 (rel=2.731e-03, tstd=1.381e+00)\n","[ 55.0%] train loss=3.262e-06 | eval RMSE=5.621e-03 (rel=4.103e-03, tstd=1.370e+00)\n","[ 60.0%] train loss=7.216e-06 | eval RMSE=3.084e-03 (rel=2.239e-03, tstd=1.378e+00)\n","[ 65.0%] train loss=7.836e-06 | eval RMSE=4.045e-03 (rel=2.887e-03, tstd=1.401e+00)\n","[ 70.0%] train loss=5.398e-06 | eval RMSE=3.097e-03 (rel=2.207e-03, tstd=1.403e+00)\n","[ 75.0%] train loss=3.015e-06 | eval RMSE=1.202e-03 (rel=8.696e-04, tstd=1.382e+00) *best\n","[ 80.0%] train loss=2.104e-06 | eval RMSE=6.020e-03 (rel=4.365e-03, tstd=1.379e+00)\n","[ 85.0%] train loss=1.140e-05 | eval RMSE=3.952e-03 (rel=2.867e-03, tstd=1.379e+00)\n","[ 90.0%] train loss=7.011e-07 | eval RMSE=1.275e-03 (rel=9.287e-04, tstd=1.373e+00)\n","[ 95.0%] train loss=6.774e-07 | eval RMSE=1.403e-03 (rel=1.005e-03, tstd=1.396e+00)\n","[100.0%] train loss=2.097e-06 | eval RMSE=2.638e-03 (rel=1.951e-03, tstd=1.352e+00)\n","Final: RMSE=1.412e-03 (rel=1.013e-03) | time=515.3s\n","\n","================= quad_spd — d=50 =================\n","Q stats: ||Q||₂≈1.010, κ=100.45, trace=14.0 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=9.702e-05 | eval RMSE=2.721e-02 (rel=1.396e-02, tstd=1.950e+00) *best\n","[ 10.0%] train loss=3.096e-05 | eval RMSE=1.392e-02 (rel=7.085e-03, tstd=1.965e+00) *best\n","[ 15.0%] train loss=5.205e-06 | eval RMSE=8.553e-03 (rel=4.414e-03, tstd=1.938e+00) *best\n","[ 20.0%] train loss=1.409e-05 | eval RMSE=1.081e-02 (rel=5.496e-03, tstd=1.967e+00)\n","[ 25.0%] train loss=1.832e-05 | eval RMSE=1.094e-02 (rel=5.536e-03, tstd=1.977e+00)\n","[ 30.0%] train loss=1.980e-05 | eval RMSE=1.505e-02 (rel=7.765e-03, tstd=1.938e+00)\n","[ 35.0%] train loss=1.141e-05 | eval RMSE=1.376e-02 (rel=7.058e-03, tstd=1.949e+00)\n","[ 40.0%] train loss=1.074e-06 | eval RMSE=2.700e-03 (rel=1.397e-03, tstd=1.933e+00) *best\n","[ 45.0%] train loss=6.208e-06 | eval RMSE=8.466e-03 (rel=4.307e-03, tstd=1.966e+00)\n","[ 50.0%] train loss=7.549e-07 | eval RMSE=1.171e-03 (rel=6.092e-04, tstd=1.922e+00) *best\n","[ 55.0%] train loss=2.189e-06 | eval RMSE=8.108e-03 (rel=4.166e-03, tstd=1.946e+00)\n","[ 60.0%] train loss=7.957e-07 | eval RMSE=5.331e-03 (rel=2.720e-03, tstd=1.960e+00)\n","[ 65.0%] train loss=3.617e-06 | eval RMSE=1.133e-03 (rel=5.781e-04, tstd=1.960e+00) *best\n","[ 70.0%] train loss=1.564e-06 | eval RMSE=3.606e-03 (rel=1.847e-03, tstd=1.953e+00)\n","[ 75.0%] train loss=1.443e-07 | eval RMSE=1.542e-03 (rel=7.837e-04, tstd=1.968e+00)\n","[ 80.0%] train loss=2.237e-07 | eval RMSE=1.722e-03 (rel=8.859e-04, tstd=1.944e+00)\n","[ 85.0%] train loss=7.952e-07 | eval RMSE=2.228e-03 (rel=1.141e-03, tstd=1.952e+00)\n","[ 90.0%] train loss=3.665e-07 | eval RMSE=1.076e-03 (rel=5.579e-04, tstd=1.929e+00) *best\n","[ 95.0%] train loss=3.214e-06 | eval RMSE=4.285e-03 (rel=2.225e-03, tstd=1.926e+00)\n","[100.0%] train loss=2.624e-07 | eval RMSE=1.574e-03 (rel=7.962e-04, tstd=1.977e+00)\n","Final: RMSE=4.475e-03 (rel=2.269e-03) | time=1336.1s\n","\n","================= exp_minus_lin — d=10 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=384, depth=4), params=1.19M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=6.767e-04 | eval RMSE=1.540e-01 (rel=4.011e-02, tstd=3.838e+00) *best\n","[ 10.0%] train loss=1.320e-04 | eval RMSE=7.526e-02 (rel=1.951e-02, tstd=3.858e+00) *best\n","[ 15.0%] train loss=1.855e-04 | eval RMSE=3.359e-02 (rel=9.779e-03, tstd=3.435e+00) *best\n","[ 20.0%] train loss=1.122e-03 | eval RMSE=2.551e-01 (rel=6.201e-02, tstd=4.115e+00)\n","[ 25.0%] train loss=7.906e-05 | eval RMSE=7.464e-02 (rel=2.088e-02, tstd=3.574e+00)\n","[ 30.0%] train loss=3.616e-04 | eval RMSE=1.772e-01 (rel=3.784e-02, tstd=4.684e+00)\n","[ 35.0%] train loss=7.784e-05 | eval RMSE=7.295e-01 (rel=1.320e-01, tstd=5.525e+00)\n","[ 40.0%] train loss=9.710e-05 | eval RMSE=4.072e-02 (rel=1.025e-02, tstd=3.973e+00)\n","[ 45.0%] train loss=5.905e-05 | eval RMSE=3.886e-02 (rel=1.229e-02, tstd=3.161e+00)\n","[ 50.0%] train loss=4.594e-05 | eval RMSE=3.443e-02 (rel=1.176e-02, tstd=2.929e+00)\n","[ 55.0%] train loss=6.732e-04 | eval RMSE=1.521e-01 (rel=3.785e-02, tstd=4.018e+00)\n","[ 60.0%] train loss=4.673e-05 | eval RMSE=2.060e-02 (rel=6.296e-03, tstd=3.271e+00) *best\n","[ 65.0%] train loss=4.502e-05 | eval RMSE=1.990e-02 (rel=6.299e-03, tstd=3.159e+00) *best\n","[ 70.0%] train loss=5.616e-05 | eval RMSE=6.686e-02 (rel=1.858e-02, tstd=3.598e+00)\n","[ 75.0%] train loss=3.127e-04 | eval RMSE=1.322e-01 (rel=3.539e-02, tstd=3.735e+00)\n","[ 80.0%] train loss=3.738e-04 | eval RMSE=7.198e-02 (rel=2.147e-02, tstd=3.353e+00)\n","[ 85.0%] train loss=9.224e-06 | eval RMSE=1.212e-02 (rel=3.574e-03, tstd=3.390e+00) *best\n","[ 90.0%] train loss=3.653e-05 | eval RMSE=3.298e-02 (rel=1.069e-02, tstd=3.086e+00)\n","[ 95.0%] train loss=1.078e-03 | eval RMSE=2.125e-01 (rel=5.914e-02, tstd=3.594e+00)\n","[100.0%] train loss=3.572e-05 | eval RMSE=5.611e-02 (rel=1.552e-02, tstd=3.616e+00)\n","Final: RMSE=1.368e-01 (rel=3.686e-02) | time=309.0s\n","\n","================= exp_minus_lin — d=20 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=512, depth=5), params=2.64M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=4.378e-05 | eval RMSE=7.109e-02 (rel=2.026e-02, tstd=3.510e+00) *best\n","[ 10.0%] train loss=2.148e-04 | eval RMSE=6.369e-02 (rel=1.711e-02, tstd=3.723e+00) *best\n","[ 15.0%] train loss=3.058e-04 | eval RMSE=4.857e-02 (rel=1.432e-02, tstd=3.391e+00) *best\n","[ 20.0%] train loss=8.306e-05 | eval RMSE=4.026e-02 (rel=9.117e-03, tstd=4.416e+00) *best\n","[ 25.0%] train loss=1.265e-04 | eval RMSE=4.588e-02 (rel=1.403e-02, tstd=3.270e+00)\n","[ 30.0%] train loss=5.319e-05 | eval RMSE=1.720e-01 (rel=3.741e-02, tstd=4.599e+00)\n","[ 35.0%] train loss=2.586e-04 | eval RMSE=7.669e-02 (rel=1.949e-02, tstd=3.934e+00)\n","[ 40.0%] train loss=5.873e-06 | eval RMSE=5.663e-02 (rel=1.166e-02, tstd=4.857e+00)\n","[ 45.0%] train loss=2.405e-04 | eval RMSE=6.449e-02 (rel=1.929e-02, tstd=3.344e+00)\n","[ 50.0%] train loss=1.403e-05 | eval RMSE=2.723e-02 (rel=8.725e-03, tstd=3.121e+00) *best\n","[ 55.0%] train loss=1.270e-04 | eval RMSE=5.030e-02 (rel=1.272e-02, tstd=3.954e+00)\n","[ 60.0%] train loss=1.409e-04 | eval RMSE=7.324e-02 (rel=1.822e-02, tstd=4.021e+00)\n","[ 65.0%] train loss=1.899e-04 | eval RMSE=6.269e-02 (rel=1.969e-02, tstd=3.184e+00)\n","[ 70.0%] train loss=9.502e-04 | eval RMSE=1.063e-01 (rel=3.081e-02, tstd=3.449e+00)\n","[ 75.0%] train loss=3.473e-05 | eval RMSE=2.678e-02 (rel=7.587e-03, tstd=3.530e+00) *best\n","[ 80.0%] train loss=1.705e-03 | eval RMSE=1.436e-01 (rel=4.522e-02, tstd=3.176e+00)\n","[ 85.0%] train loss=6.759e-05 | eval RMSE=3.429e-02 (rel=9.713e-03, tstd=3.530e+00)\n","[ 90.0%] train loss=4.174e-04 | eval RMSE=1.857e-01 (rel=3.584e-02, tstd=5.183e+00)\n","[ 95.0%] train loss=2.412e-04 | eval RMSE=8.405e-02 (rel=2.282e-02, tstd=3.683e+00)\n","[100.0%] train loss=6.165e-04 | eval RMSE=2.322e-01 (rel=4.278e-02, tstd=5.428e+00)\n","Final: RMSE=4.442e-02 (rel=1.310e-02) | time=532.3s\n","\n","================= exp_minus_lin — d=50 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=768, depth=6), params=7.11M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=1.860e-05 | eval RMSE=3.731e-02 (rel=1.080e-02, tstd=3.456e+00) *best\n","[ 10.0%] train loss=2.105e-04 | eval RMSE=4.802e-01 (rel=7.788e-02, tstd=6.166e+00)\n","[ 15.0%] train loss=2.747e-04 | eval RMSE=1.132e-01 (rel=2.830e-02, tstd=3.999e+00)\n","[ 20.0%] train loss=3.369e-04 | eval RMSE=6.882e-02 (rel=2.054e-02, tstd=3.351e+00)\n","[ 25.0%] train loss=1.244e-05 | eval RMSE=2.817e-02 (rel=9.194e-03, tstd=3.064e+00) *best\n","[ 30.0%] train loss=6.195e-06 | eval RMSE=2.237e-02 (rel=5.235e-03, tstd=4.274e+00) *best\n","[ 35.0%] train loss=1.029e-04 | eval RMSE=3.750e-02 (rel=7.932e-03, tstd=4.728e+00)\n","[ 40.0%] train loss=4.737e-06 | eval RMSE=2.949e-02 (rel=6.157e-03, tstd=4.791e+00)\n","[ 45.0%] train loss=4.167e-05 | eval RMSE=4.250e-02 (rel=9.734e-03, tstd=4.366e+00)\n","[ 50.0%] train loss=3.705e-05 | eval RMSE=2.471e-02 (rel=4.965e-03, tstd=4.977e+00)\n","[ 55.0%] train loss=3.759e-05 | eval RMSE=2.379e-02 (rel=7.837e-03, tstd=3.036e+00)\n","[ 60.0%] train loss=2.121e-05 | eval RMSE=2.629e-02 (rel=7.568e-03, tstd=3.474e+00)\n","[ 65.0%] train loss=1.049e-04 | eval RMSE=5.902e-02 (rel=1.799e-02, tstd=3.281e+00)\n","[ 70.0%] train loss=1.071e-04 | eval RMSE=6.233e-02 (rel=1.582e-02, tstd=3.941e+00)\n","[ 75.0%] train loss=2.420e-05 | eval RMSE=2.620e-02 (rel=6.497e-03, tstd=4.033e+00)\n","[ 80.0%] train loss=5.943e-06 | eval RMSE=2.527e-02 (rel=5.981e-03, tstd=4.224e+00)\n","[ 85.0%] train loss=9.511e-05 | eval RMSE=4.435e-02 (rel=1.215e-02, tstd=3.651e+00)\n","[ 90.0%] train loss=6.804e-04 | eval RMSE=8.584e-02 (rel=2.581e-02, tstd=3.326e+00)\n","[ 95.0%] train loss=8.831e-06 | eval RMSE=1.084e-02 (rel=2.689e-03, tstd=4.031e+00) *best\n","[100.0%] train loss=1.112e-04 | eval RMSE=7.209e-02 (rel=1.783e-02, tstd=4.043e+00)\n","Final: RMSE=4.462e-02 (rel=1.182e-02) | time=1333.1s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=91.28, ridge=0.01\n","[pre  20.0%] train MSE=2.864e+07 | eval RMSE=5.327e+03\n","[pre  40.0%] train MSE=1.897e+07 | eval RMSE=4.341e+03\n","[pre  60.0%] train MSE=1.465e+07 | eval RMSE=3.826e+03\n","[pre  80.0%] train MSE=1.312e+07 | eval RMSE=3.618e+03\n","[pre 100.0%] train MSE=1.283e+07 | eval RMSE=3.578e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=10 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=127.87 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=5.718e-03 | eval RMSE=2.427e-01 (rel=2.837e-01, tstd=8.557e-01) *best\n","[ 10.0%] train loss=3.351e-05 | eval RMSE=5.015e-03 (rel=5.830e-03, tstd=8.603e-01) *best\n","[ 15.0%] train loss=1.154e-04 | eval RMSE=1.426e-02 (rel=1.666e-02, tstd=8.563e-01)\n","[ 20.0%] train loss=2.013e-05 | eval RMSE=6.739e-03 (rel=7.986e-03, tstd=8.438e-01)\n","[ 25.0%] train loss=4.638e-06 | eval RMSE=5.651e-03 (rel=6.623e-03, tstd=8.532e-01)\n","[ 30.0%] train loss=3.797e-05 | eval RMSE=4.001e-03 (rel=4.764e-03, tstd=8.397e-01) *best\n","[ 35.0%] train loss=1.470e-04 | eval RMSE=1.120e-02 (rel=1.324e-02, tstd=8.462e-01)\n","[ 40.0%] train loss=1.276e-05 | eval RMSE=5.090e-03 (rel=6.078e-03, tstd=8.375e-01)\n","[ 45.0%] train loss=8.369e-06 | eval RMSE=4.551e-03 (rel=5.433e-03, tstd=8.376e-01)\n","[ 50.0%] train loss=2.529e-06 | eval RMSE=3.717e-03 (rel=4.379e-03, tstd=8.488e-01) *best\n","[ 55.0%] train loss=9.298e-06 | eval RMSE=2.943e-03 (rel=3.476e-03, tstd=8.467e-01) *best\n","[ 60.0%] train loss=7.293e-07 | eval RMSE=1.181e-03 (rel=1.369e-03, tstd=8.628e-01) *best\n","[ 65.0%] train loss=8.670e-06 | eval RMSE=3.744e-03 (rel=4.398e-03, tstd=8.514e-01)\n","[ 70.0%] train loss=9.720e-07 | eval RMSE=2.354e-03 (rel=2.712e-03, tstd=8.682e-01)\n","[ 75.0%] train loss=2.109e-06 | eval RMSE=1.026e-03 (rel=1.182e-03, tstd=8.679e-01) *best\n","[ 80.0%] train loss=3.847e-06 | eval RMSE=8.267e-04 (rel=9.656e-04, tstd=8.562e-01) *best\n","[ 85.0%] train loss=1.053e-06 | eval RMSE=2.207e-03 (rel=2.644e-03, tstd=8.348e-01)\n","[ 90.0%] train loss=2.245e-06 | eval RMSE=3.426e-03 (rel=4.044e-03, tstd=8.474e-01)\n","[ 95.0%] train loss=7.663e-06 | eval RMSE=2.149e-03 (rel=2.554e-03, tstd=8.414e-01)\n","[100.0%] train loss=4.613e-06 | eval RMSE=2.455e-03 (rel=2.858e-03, tstd=8.589e-01)\n","Final: RMSE=1.560e-02 (rel=1.819e-02) | time=361.7s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=100.85, ridge=0.01\n","[pre  20.0%] train MSE=2.803e+07 | eval RMSE=5.275e+03\n","[pre  40.0%] train MSE=1.837e+07 | eval RMSE=4.276e+03\n","[pre  60.0%] train MSE=1.414e+07 | eval RMSE=3.758e+03\n","[pre  80.0%] train MSE=1.268e+07 | eval RMSE=3.556e+03\n","[pre 100.0%] train MSE=1.240e+07 | eval RMSE=3.518e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=20 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=71.07 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=9.060e-05 | eval RMSE=5.756e-03 (rel=1.463e-02, tstd=3.935e-01) *best\n","[ 10.0%] train loss=8.227e-05 | eval RMSE=5.626e-03 (rel=1.464e-02, tstd=3.843e-01) *best\n","[ 15.0%] train loss=1.838e-05 | eval RMSE=2.538e-03 (rel=6.471e-03, tstd=3.922e-01) *best\n","[ 20.0%] train loss=6.326e-06 | eval RMSE=1.420e-03 (rel=3.638e-03, tstd=3.902e-01) *best\n","[ 25.0%] train loss=2.058e-05 | eval RMSE=4.379e-03 (rel=1.150e-02, tstd=3.808e-01)\n","[ 30.0%] train loss=2.083e-05 | eval RMSE=2.041e-03 (rel=5.220e-03, tstd=3.909e-01)\n","[ 35.0%] train loss=3.469e-05 | eval RMSE=2.995e-03 (rel=7.660e-03, tstd=3.910e-01)\n","[ 40.0%] train loss=7.873e-06 | eval RMSE=9.119e-04 (rel=2.349e-03, tstd=3.882e-01) *best\n","[ 45.0%] train loss=3.509e-06 | eval RMSE=1.457e-03 (rel=3.749e-03, tstd=3.886e-01)\n","[ 50.0%] train loss=1.182e-06 | eval RMSE=7.760e-04 (rel=2.003e-03, tstd=3.874e-01) *best\n","[ 55.0%] train loss=1.749e-06 | eval RMSE=8.295e-04 (rel=2.143e-03, tstd=3.871e-01)\n","[ 60.0%] train loss=1.919e-06 | eval RMSE=5.848e-04 (rel=1.528e-03, tstd=3.828e-01) *best\n","[ 65.0%] train loss=2.026e-06 | eval RMSE=4.606e-04 (rel=1.191e-03, tstd=3.866e-01) *best\n","[ 70.0%] train loss=1.464e-06 | eval RMSE=7.824e-04 (rel=2.005e-03, tstd=3.902e-01)\n","[ 75.0%] train loss=3.070e-06 | eval RMSE=4.932e-04 (rel=1.253e-03, tstd=3.935e-01)\n","[ 80.0%] train loss=4.999e-07 | eval RMSE=3.390e-04 (rel=8.584e-04, tstd=3.949e-01) *best\n","[ 85.0%] train loss=4.773e-07 | eval RMSE=7.634e-04 (rel=1.970e-03, tstd=3.875e-01)\n","[ 90.0%] train loss=2.312e-06 | eval RMSE=1.032e-03 (rel=2.598e-03, tstd=3.973e-01)\n","[ 95.0%] train loss=1.568e-06 | eval RMSE=7.349e-04 (rel=1.878e-03, tstd=3.913e-01)\n","[100.0%] train loss=1.203e-06 | eval RMSE=5.901e-04 (rel=1.520e-03, tstd=3.883e-01)\n","Final: RMSE=5.532e-04 (rel=1.422e-03) | time=611.6s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=100.93, ridge=0.01\n","[pre  20.0%] train MSE=2.835e+07 | eval RMSE=5.294e+03\n","[pre  40.0%] train MSE=1.854e+07 | eval RMSE=4.301e+03\n","[pre  60.0%] train MSE=1.445e+07 | eval RMSE=3.797e+03\n","[pre  80.0%] train MSE=1.298e+07 | eval RMSE=3.602e+03\n","[pre 100.0%] train MSE=1.272e+07 | eval RMSE=3.566e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=50 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=28.38 → scale=0.11); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=3.041e-04 | eval RMSE=2.716e-03 (rel=3.006e-02, tstd=9.036e-02) *best\n","[ 10.0%] train loss=2.347e-04 | eval RMSE=1.342e-03 (rel=1.469e-02, tstd=9.139e-02) *best\n","[ 15.0%] train loss=1.614e-05 | eval RMSE=1.429e-03 (rel=1.553e-02, tstd=9.207e-02)\n","[ 20.0%] train loss=7.978e-05 | eval RMSE=5.253e-04 (rel=5.669e-03, tstd=9.265e-02) *best\n","[ 25.0%] train loss=1.594e-04 | eval RMSE=1.053e-03 (rel=1.148e-02, tstd=9.176e-02)\n","[ 30.0%] train loss=6.004e-06 | eval RMSE=4.957e-04 (rel=5.390e-03, tstd=9.196e-02) *best\n","[ 35.0%] train loss=7.275e-06 | eval RMSE=3.936e-04 (rel=4.352e-03, tstd=9.044e-02) *best\n","[ 40.0%] train loss=4.162e-05 | eval RMSE=4.353e-04 (rel=4.816e-03, tstd=9.037e-02)\n","[ 45.0%] train loss=8.816e-06 | eval RMSE=7.159e-04 (rel=7.853e-03, tstd=9.116e-02)\n","[ 50.0%] train loss=1.799e-05 | eval RMSE=5.100e-04 (rel=5.637e-03, tstd=9.046e-02)\n","[ 55.0%] train loss=5.403e-06 | eval RMSE=8.752e-04 (rel=9.650e-03, tstd=9.069e-02)\n","[ 60.0%] train loss=9.307e-06 | eval RMSE=5.110e-04 (rel=5.556e-03, tstd=9.196e-02)\n","[ 65.0%] train loss=7.672e-06 | eval RMSE=3.100e-04 (rel=3.354e-03, tstd=9.243e-02) *best\n","[ 70.0%] train loss=4.950e-06 | eval RMSE=4.946e-04 (rel=5.357e-03, tstd=9.233e-02)\n","[ 75.0%] train loss=3.908e-06 | eval RMSE=2.721e-04 (rel=3.003e-03, tstd=9.060e-02) *best\n","[ 80.0%] train loss=3.783e-06 | eval RMSE=2.777e-04 (rel=3.021e-03, tstd=9.193e-02)\n","[ 85.0%] train loss=5.768e-06 | eval RMSE=3.528e-04 (rel=3.853e-03, tstd=9.156e-02)\n","[ 90.0%] train loss=4.019e-06 | eval RMSE=2.683e-04 (rel=2.931e-03, tstd=9.156e-02) *best\n","[ 95.0%] train loss=4.249e-06 | eval RMSE=2.678e-04 (rel=3.001e-03, tstd=8.923e-02) *best\n","[100.0%] train loss=4.459e-06 | eval RMSE=2.671e-04 (rel=2.938e-03, tstd=9.091e-02) *best\n","Final: RMSE=2.657e-04 (rel=2.885e-03) | time=1359.9s\n","\n","================= softplus_pairs — d=10 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=6.143e-04 | eval RMSE=3.773e-02 (rel=3.907e-02, tstd=9.656e-01) *best\n","[ 10.0%] train loss=1.347e-04 | eval RMSE=8.864e-03 (rel=9.097e-03, tstd=9.744e-01) *best\n","[ 15.0%] train loss=1.935e-04 | eval RMSE=2.561e-02 (rel=2.638e-02, tstd=9.709e-01)\n","[ 20.0%] train loss=2.617e-05 | eval RMSE=5.908e-03 (rel=6.130e-03, tstd=9.639e-01) *best\n","[ 25.0%] train loss=2.163e-05 | eval RMSE=5.099e-03 (rel=5.259e-03, tstd=9.695e-01) *best\n","[ 30.0%] train loss=2.407e-05 | eval RMSE=7.199e-03 (rel=7.369e-03, tstd=9.770e-01)\n","[ 35.0%] train loss=1.491e-05 | eval RMSE=4.056e-03 (rel=4.147e-03, tstd=9.780e-01) *best\n","[ 40.0%] train loss=2.963e-05 | eval RMSE=7.636e-03 (rel=7.762e-03, tstd=9.837e-01)\n","[ 45.0%] train loss=2.525e-05 | eval RMSE=4.594e-03 (rel=4.762e-03, tstd=9.647e-01)\n","[ 50.0%] train loss=1.916e-05 | eval RMSE=5.657e-03 (rel=5.874e-03, tstd=9.631e-01)\n","[ 55.0%] train loss=3.883e-06 | eval RMSE=5.114e-03 (rel=5.298e-03, tstd=9.654e-01)\n","[ 60.0%] train loss=1.389e-05 | eval RMSE=4.467e-03 (rel=4.562e-03, tstd=9.791e-01)\n","[ 65.0%] train loss=1.769e-06 | eval RMSE=3.345e-03 (rel=3.425e-03, tstd=9.766e-01) *best\n","[ 70.0%] train loss=1.070e-06 | eval RMSE=2.499e-03 (rel=2.579e-03, tstd=9.689e-01) *best\n","[ 75.0%] train loss=1.206e-06 | eval RMSE=2.241e-03 (rel=2.317e-03, tstd=9.671e-01) *best\n","[ 80.0%] train loss=7.627e-06 | eval RMSE=1.727e-03 (rel=1.759e-03, tstd=9.816e-01) *best\n","[ 85.0%] train loss=2.740e-06 | eval RMSE=1.896e-03 (rel=1.925e-03, tstd=9.852e-01)\n","[ 90.0%] train loss=1.141e-06 | eval RMSE=1.150e-03 (rel=1.193e-03, tstd=9.641e-01) *best\n","[ 95.0%] train loss=2.270e-06 | eval RMSE=2.692e-03 (rel=2.734e-03, tstd=9.844e-01)\n","[100.0%] train loss=9.917e-06 | eval RMSE=4.246e-03 (rel=4.386e-03, tstd=9.680e-01)\n","Final: RMSE=4.268e-03 (rel=4.375e-03) | time=493.8s\n","\n","================= softplus_pairs — d=20 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=5.880e-04 | eval RMSE=8.931e-02 (rel=3.147e-02, tstd=2.838e+00) *best\n","[ 10.0%] train loss=1.067e-04 | eval RMSE=3.294e-02 (rel=1.153e-02, tstd=2.857e+00) *best\n","[ 15.0%] train loss=1.673e-04 | eval RMSE=4.648e-02 (rel=1.640e-02, tstd=2.834e+00)\n","[ 20.0%] train loss=6.587e-05 | eval RMSE=3.401e-02 (rel=1.196e-02, tstd=2.843e+00)\n","[ 25.0%] train loss=1.426e-05 | eval RMSE=1.231e-02 (rel=4.315e-03, tstd=2.854e+00) *best\n","[ 30.0%] train loss=1.428e-05 | eval RMSE=1.164e-02 (rel=4.060e-03, tstd=2.866e+00) *best\n","[ 35.0%] train loss=8.867e-06 | eval RMSE=1.248e-02 (rel=4.470e-03, tstd=2.793e+00)\n","[ 40.0%] train loss=7.708e-06 | eval RMSE=1.200e-02 (rel=4.300e-03, tstd=2.792e+00)\n","[ 45.0%] train loss=1.133e-05 | eval RMSE=2.238e-02 (rel=8.009e-03, tstd=2.794e+00)\n","[ 50.0%] train loss=8.193e-06 | eval RMSE=7.790e-03 (rel=2.728e-03, tstd=2.856e+00) *best\n","[ 55.0%] train loss=6.584e-06 | eval RMSE=1.071e-02 (rel=3.802e-03, tstd=2.818e+00)\n","[ 60.0%] train loss=1.480e-05 | eval RMSE=6.842e-03 (rel=2.415e-03, tstd=2.834e+00) *best\n","[ 65.0%] train loss=3.664e-06 | eval RMSE=5.765e-03 (rel=2.016e-03, tstd=2.859e+00) *best\n","[ 70.0%] train loss=3.354e-06 | eval RMSE=4.964e-03 (rel=1.759e-03, tstd=2.821e+00) *best\n","[ 75.0%] train loss=1.044e-06 | eval RMSE=3.805e-03 (rel=1.335e-03, tstd=2.851e+00) *best\n","[ 80.0%] train loss=1.592e-06 | eval RMSE=1.240e-02 (rel=4.279e-03, tstd=2.898e+00)\n","[ 85.0%] train loss=7.632e-07 | eval RMSE=5.653e-03 (rel=2.008e-03, tstd=2.816e+00)\n","[ 90.0%] train loss=3.368e-06 | eval RMSE=2.711e-03 (rel=9.623e-04, tstd=2.817e+00) *best\n","[ 95.0%] train loss=2.451e-06 | eval RMSE=4.310e-03 (rel=1.509e-03, tstd=2.856e+00)\n","[100.0%] train loss=4.538e-07 | eval RMSE=2.625e-03 (rel=9.277e-04, tstd=2.829e+00) *best\n","Final: RMSE=2.646e-03 (rel=9.254e-04) | time=1131.9s\n","\n","================= softplus_pairs — d=50 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=8.947e-04 | eval RMSE=3.601e-01 (rel=3.152e-02, tstd=1.143e+01) *best\n","[ 10.0%] train loss=3.224e-04 | eval RMSE=2.808e-01 (rel=2.529e-02, tstd=1.111e+01) *best\n","[ 15.0%] train loss=3.662e-04 | eval RMSE=3.073e-01 (rel=2.749e-02, tstd=1.118e+01)\n","[ 20.0%] train loss=2.996e-04 | eval RMSE=2.797e-01 (rel=2.483e-02, tstd=1.126e+01) *best\n","[ 25.0%] train loss=2.942e-04 | eval RMSE=2.800e-01 (rel=2.478e-02, tstd=1.130e+01)\n","[ 30.0%] train loss=2.829e-04 | eval RMSE=2.811e-01 (rel=2.473e-02, tstd=1.137e+01)\n","[ 35.0%] train loss=2.487e-04 | eval RMSE=2.673e-01 (rel=2.328e-02, tstd=1.148e+01) *best\n","[ 40.0%] train loss=2.571e-04 | eval RMSE=2.597e-01 (rel=2.283e-02, tstd=1.137e+01) *best\n","[ 45.0%] train loss=2.869e-04 | eval RMSE=2.705e-01 (rel=2.344e-02, tstd=1.154e+01)\n","[ 50.0%] train loss=2.545e-04 | eval RMSE=2.641e-01 (rel=2.296e-02, tstd=1.150e+01)\n","[ 55.0%] train loss=2.314e-04 | eval RMSE=2.463e-01 (rel=2.162e-02, tstd=1.139e+01) *best\n","[ 60.0%] train loss=2.361e-04 | eval RMSE=2.446e-01 (rel=2.162e-02, tstd=1.132e+01) *best\n","[ 65.0%] train loss=1.958e-04 | eval RMSE=2.390e-01 (rel=2.105e-02, tstd=1.136e+01) *best\n","[ 70.0%] train loss=2.213e-04 | eval RMSE=2.344e-01 (rel=2.067e-02, tstd=1.134e+01) *best\n","[ 75.0%] train loss=1.971e-04 | eval RMSE=2.291e-01 (rel=1.986e-02, tstd=1.153e+01) *best\n","[ 80.0%] train loss=1.999e-04 | eval RMSE=2.219e-01 (rel=1.956e-02, tstd=1.134e+01) *best\n","[ 85.0%] train loss=1.857e-04 | eval RMSE=2.290e-01 (rel=2.022e-02, tstd=1.132e+01)\n","[ 90.0%] train loss=2.194e-04 | eval RMSE=2.199e-01 (rel=1.945e-02, tstd=1.131e+01) *best\n","[ 95.0%] train loss=1.926e-04 | eval RMSE=2.251e-01 (rel=1.996e-02, tstd=1.128e+01)\n","[100.0%] train loss=1.966e-04 | eval RMSE=2.233e-01 (rel=1.979e-02, tstd=1.129e+01)\n","Final: RMSE=2.264e-01 (rel=1.986e-02) | time=2857.2s\n","\n","============================ TRIAL 4/5 (seed=3000051) ============================\n","\n","================= quad_spd — d=10 =================\n","Q stats: ||Q||₂≈1.010, κ=100.73, trace=3.5 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=9.496e-05 | eval RMSE=1.417e-02 (rel=1.338e-02, tstd=1.059e+00) *best\n","[ 10.0%] train loss=1.214e-04 | eval RMSE=1.649e-02 (rel=1.557e-02, tstd=1.059e+00)\n","[ 15.0%] train loss=5.872e-05 | eval RMSE=7.291e-03 (rel=7.004e-03, tstd=1.041e+00) *best\n","[ 20.0%] train loss=4.431e-05 | eval RMSE=8.647e-03 (rel=8.068e-03, tstd=1.072e+00)\n","[ 25.0%] train loss=4.901e-05 | eval RMSE=7.389e-03 (rel=6.912e-03, tstd=1.069e+00)\n","[ 30.0%] train loss=1.965e-05 | eval RMSE=1.088e-02 (rel=1.022e-02, tstd=1.065e+00)\n","[ 35.0%] train loss=1.704e-05 | eval RMSE=1.016e-02 (rel=9.591e-03, tstd=1.059e+00)\n","[ 40.0%] train loss=2.534e-05 | eval RMSE=6.915e-03 (rel=6.498e-03, tstd=1.064e+00) *best\n","[ 45.0%] train loss=1.743e-05 | eval RMSE=3.950e-03 (rel=3.739e-03, tstd=1.056e+00) *best\n","[ 50.0%] train loss=4.568e-06 | eval RMSE=4.442e-03 (rel=4.139e-03, tstd=1.073e+00)\n","[ 55.0%] train loss=9.460e-06 | eval RMSE=2.562e-03 (rel=2.419e-03, tstd=1.059e+00) *best\n","[ 60.0%] train loss=3.654e-05 | eval RMSE=2.960e-03 (rel=2.796e-03, tstd=1.059e+00)\n","[ 65.0%] train loss=1.594e-05 | eval RMSE=4.837e-03 (rel=4.510e-03, tstd=1.073e+00)\n","[ 70.0%] train loss=1.482e-06 | eval RMSE=5.260e-03 (rel=4.971e-03, tstd=1.058e+00)\n","[ 75.0%] train loss=2.748e-06 | eval RMSE=3.766e-03 (rel=3.593e-03, tstd=1.048e+00)\n","[ 80.0%] train loss=1.859e-06 | eval RMSE=1.774e-03 (rel=1.700e-03, tstd=1.044e+00) *best\n","[ 85.0%] train loss=3.708e-07 | eval RMSE=9.829e-04 (rel=9.438e-04, tstd=1.041e+00) *best\n","[ 90.0%] train loss=5.951e-06 | eval RMSE=3.044e-03 (rel=2.860e-03, tstd=1.064e+00)\n","[ 95.0%] train loss=2.934e-05 | eval RMSE=6.509e-03 (rel=6.209e-03, tstd=1.048e+00)\n","[100.0%] train loss=5.552e-06 | eval RMSE=3.524e-03 (rel=3.282e-03, tstd=1.074e+00)\n","Final: RMSE=1.410e-02 (rel=1.319e-02) | time=296.0s\n","\n","================= quad_spd — d=20 =================\n","Q stats: ||Q||₂≈1.010, κ=96.67, trace=6.3 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=1.565e-04 | eval RMSE=2.389e-02 (rel=1.771e-02, tstd=1.349e+00) *best\n","[ 10.0%] train loss=6.047e-05 | eval RMSE=1.944e-02 (rel=1.433e-02, tstd=1.356e+00) *best\n","[ 15.0%] train loss=1.158e-05 | eval RMSE=1.026e-02 (rel=7.645e-03, tstd=1.342e+00) *best\n","[ 20.0%] train loss=2.554e-05 | eval RMSE=5.765e-03 (rel=4.370e-03, tstd=1.319e+00) *best\n","[ 25.0%] train loss=1.764e-05 | eval RMSE=5.343e-03 (rel=3.982e-03, tstd=1.342e+00) *best\n","[ 30.0%] train loss=8.103e-06 | eval RMSE=7.685e-03 (rel=5.752e-03, tstd=1.336e+00)\n","[ 35.0%] train loss=8.006e-06 | eval RMSE=5.537e-03 (rel=4.086e-03, tstd=1.355e+00)\n","[ 40.0%] train loss=2.550e-05 | eval RMSE=7.071e-03 (rel=5.151e-03, tstd=1.373e+00)\n","[ 45.0%] train loss=2.042e-05 | eval RMSE=6.638e-03 (rel=4.955e-03, tstd=1.340e+00)\n","[ 50.0%] train loss=4.202e-06 | eval RMSE=5.069e-03 (rel=3.765e-03, tstd=1.347e+00) *best\n","[ 55.0%] train loss=3.549e-06 | eval RMSE=7.179e-03 (rel=5.265e-03, tstd=1.364e+00)\n","[ 60.0%] train loss=7.107e-06 | eval RMSE=1.895e-03 (rel=1.393e-03, tstd=1.360e+00) *best\n","[ 65.0%] train loss=2.042e-05 | eval RMSE=4.244e-03 (rel=3.162e-03, tstd=1.342e+00)\n","[ 70.0%] train loss=1.284e-06 | eval RMSE=5.921e-03 (rel=4.364e-03, tstd=1.357e+00)\n","[ 75.0%] train loss=5.570e-07 | eval RMSE=2.426e-03 (rel=1.789e-03, tstd=1.356e+00)\n","[ 80.0%] train loss=8.001e-07 | eval RMSE=3.398e-03 (rel=2.530e-03, tstd=1.343e+00)\n","[ 85.0%] train loss=7.989e-07 | eval RMSE=2.577e-03 (rel=1.928e-03, tstd=1.336e+00)\n","[ 90.0%] train loss=1.323e-06 | eval RMSE=1.953e-03 (rel=1.419e-03, tstd=1.376e+00)\n","[ 95.0%] train loss=2.492e-06 | eval RMSE=2.534e-03 (rel=1.844e-03, tstd=1.374e+00)\n","[100.0%] train loss=3.261e-07 | eval RMSE=1.024e-03 (rel=7.650e-04, tstd=1.339e+00) *best\n","Final: RMSE=1.055e-03 (rel=7.743e-04) | time=515.6s\n","\n","================= quad_spd — d=50 =================\n","Q stats: ||Q||₂≈1.010, κ=100.91, trace=14.3 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=3.273e-05 | eval RMSE=1.666e-02 (rel=8.378e-03, tstd=1.989e+00) *best\n","[ 10.0%] train loss=2.004e-04 | eval RMSE=3.886e-02 (rel=1.952e-02, tstd=1.991e+00)\n","[ 15.0%] train loss=1.564e-06 | eval RMSE=4.224e-03 (rel=2.129e-03, tstd=1.984e+00) *best\n","[ 20.0%] train loss=2.275e-05 | eval RMSE=1.453e-02 (rel=7.291e-03, tstd=1.992e+00)\n","[ 25.0%] train loss=4.002e-05 | eval RMSE=2.022e-02 (rel=1.013e-02, tstd=1.997e+00)\n","[ 30.0%] train loss=1.266e-06 | eval RMSE=2.471e-03 (rel=1.236e-03, tstd=1.999e+00) *best\n","[ 35.0%] train loss=3.810e-06 | eval RMSE=4.524e-03 (rel=2.296e-03, tstd=1.970e+00)\n","[ 40.0%] train loss=8.274e-06 | eval RMSE=3.525e-03 (rel=1.779e-03, tstd=1.981e+00)\n","[ 45.0%] train loss=7.559e-06 | eval RMSE=9.043e-03 (rel=4.522e-03, tstd=2.000e+00)\n","[ 50.0%] train loss=4.419e-06 | eval RMSE=4.156e-03 (rel=2.084e-03, tstd=1.994e+00)\n","[ 55.0%] train loss=4.853e-06 | eval RMSE=4.768e-03 (rel=2.372e-03, tstd=2.010e+00)\n","[ 60.0%] train loss=6.164e-06 | eval RMSE=4.397e-03 (rel=2.219e-03, tstd=1.981e+00)\n","[ 65.0%] train loss=8.968e-07 | eval RMSE=1.013e-03 (rel=5.045e-04, tstd=2.009e+00) *best\n","[ 70.0%] train loss=3.553e-07 | eval RMSE=4.427e-03 (rel=2.249e-03, tstd=1.969e+00)\n","[ 75.0%] train loss=7.432e-07 | eval RMSE=4.025e-03 (rel=2.004e-03, tstd=2.008e+00)\n","[ 80.0%] train loss=6.352e-07 | eval RMSE=9.552e-04 (rel=4.866e-04, tstd=1.963e+00) *best\n","[ 85.0%] train loss=7.344e-08 | eval RMSE=1.869e-03 (rel=9.393e-04, tstd=1.990e+00)\n","[ 90.0%] train loss=2.119e-06 | eval RMSE=2.918e-03 (rel=1.481e-03, tstd=1.970e+00)\n","[ 95.0%] train loss=1.203e-06 | eval RMSE=2.570e-03 (rel=1.304e-03, tstd=1.971e+00)\n","[100.0%] train loss=5.675e-07 | eval RMSE=2.116e-03 (rel=1.061e-03, tstd=1.994e+00)\n","Final: RMSE=7.456e-03 (rel=3.768e-03) | time=1335.9s\n","\n","================= exp_minus_lin — d=10 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=384, depth=4), params=1.19M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=5.804e-03 | eval RMSE=1.207e-01 (rel=3.383e-02, tstd=3.567e+00) *best\n","[ 10.0%] train loss=5.361e-05 | eval RMSE=2.990e-02 (rel=8.225e-03, tstd=3.635e+00) *best\n","[ 15.0%] train loss=4.028e-04 | eval RMSE=8.184e-02 (rel=2.144e-02, tstd=3.818e+00)\n","[ 20.0%] train loss=2.803e-04 | eval RMSE=6.994e-02 (rel=2.244e-02, tstd=3.117e+00)\n","[ 25.0%] train loss=4.682e-05 | eval RMSE=3.300e-02 (rel=8.980e-03, tstd=3.674e+00)\n","[ 30.0%] train loss=4.643e-05 | eval RMSE=3.824e-02 (rel=1.027e-02, tstd=3.724e+00)\n","[ 35.0%] train loss=5.675e-04 | eval RMSE=7.077e-02 (rel=1.647e-02, tstd=4.296e+00)\n","[ 40.0%] train loss=9.427e-06 | eval RMSE=2.062e-02 (rel=6.148e-03, tstd=3.354e+00) *best\n","[ 45.0%] train loss=3.229e-04 | eval RMSE=6.556e-02 (rel=1.859e-02, tstd=3.526e+00)\n","[ 50.0%] train loss=2.391e-04 | eval RMSE=3.107e-01 (rel=5.603e-02, tstd=5.547e+00)\n","[ 55.0%] train loss=1.893e-04 | eval RMSE=3.147e-02 (rel=9.654e-03, tstd=3.260e+00)\n","[ 60.0%] train loss=2.897e-04 | eval RMSE=1.157e-01 (rel=3.238e-02, tstd=3.574e+00)\n","[ 65.0%] train loss=1.297e-04 | eval RMSE=3.942e-02 (rel=1.100e-02, tstd=3.585e+00)\n","[ 70.0%] train loss=1.126e-04 | eval RMSE=3.680e-02 (rel=9.741e-03, tstd=3.777e+00)\n","[ 75.0%] train loss=3.973e-05 | eval RMSE=2.285e-02 (rel=6.836e-03, tstd=3.342e+00)\n","[ 80.0%] train loss=1.019e-03 | eval RMSE=1.694e-01 (rel=4.838e-02, tstd=3.501e+00)\n","[ 85.0%] train loss=1.685e-03 | eval RMSE=1.800e-01 (rel=5.011e-02, tstd=3.592e+00)\n","[ 90.0%] train loss=3.977e-04 | eval RMSE=7.946e-02 (rel=1.934e-02, tstd=4.109e+00)\n","[ 95.0%] train loss=4.559e-05 | eval RMSE=5.344e-02 (rel=1.282e-02, tstd=4.167e+00)\n","[100.0%] train loss=1.903e-04 | eval RMSE=5.785e-02 (rel=1.667e-02, tstd=3.471e+00)\n","Final: RMSE=1.128e-01 (rel=3.061e-02) | time=307.1s\n","\n","================= exp_minus_lin — d=20 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=512, depth=5), params=2.64M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=6.942e-04 | eval RMSE=1.689e-01 (rel=5.689e-02, tstd=2.968e+00) *best\n","[ 10.0%] train loss=6.301e-04 | eval RMSE=1.053e-01 (rel=2.959e-02, tstd=3.558e+00) *best\n","[ 15.0%] train loss=2.619e-05 | eval RMSE=2.576e-02 (rel=7.287e-03, tstd=3.535e+00) *best\n","[ 20.0%] train loss=4.441e-05 | eval RMSE=3.039e-02 (rel=8.853e-03, tstd=3.433e+00)\n","[ 25.0%] train loss=1.410e-04 | eval RMSE=5.839e-02 (rel=1.403e-02, tstd=4.163e+00)\n","[ 30.0%] train loss=1.584e-05 | eval RMSE=2.392e-02 (rel=6.569e-03, tstd=3.642e+00) *best\n","[ 35.0%] train loss=6.325e-06 | eval RMSE=1.768e-02 (rel=4.609e-03, tstd=3.836e+00) *best\n","[ 40.0%] train loss=2.468e-05 | eval RMSE=2.365e-02 (rel=6.062e-03, tstd=3.901e+00)\n","[ 45.0%] train loss=6.452e-05 | eval RMSE=5.881e-02 (rel=1.619e-02, tstd=3.632e+00)\n","[ 50.0%] train loss=1.394e-05 | eval RMSE=2.915e-02 (rel=7.907e-03, tstd=3.687e+00)\n","[ 55.0%] train loss=2.175e-04 | eval RMSE=1.131e-01 (rel=2.748e-02, tstd=4.115e+00)\n","[ 60.0%] train loss=5.496e-04 | eval RMSE=9.323e-02 (rel=2.427e-02, tstd=3.841e+00)\n","[ 65.0%] train loss=2.319e-05 | eval RMSE=2.227e-02 (rel=5.886e-03, tstd=3.783e+00)\n","[ 70.0%] train loss=1.378e-04 | eval RMSE=6.575e-02 (rel=1.889e-02, tstd=3.480e+00)\n","[ 75.0%] train loss=6.797e-05 | eval RMSE=4.761e-02 (rel=1.140e-02, tstd=4.178e+00)\n","[ 80.0%] train loss=2.165e-04 | eval RMSE=7.364e-02 (rel=2.358e-02, tstd=3.122e+00)\n","[ 85.0%] train loss=1.770e-03 | eval RMSE=1.711e-01 (rel=4.654e-02, tstd=3.677e+00)\n","[ 90.0%] train loss=2.600e-05 | eval RMSE=2.061e-02 (rel=5.529e-03, tstd=3.727e+00)\n","[ 95.0%] train loss=4.586e-05 | eval RMSE=3.079e-02 (rel=9.236e-03, tstd=3.333e+00)\n","[100.0%] train loss=5.935e-05 | eval RMSE=3.578e-02 (rel=9.973e-03, tstd=3.588e+00)\n","Final: RMSE=4.826e-02 (rel=1.131e-02) | time=529.5s\n","\n","================= exp_minus_lin — d=50 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=768, depth=6), params=7.11M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=4.654e-04 | eval RMSE=1.090e-01 (rel=2.558e-02, tstd=4.259e+00) *best\n","[ 10.0%] train loss=2.809e-05 | eval RMSE=5.185e-01 (rel=9.948e-02, tstd=5.213e+00)\n","[ 15.0%] train loss=2.219e-05 | eval RMSE=4.681e-02 (rel=1.328e-02, tstd=3.526e+00) *best\n","[ 20.0%] train loss=1.444e-04 | eval RMSE=7.246e-02 (rel=2.113e-02, tstd=3.429e+00)\n","[ 25.0%] train loss=2.970e-05 | eval RMSE=3.828e-02 (rel=1.258e-02, tstd=3.043e+00) *best\n","[ 30.0%] train loss=4.909e-05 | eval RMSE=4.958e-02 (rel=1.404e-02, tstd=3.530e+00)\n","[ 35.0%] train loss=5.542e-06 | eval RMSE=1.788e-02 (rel=5.287e-03, tstd=3.381e+00) *best\n","[ 40.0%] train loss=7.682e-05 | eval RMSE=1.244e-01 (rel=2.206e-02, tstd=5.640e+00)\n","[ 45.0%] train loss=2.095e-05 | eval RMSE=2.861e-02 (rel=7.395e-03, tstd=3.868e+00)\n","[ 50.0%] train loss=7.931e-05 | eval RMSE=3.508e-02 (rel=1.076e-02, tstd=3.260e+00)\n","[ 55.0%] train loss=3.811e-05 | eval RMSE=4.506e-02 (rel=1.036e-02, tstd=4.349e+00)\n","[ 60.0%] train loss=8.948e-06 | eval RMSE=1.634e-02 (rel=3.559e-03, tstd=4.593e+00) *best\n","[ 65.0%] train loss=2.399e-05 | eval RMSE=3.814e-02 (rel=9.558e-03, tstd=3.990e+00)\n","[ 70.0%] train loss=8.923e-06 | eval RMSE=1.626e-02 (rel=5.026e-03, tstd=3.235e+00) *best\n","[ 75.0%] train loss=3.801e-04 | eval RMSE=7.429e-02 (rel=2.280e-02, tstd=3.258e+00)\n","[ 80.0%] train loss=8.828e-05 | eval RMSE=1.454e-02 (rel=3.822e-03, tstd=3.805e+00) *best\n","[ 85.0%] train loss=2.231e-05 | eval RMSE=2.014e-02 (rel=5.028e-03, tstd=4.006e+00)\n","[ 90.0%] train loss=2.738e-04 | eval RMSE=8.889e-02 (rel=2.260e-02, tstd=3.934e+00)\n","[ 95.0%] train loss=4.301e-05 | eval RMSE=4.071e-02 (rel=9.797e-03, tstd=4.155e+00)\n","[100.0%] train loss=5.608e-05 | eval RMSE=4.629e-02 (rel=1.236e-02, tstd=3.746e+00)\n","Final: RMSE=3.373e-02 (rel=8.383e-03) | time=1332.7s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=66.92, ridge=0.01\n","[pre  20.0%] train MSE=3.117e+07 | eval RMSE=5.574e+03\n","[pre  40.0%] train MSE=2.091e+07 | eval RMSE=4.562e+03\n","[pre  60.0%] train MSE=1.616e+07 | eval RMSE=4.017e+03\n","[pre  80.0%] train MSE=1.446e+07 | eval RMSE=3.804e+03\n","[pre 100.0%] train MSE=1.418e+07 | eval RMSE=3.766e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=10 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=125.57 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=5.805e-04 | eval RMSE=2.451e-02 (rel=2.723e-02, tstd=9.001e-01) *best\n","[ 10.0%] train loss=6.855e-05 | eval RMSE=9.785e-03 (rel=1.078e-02, tstd=9.081e-01) *best\n","[ 15.0%] train loss=3.631e-05 | eval RMSE=8.055e-03 (rel=8.889e-03, tstd=9.061e-01) *best\n","[ 20.0%] train loss=5.817e-05 | eval RMSE=5.637e-03 (rel=6.157e-03, tstd=9.156e-01) *best\n","[ 25.0%] train loss=2.356e-05 | eval RMSE=5.947e-03 (rel=6.581e-03, tstd=9.036e-01)\n","[ 30.0%] train loss=1.047e-05 | eval RMSE=2.759e-03 (rel=3.023e-03, tstd=9.128e-01) *best\n","[ 35.0%] train loss=5.113e-06 | eval RMSE=5.385e-03 (rel=5.863e-03, tstd=9.185e-01)\n","[ 40.0%] train loss=1.254e-05 | eval RMSE=3.592e-03 (rel=3.949e-03, tstd=9.097e-01)\n","[ 45.0%] train loss=2.543e-05 | eval RMSE=2.387e-03 (rel=2.600e-03, tstd=9.181e-01) *best\n","[ 50.0%] train loss=1.660e-05 | eval RMSE=5.465e-03 (rel=5.969e-03, tstd=9.154e-01)\n","[ 55.0%] train loss=1.297e-06 | eval RMSE=3.943e-03 (rel=4.265e-03, tstd=9.245e-01)\n","[ 60.0%] train loss=6.767e-06 | eval RMSE=4.252e-03 (rel=4.627e-03, tstd=9.188e-01)\n","[ 65.0%] train loss=1.486e-05 | eval RMSE=2.338e-03 (rel=2.543e-03, tstd=9.195e-01) *best\n","[ 70.0%] train loss=1.092e-06 | eval RMSE=2.335e-03 (rel=2.556e-03, tstd=9.134e-01) *best\n","[ 75.0%] train loss=1.318e-05 | eval RMSE=2.112e-03 (rel=2.333e-03, tstd=9.053e-01) *best\n","[ 80.0%] train loss=6.820e-06 | eval RMSE=3.636e-03 (rel=3.979e-03, tstd=9.139e-01)\n","[ 85.0%] train loss=9.300e-07 | eval RMSE=1.577e-03 (rel=1.736e-03, tstd=9.083e-01) *best\n","[ 90.0%] train loss=6.967e-06 | eval RMSE=3.095e-03 (rel=3.373e-03, tstd=9.177e-01)\n","[ 95.0%] train loss=4.873e-06 | eval RMSE=2.226e-03 (rel=2.476e-03, tstd=8.990e-01)\n","[100.0%] train loss=1.518e-06 | eval RMSE=1.626e-03 (rel=1.813e-03, tstd=8.970e-01)\n","Final: RMSE=7.419e-03 (rel=8.114e-03) | time=358.7s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=72.53, ridge=0.01\n","[pre  20.0%] train MSE=2.769e+07 | eval RMSE=5.252e+03\n","[pre  40.0%] train MSE=1.822e+07 | eval RMSE=4.264e+03\n","[pre  60.0%] train MSE=1.410e+07 | eval RMSE=3.746e+03\n","[pre  80.0%] train MSE=1.260e+07 | eval RMSE=3.546e+03\n","[pre 100.0%] train MSE=1.231e+07 | eval RMSE=3.508e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=20 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=52.74 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=3.217e-04 | eval RMSE=1.212e-02 (rel=3.403e-02, tstd=3.561e-01) *best\n","[ 10.0%] train loss=4.679e-05 | eval RMSE=3.510e-03 (rel=9.881e-03, tstd=3.553e-01) *best\n","[ 15.0%] train loss=4.443e-06 | eval RMSE=9.364e-04 (rel=2.630e-03, tstd=3.561e-01) *best\n","[ 20.0%] train loss=2.794e-05 | eval RMSE=1.576e-03 (rel=4.396e-03, tstd=3.584e-01)\n","[ 25.0%] train loss=4.084e-05 | eval RMSE=3.938e-03 (rel=1.113e-02, tstd=3.539e-01)\n","[ 30.0%] train loss=6.799e-06 | eval RMSE=1.229e-03 (rel=3.411e-03, tstd=3.602e-01)\n","[ 35.0%] train loss=3.777e-06 | eval RMSE=7.715e-04 (rel=2.186e-03, tstd=3.529e-01) *best\n","[ 40.0%] train loss=6.278e-06 | eval RMSE=2.014e-03 (rel=5.633e-03, tstd=3.576e-01)\n","[ 45.0%] train loss=6.108e-06 | eval RMSE=1.840e-03 (rel=5.120e-03, tstd=3.594e-01)\n","[ 50.0%] train loss=4.531e-06 | eval RMSE=6.679e-04 (rel=1.857e-03, tstd=3.596e-01) *best\n","[ 55.0%] train loss=1.977e-06 | eval RMSE=1.329e-03 (rel=3.625e-03, tstd=3.667e-01)\n","[ 60.0%] train loss=7.189e-06 | eval RMSE=4.637e-04 (rel=1.310e-03, tstd=3.539e-01) *best\n","[ 65.0%] train loss=1.268e-06 | eval RMSE=8.063e-04 (rel=2.222e-03, tstd=3.629e-01)\n","[ 70.0%] train loss=1.941e-06 | eval RMSE=1.450e-03 (rel=4.040e-03, tstd=3.589e-01)\n","[ 75.0%] train loss=6.622e-06 | eval RMSE=6.624e-04 (rel=1.874e-03, tstd=3.535e-01)\n","[ 80.0%] train loss=2.973e-06 | eval RMSE=4.620e-04 (rel=1.299e-03, tstd=3.556e-01) *best\n","[ 85.0%] train loss=1.279e-06 | eval RMSE=9.089e-04 (rel=2.510e-03, tstd=3.621e-01)\n","[ 90.0%] train loss=9.022e-07 | eval RMSE=6.751e-04 (rel=1.908e-03, tstd=3.538e-01)\n","[ 95.0%] train loss=1.684e-06 | eval RMSE=7.204e-04 (rel=2.014e-03, tstd=3.577e-01)\n","[100.0%] train loss=6.036e-07 | eval RMSE=4.057e-04 (rel=1.134e-03, tstd=3.577e-01) *best\n","Final: RMSE=4.125e-04 (rel=1.158e-03) | time=607.5s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=101.00, ridge=0.01\n","[pre  20.0%] train MSE=2.417e+07 | eval RMSE=4.898e+03\n","[pre  40.0%] train MSE=1.580e+07 | eval RMSE=3.970e+03\n","[pre  60.0%] train MSE=1.228e+07 | eval RMSE=3.501e+03\n","[pre  80.0%] train MSE=1.102e+07 | eval RMSE=3.319e+03\n","[pre 100.0%] train MSE=1.078e+07 | eval RMSE=3.284e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=50 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=27.83 → scale=0.11); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=4.268e-05 | eval RMSE=6.225e-04 (rel=6.418e-03, tstd=9.699e-02) *best\n","[ 10.0%] train loss=7.176e-06 | eval RMSE=3.722e-04 (rel=3.822e-03, tstd=9.738e-02) *best\n","[ 15.0%] train loss=6.595e-04 | eval RMSE=3.853e-03 (rel=3.972e-02, tstd=9.701e-02)\n","[ 20.0%] train loss=1.196e-05 | eval RMSE=4.465e-04 (rel=4.665e-03, tstd=9.570e-02)\n","[ 25.0%] train loss=6.421e-06 | eval RMSE=3.044e-04 (rel=3.154e-03, tstd=9.651e-02) *best\n","[ 30.0%] train loss=4.902e-06 | eval RMSE=2.949e-04 (rel=3.036e-03, tstd=9.715e-02) *best\n","[ 35.0%] train loss=4.443e-06 | eval RMSE=3.200e-04 (rel=3.349e-03, tstd=9.553e-02)\n","[ 40.0%] train loss=5.356e-06 | eval RMSE=4.007e-04 (rel=4.184e-03, tstd=9.579e-02)\n","[ 45.0%] train loss=4.703e-06 | eval RMSE=2.875e-04 (rel=3.043e-03, tstd=9.450e-02) *best\n","[ 50.0%] train loss=4.846e-06 | eval RMSE=3.059e-04 (rel=3.198e-03, tstd=9.565e-02)\n","[ 55.0%] train loss=4.085e-06 | eval RMSE=2.753e-04 (rel=2.840e-03, tstd=9.695e-02) *best\n","[ 60.0%] train loss=3.872e-06 | eval RMSE=2.735e-04 (rel=2.832e-03, tstd=9.657e-02) *best\n","[ 65.0%] train loss=4.059e-06 | eval RMSE=2.865e-04 (rel=2.960e-03, tstd=9.679e-02)\n","[ 70.0%] train loss=3.874e-06 | eval RMSE=2.652e-04 (rel=2.766e-03, tstd=9.586e-02) *best\n","[ 75.0%] train loss=3.572e-06 | eval RMSE=2.750e-04 (rel=2.850e-03, tstd=9.650e-02)\n","[ 80.0%] train loss=3.745e-06 | eval RMSE=2.628e-04 (rel=2.735e-03, tstd=9.608e-02) *best\n","[ 85.0%] train loss=3.440e-06 | eval RMSE=2.612e-04 (rel=2.738e-03, tstd=9.539e-02) *best\n","[ 90.0%] train loss=3.363e-06 | eval RMSE=2.566e-04 (rel=2.643e-03, tstd=9.708e-02) *best\n","[ 95.0%] train loss=3.615e-06 | eval RMSE=2.601e-04 (rel=2.747e-03, tstd=9.468e-02)\n","[100.0%] train loss=3.371e-06 | eval RMSE=2.545e-04 (rel=2.639e-03, tstd=9.647e-02) *best\n","Final: RMSE=2.594e-04 (rel=2.692e-03) | time=1360.7s\n","\n","================= softplus_pairs — d=10 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=2.381e-04 | eval RMSE=1.913e-02 (rel=1.942e-02, tstd=9.851e-01) *best\n","[ 10.0%] train loss=2.214e-04 | eval RMSE=2.099e-02 (rel=2.164e-02, tstd=9.699e-01)\n","[ 15.0%] train loss=3.854e-04 | eval RMSE=2.540e-02 (rel=2.632e-02, tstd=9.652e-01)\n","[ 20.0%] train loss=2.282e-05 | eval RMSE=5.921e-03 (rel=6.083e-03, tstd=9.734e-01) *best\n","[ 25.0%] train loss=3.304e-05 | eval RMSE=7.779e-03 (rel=7.970e-03, tstd=9.760e-01)\n","[ 30.0%] train loss=4.379e-05 | eval RMSE=1.321e-02 (rel=1.345e-02, tstd=9.824e-01)\n","[ 35.0%] train loss=1.230e-05 | eval RMSE=8.539e-03 (rel=8.869e-03, tstd=9.628e-01)\n","[ 40.0%] train loss=1.545e-05 | eval RMSE=5.512e-03 (rel=5.666e-03, tstd=9.728e-01) *best\n","[ 45.0%] train loss=7.923e-06 | eval RMSE=5.568e-03 (rel=5.857e-03, tstd=9.507e-01)\n","[ 50.0%] train loss=1.959e-05 | eval RMSE=3.596e-03 (rel=3.723e-03, tstd=9.659e-01) *best\n","[ 55.0%] train loss=2.174e-06 | eval RMSE=4.282e-03 (rel=4.390e-03, tstd=9.756e-01)\n","[ 60.0%] train loss=1.175e-05 | eval RMSE=3.099e-03 (rel=3.201e-03, tstd=9.680e-01) *best\n","[ 65.0%] train loss=2.353e-06 | eval RMSE=2.437e-03 (rel=2.519e-03, tstd=9.674e-01) *best\n","[ 70.0%] train loss=3.429e-06 | eval RMSE=1.381e-03 (rel=1.427e-03, tstd=9.676e-01) *best\n","[ 75.0%] train loss=3.155e-06 | eval RMSE=2.280e-03 (rel=2.341e-03, tstd=9.742e-01)\n","[ 80.0%] train loss=1.169e-06 | eval RMSE=1.099e-03 (rel=1.125e-03, tstd=9.773e-01) *best\n","[ 85.0%] train loss=9.217e-07 | eval RMSE=1.052e-03 (rel=1.082e-03, tstd=9.726e-01) *best\n","[ 90.0%] train loss=1.320e-06 | eval RMSE=1.064e-03 (rel=1.077e-03, tstd=9.873e-01)\n","[ 95.0%] train loss=4.480e-07 | eval RMSE=7.791e-04 (rel=8.087e-04, tstd=9.634e-01) *best\n","[100.0%] train loss=7.016e-06 | eval RMSE=3.570e-03 (rel=3.730e-03, tstd=9.571e-01)\n","Final: RMSE=6.931e-03 (rel=7.124e-03) | time=493.5s\n","\n","================= softplus_pairs — d=20 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=1.479e-03 | eval RMSE=1.280e-01 (rel=4.516e-02, tstd=2.833e+00) *best\n","[ 10.0%] train loss=3.080e-05 | eval RMSE=2.185e-02 (rel=7.667e-03, tstd=2.850e+00) *best\n","[ 15.0%] train loss=2.682e-05 | eval RMSE=2.144e-02 (rel=7.550e-03, tstd=2.840e+00) *best\n","[ 20.0%] train loss=2.598e-05 | eval RMSE=2.503e-02 (rel=8.862e-03, tstd=2.824e+00)\n","[ 25.0%] train loss=6.232e-05 | eval RMSE=3.179e-02 (rel=1.135e-02, tstd=2.802e+00)\n","[ 30.0%] train loss=1.929e-05 | eval RMSE=1.933e-02 (rel=6.792e-03, tstd=2.845e+00) *best\n","[ 35.0%] train loss=2.247e-05 | eval RMSE=1.739e-02 (rel=6.101e-03, tstd=2.850e+00) *best\n","[ 40.0%] train loss=5.986e-06 | eval RMSE=1.214e-02 (rel=4.303e-03, tstd=2.822e+00) *best\n","[ 45.0%] train loss=1.444e-05 | eval RMSE=1.074e-02 (rel=3.800e-03, tstd=2.827e+00) *best\n","[ 50.0%] train loss=3.124e-06 | eval RMSE=6.107e-03 (rel=2.163e-03, tstd=2.824e+00) *best\n","[ 55.0%] train loss=1.243e-05 | eval RMSE=1.017e-02 (rel=3.581e-03, tstd=2.840e+00)\n","[ 60.0%] train loss=8.363e-06 | eval RMSE=6.238e-03 (rel=2.206e-03, tstd=2.827e+00)\n","[ 65.0%] train loss=9.790e-06 | eval RMSE=6.937e-03 (rel=2.461e-03, tstd=2.819e+00)\n","[ 70.0%] train loss=2.266e-06 | eval RMSE=4.014e-03 (rel=1.427e-03, tstd=2.812e+00) *best\n","[ 75.0%] train loss=2.266e-06 | eval RMSE=4.455e-03 (rel=1.570e-03, tstd=2.838e+00)\n","[ 80.0%] train loss=8.513e-07 | eval RMSE=5.022e-03 (rel=1.754e-03, tstd=2.864e+00)\n","[ 85.0%] train loss=5.649e-06 | eval RMSE=7.375e-03 (rel=2.623e-03, tstd=2.812e+00)\n","[ 90.0%] train loss=9.244e-07 | eval RMSE=5.535e-03 (rel=1.934e-03, tstd=2.862e+00)\n","[ 95.0%] train loss=8.535e-07 | eval RMSE=2.966e-03 (rel=1.052e-03, tstd=2.819e+00) *best\n","[100.0%] train loss=2.514e-07 | eval RMSE=2.053e-03 (rel=7.340e-04, tstd=2.797e+00) *best\n","Final: RMSE=2.019e-03 (rel=7.078e-04) | time=1122.5s\n","\n","================= softplus_pairs — d=50 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=1.741e-03 | eval RMSE=7.564e-01 (rel=6.607e-02, tstd=1.145e+01) *best\n","[ 10.0%] train loss=3.253e-04 | eval RMSE=3.115e-01 (rel=2.717e-02, tstd=1.147e+01) *best\n","[ 15.0%] train loss=3.640e-04 | eval RMSE=2.983e-01 (rel=2.648e-02, tstd=1.126e+01) *best\n","[ 20.0%] train loss=3.380e-04 | eval RMSE=2.773e-01 (rel=2.455e-02, tstd=1.129e+01) *best\n","[ 25.0%] train loss=2.859e-04 | eval RMSE=2.581e-01 (rel=2.283e-02, tstd=1.131e+01) *best\n","[ 30.0%] train loss=2.832e-04 | eval RMSE=2.470e-01 (rel=2.186e-02, tstd=1.130e+01) *best\n","[ 35.0%] train loss=2.078e-04 | eval RMSE=2.329e-01 (rel=2.042e-02, tstd=1.140e+01) *best\n","[ 40.0%] train loss=2.370e-04 | eval RMSE=2.428e-01 (rel=2.130e-02, tstd=1.140e+01)\n","[ 45.0%] train loss=2.214e-04 | eval RMSE=2.200e-01 (rel=1.940e-02, tstd=1.134e+01) *best\n","[ 50.0%] train loss=1.792e-04 | eval RMSE=2.224e-01 (rel=1.967e-02, tstd=1.130e+01)\n","[ 55.0%] train loss=1.763e-04 | eval RMSE=2.157e-01 (rel=1.890e-02, tstd=1.141e+01) *best\n","[ 60.0%] train loss=1.798e-04 | eval RMSE=2.215e-01 (rel=1.956e-02, tstd=1.133e+01)\n","[ 65.0%] train loss=1.702e-04 | eval RMSE=2.133e-01 (rel=1.895e-02, tstd=1.126e+01) *best\n","[ 70.0%] train loss=1.614e-04 | eval RMSE=1.999e-01 (rel=1.771e-02, tstd=1.129e+01) *best\n","[ 75.0%] train loss=1.756e-04 | eval RMSE=2.099e-01 (rel=1.857e-02, tstd=1.131e+01)\n","[ 80.0%] train loss=1.672e-04 | eval RMSE=2.041e-01 (rel=1.796e-02, tstd=1.136e+01)\n","[ 85.0%] train loss=1.606e-04 | eval RMSE=2.073e-01 (rel=1.809e-02, tstd=1.146e+01)\n","[ 90.0%] train loss=1.658e-04 | eval RMSE=2.029e-01 (rel=1.777e-02, tstd=1.142e+01)\n","[ 95.0%] train loss=1.598e-04 | eval RMSE=2.027e-01 (rel=1.759e-02, tstd=1.153e+01)\n","[100.0%] train loss=1.537e-04 | eval RMSE=1.998e-01 (rel=1.757e-02, tstd=1.137e+01) *best\n","Final: RMSE=2.048e-01 (rel=1.811e-02) | time=2858.6s\n","\n","============================ TRIAL 5/5 (seed=4000054) ============================\n","\n","================= quad_spd — d=10 =================\n","Q stats: ||Q||₂≈1.010, κ=100.72, trace=2.8 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=1.011e-04 | eval RMSE=9.293e-03 (rel=1.023e-02, tstd=9.080e-01) *best\n","[ 10.0%] train loss=1.638e-05 | eval RMSE=6.578e-03 (rel=7.446e-03, tstd=8.834e-01) *best\n","[ 15.0%] train loss=2.800e-05 | eval RMSE=5.927e-03 (rel=6.894e-03, tstd=8.598e-01) *best\n","[ 20.0%] train loss=2.143e-04 | eval RMSE=1.585e-02 (rel=1.780e-02, tstd=8.905e-01)\n","[ 25.0%] train loss=1.564e-04 | eval RMSE=8.218e-03 (rel=9.002e-03, tstd=9.129e-01)\n","[ 30.0%] train loss=7.058e-05 | eval RMSE=9.381e-03 (rel=1.067e-02, tstd=8.794e-01)\n","[ 35.0%] train loss=4.297e-05 | eval RMSE=7.780e-03 (rel=8.678e-03, tstd=8.965e-01)\n","[ 40.0%] train loss=1.724e-05 | eval RMSE=3.710e-03 (rel=4.067e-03, tstd=9.123e-01) *best\n","[ 45.0%] train loss=3.726e-06 | eval RMSE=1.564e-03 (rel=1.749e-03, tstd=8.943e-01) *best\n","[ 50.0%] train loss=3.792e-05 | eval RMSE=2.625e-03 (rel=2.903e-03, tstd=9.044e-01)\n","[ 55.0%] train loss=8.715e-06 | eval RMSE=4.771e-03 (rel=5.368e-03, tstd=8.887e-01)\n","[ 60.0%] train loss=5.123e-06 | eval RMSE=2.756e-03 (rel=3.077e-03, tstd=8.955e-01)\n","[ 65.0%] train loss=1.117e-05 | eval RMSE=4.746e-03 (rel=5.281e-03, tstd=8.987e-01)\n","[ 70.0%] train loss=6.144e-06 | eval RMSE=4.032e-03 (rel=4.434e-03, tstd=9.092e-01)\n","[ 75.0%] train loss=6.521e-07 | eval RMSE=2.187e-03 (rel=2.418e-03, tstd=9.045e-01)\n","[ 80.0%] train loss=4.535e-07 | eval RMSE=1.512e-03 (rel=1.660e-03, tstd=9.111e-01) *best\n","[ 85.0%] train loss=1.110e-06 | eval RMSE=1.004e-03 (rel=1.140e-03, tstd=8.805e-01) *best\n","[ 90.0%] train loss=2.644e-06 | eval RMSE=3.277e-03 (rel=3.663e-03, tstd=8.947e-01)\n","[ 95.0%] train loss=1.648e-06 | eval RMSE=1.313e-03 (rel=1.457e-03, tstd=9.009e-01)\n","[100.0%] train loss=6.835e-06 | eval RMSE=3.236e-03 (rel=3.640e-03, tstd=8.889e-01)\n","Final: RMSE=1.417e-02 (rel=1.574e-02) | time=299.2s\n","\n","================= quad_spd — d=20 =================\n","Q stats: ||Q||₂≈1.010, κ=100.86, trace=5.6 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=2.628e-04 | eval RMSE=2.780e-02 (rel=2.261e-02, tstd=1.229e+00) *best\n","[ 10.0%] train loss=3.918e-04 | eval RMSE=3.587e-02 (rel=2.845e-02, tstd=1.261e+00)\n","[ 15.0%] train loss=2.440e-05 | eval RMSE=7.427e-03 (rel=5.920e-03, tstd=1.255e+00) *best\n","[ 20.0%] train loss=1.108e-04 | eval RMSE=1.969e-02 (rel=1.628e-02, tstd=1.209e+00)\n","[ 25.0%] train loss=3.883e-05 | eval RMSE=1.788e-02 (rel=1.444e-02, tstd=1.238e+00)\n","[ 30.0%] train loss=1.471e-05 | eval RMSE=7.367e-03 (rel=5.995e-03, tstd=1.229e+00) *best\n","[ 35.0%] train loss=1.554e-05 | eval RMSE=3.771e-03 (rel=3.089e-03, tstd=1.221e+00) *best\n","[ 40.0%] train loss=4.248e-06 | eval RMSE=3.086e-03 (rel=2.453e-03, tstd=1.258e+00) *best\n","[ 45.0%] train loss=8.303e-06 | eval RMSE=5.944e-03 (rel=4.674e-03, tstd=1.272e+00)\n","[ 50.0%] train loss=2.250e-06 | eval RMSE=6.329e-03 (rel=5.063e-03, tstd=1.250e+00)\n","[ 55.0%] train loss=1.086e-05 | eval RMSE=5.584e-03 (rel=4.411e-03, tstd=1.266e+00)\n","[ 60.0%] train loss=8.244e-06 | eval RMSE=5.342e-03 (rel=4.327e-03, tstd=1.235e+00)\n","[ 65.0%] train loss=8.452e-07 | eval RMSE=3.230e-03 (rel=2.631e-03, tstd=1.228e+00)\n","[ 70.0%] train loss=2.106e-06 | eval RMSE=1.814e-03 (rel=1.454e-03, tstd=1.248e+00) *best\n","[ 75.0%] train loss=8.450e-07 | eval RMSE=2.317e-03 (rel=1.859e-03, tstd=1.247e+00)\n","[ 80.0%] train loss=9.286e-07 | eval RMSE=2.592e-03 (rel=2.118e-03, tstd=1.224e+00)\n","[ 85.0%] train loss=4.394e-06 | eval RMSE=2.038e-03 (rel=1.624e-03, tstd=1.254e+00)\n","[ 90.0%] train loss=1.875e-06 | eval RMSE=3.763e-03 (rel=2.998e-03, tstd=1.255e+00)\n","[ 95.0%] train loss=8.581e-07 | eval RMSE=1.653e-03 (rel=1.332e-03, tstd=1.241e+00) *best\n","[100.0%] train loss=3.396e-06 | eval RMSE=3.147e-03 (rel=2.518e-03, tstd=1.250e+00)\n","Final: RMSE=1.768e-02 (rel=1.457e-02) | time=518.0s\n","\n","================= quad_spd — d=50 =================\n","Q stats: ||Q||₂≈1.010, κ=100.98, trace=13.0 | features: z=Q^(-1/2) y (+ z^2 in model)\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=9.020e-05 | eval RMSE=2.409e-02 (rel=1.313e-02, tstd=1.835e+00) *best\n","[ 10.0%] train loss=5.041e-05 | eval RMSE=1.750e-02 (rel=9.522e-03, tstd=1.838e+00) *best\n","[ 15.0%] train loss=4.786e-05 | eval RMSE=1.313e-02 (rel=7.133e-03, tstd=1.841e+00) *best\n","[ 20.0%] train loss=3.776e-05 | eval RMSE=1.922e-02 (rel=1.060e-02, tstd=1.814e+00)\n","[ 25.0%] train loss=1.513e-05 | eval RMSE=1.265e-02 (rel=6.910e-03, tstd=1.830e+00) *best\n","[ 30.0%] train loss=3.546e-05 | eval RMSE=1.535e-02 (rel=8.546e-03, tstd=1.797e+00)\n","[ 35.0%] train loss=1.823e-06 | eval RMSE=3.204e-03 (rel=1.761e-03, tstd=1.820e+00) *best\n","[ 40.0%] train loss=5.607e-06 | eval RMSE=7.410e-03 (rel=3.994e-03, tstd=1.855e+00)\n","[ 45.0%] train loss=1.602e-06 | eval RMSE=4.107e-03 (rel=2.192e-03, tstd=1.874e+00)\n","[ 50.0%] train loss=5.858e-06 | eval RMSE=9.845e-03 (rel=5.367e-03, tstd=1.834e+00)\n","[ 55.0%] train loss=1.590e-06 | eval RMSE=4.037e-03 (rel=2.173e-03, tstd=1.858e+00)\n","[ 60.0%] train loss=4.904e-06 | eval RMSE=4.905e-03 (rel=2.656e-03, tstd=1.846e+00)\n","[ 65.0%] train loss=1.906e-06 | eval RMSE=2.023e-03 (rel=1.084e-03, tstd=1.867e+00) *best\n","[ 70.0%] train loss=4.984e-06 | eval RMSE=2.397e-03 (rel=1.306e-03, tstd=1.835e+00)\n","[ 75.0%] train loss=1.046e-06 | eval RMSE=1.500e-03 (rel=8.256e-04, tstd=1.817e+00) *best\n","[ 80.0%] train loss=2.575e-07 | eval RMSE=2.022e-03 (rel=1.095e-03, tstd=1.847e+00)\n","[ 85.0%] train loss=7.623e-07 | eval RMSE=2.526e-03 (rel=1.374e-03, tstd=1.839e+00)\n","[ 90.0%] train loss=5.561e-07 | eval RMSE=2.087e-03 (rel=1.137e-03, tstd=1.835e+00)\n","[ 95.0%] train loss=1.417e-06 | eval RMSE=1.709e-03 (rel=9.236e-04, tstd=1.851e+00)\n","[100.0%] train loss=2.138e-07 | eval RMSE=1.165e-03 (rel=6.403e-04, tstd=1.820e+00) *best\n","Final: RMSE=1.161e-03 (rel=6.265e-04) | time=1336.1s\n","\n","================= exp_minus_lin — d=10 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=384, depth=4), params=1.19M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=5.190e-04 | eval RMSE=7.092e-02 (rel=2.248e-02, tstd=3.156e+00) *best\n","[ 10.0%] train loss=4.734e-05 | eval RMSE=3.189e-02 (rel=1.051e-02, tstd=3.033e+00) *best\n","[ 15.0%] train loss=1.116e-04 | eval RMSE=2.350e-02 (rel=8.368e-03, tstd=2.808e+00) *best\n","[ 20.0%] train loss=2.814e-04 | eval RMSE=8.060e-02 (rel=2.292e-02, tstd=3.517e+00)\n","[ 25.0%] train loss=1.002e-03 | eval RMSE=1.249e-01 (rel=3.614e-02, tstd=3.456e+00)\n","[ 30.0%] train loss=1.063e-03 | eval RMSE=1.111e-01 (rel=2.911e-02, tstd=3.818e+00)\n","[ 35.0%] train loss=6.470e-06 | eval RMSE=1.893e-02 (rel=5.549e-03, tstd=3.411e+00) *best\n","[ 40.0%] train loss=1.202e-03 | eval RMSE=9.458e-02 (rel=2.843e-02, tstd=3.327e+00)\n","[ 45.0%] train loss=1.904e-04 | eval RMSE=1.398e-01 (rel=3.401e-02, tstd=4.110e+00)\n","[ 50.0%] train loss=2.399e-05 | eval RMSE=5.352e-02 (rel=1.489e-02, tstd=3.596e+00)\n","[ 55.0%] train loss=1.910e-05 | eval RMSE=3.560e-02 (rel=8.502e-03, tstd=4.187e+00)\n","[ 60.0%] train loss=1.478e-04 | eval RMSE=6.410e-02 (rel=1.670e-02, tstd=3.839e+00)\n","[ 65.0%] train loss=2.046e-05 | eval RMSE=3.765e-02 (rel=9.677e-03, tstd=3.891e+00)\n","[ 70.0%] train loss=2.094e-03 | eval RMSE=1.854e-01 (rel=4.592e-02, tstd=4.039e+00)\n","[ 75.0%] train loss=1.297e-03 | eval RMSE=2.494e-01 (rel=6.359e-02, tstd=3.923e+00)\n","[ 80.0%] train loss=5.468e-06 | eval RMSE=4.722e-02 (rel=1.306e-02, tstd=3.616e+00)\n","[ 85.0%] train loss=2.018e-04 | eval RMSE=9.137e-02 (rel=2.599e-02, tstd=3.516e+00)\n","[ 90.0%] train loss=2.789e-03 | eval RMSE=2.445e-01 (rel=5.126e-02, tstd=4.770e+00)\n","[ 95.0%] train loss=1.304e-05 | eval RMSE=2.519e-02 (rel=7.283e-03, tstd=3.459e+00)\n","[100.0%] train loss=2.648e-04 | eval RMSE=2.321e-01 (rel=4.813e-02, tstd=4.823e+00)\n","Final: RMSE=1.712e-01 (rel=4.773e-02) | time=306.7s\n","\n","================= exp_minus_lin — d=20 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=512, depth=5), params=2.64M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=1.529e-04 | eval RMSE=6.370e-02 (rel=1.610e-02, tstd=3.956e+00) *best\n","[ 10.0%] train loss=8.473e-06 | eval RMSE=1.353e-01 (rel=3.474e-02, tstd=3.894e+00)\n","[ 15.0%] train loss=9.946e-04 | eval RMSE=1.765e-01 (rel=4.682e-02, tstd=3.770e+00)\n","[ 20.0%] train loss=1.668e-05 | eval RMSE=4.693e-02 (rel=1.216e-02, tstd=3.860e+00) *best\n","[ 25.0%] train loss=1.466e-05 | eval RMSE=2.727e-02 (rel=6.951e-03, tstd=3.922e+00) *best\n","[ 30.0%] train loss=4.060e-05 | eval RMSE=2.964e-02 (rel=1.009e-02, tstd=2.938e+00)\n","[ 35.0%] train loss=4.342e-05 | eval RMSE=3.191e-02 (rel=8.293e-03, tstd=3.848e+00)\n","[ 40.0%] train loss=4.394e-05 | eval RMSE=4.077e-02 (rel=1.162e-02, tstd=3.510e+00)\n","[ 45.0%] train loss=6.618e-05 | eval RMSE=7.091e-02 (rel=1.420e-02, tstd=4.995e+00)\n","[ 50.0%] train loss=2.908e-05 | eval RMSE=2.640e-02 (rel=8.387e-03, tstd=3.147e+00) *best\n","[ 55.0%] train loss=1.362e-05 | eval RMSE=4.458e-02 (rel=1.050e-02, tstd=4.247e+00)\n","[ 60.0%] train loss=4.786e-05 | eval RMSE=4.382e-02 (rel=1.242e-02, tstd=3.529e+00)\n","[ 65.0%] train loss=2.057e-04 | eval RMSE=6.897e-02 (rel=1.857e-02, tstd=3.713e+00)\n","[ 70.0%] train loss=8.854e-06 | eval RMSE=9.305e-02 (rel=2.088e-02, tstd=4.455e+00)\n","[ 75.0%] train loss=1.434e-05 | eval RMSE=2.278e-02 (rel=5.998e-03, tstd=3.798e+00) *best\n","[ 80.0%] train loss=4.692e-04 | eval RMSE=1.121e-01 (rel=3.329e-02, tstd=3.368e+00)\n","[ 85.0%] train loss=7.081e-04 | eval RMSE=9.914e-02 (rel=2.869e-02, tstd=3.455e+00)\n","[ 90.0%] train loss=1.895e-04 | eval RMSE=7.994e-02 (rel=2.219e-02, tstd=3.602e+00)\n","[ 95.0%] train loss=9.709e-04 | eval RMSE=2.219e-01 (rel=5.955e-02, tstd=3.726e+00)\n","[100.0%] train loss=9.744e-05 | eval RMSE=5.272e-02 (rel=1.348e-02, tstd=3.910e+00)\n","Final: RMSE=1.475e-01 (rel=3.580e-02) | time=529.7s\n","\n","================= exp_minus_lin — d=50 =================\n","Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\n","Model:    ResNet(width=768, depth=6), params=7.11M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=6.229e-05 | eval RMSE=5.945e-02 (rel=1.616e-02, tstd=3.680e+00) *best\n","[ 10.0%] train loss=4.030e-04 | eval RMSE=1.123e-01 (rel=3.005e-02, tstd=3.737e+00)\n","[ 15.0%] train loss=6.226e-05 | eval RMSE=2.248e-02 (rel=6.202e-03, tstd=3.625e+00) *best\n","[ 20.0%] train loss=2.356e-05 | eval RMSE=2.899e-02 (rel=7.373e-03, tstd=3.932e+00)\n","[ 25.0%] train loss=2.127e-05 | eval RMSE=5.923e-02 (rel=1.340e-02, tstd=4.421e+00)\n","[ 30.0%] train loss=1.202e-05 | eval RMSE=3.341e-02 (rel=8.313e-03, tstd=4.019e+00)\n","[ 35.0%] train loss=3.695e-05 | eval RMSE=2.822e-02 (rel=7.267e-03, tstd=3.884e+00)\n","[ 40.0%] train loss=3.517e-05 | eval RMSE=2.535e-02 (rel=7.155e-03, tstd=3.543e+00)\n","[ 45.0%] train loss=4.502e-05 | eval RMSE=2.422e-02 (rel=6.551e-03, tstd=3.697e+00)\n","[ 50.0%] train loss=1.307e-05 | eval RMSE=2.476e-02 (rel=8.111e-03, tstd=3.052e+00)\n","[ 55.0%] train loss=2.315e-05 | eval RMSE=3.011e-02 (rel=9.350e-03, tstd=3.220e+00)\n","[ 60.0%] train loss=1.640e-05 | eval RMSE=1.966e-02 (rel=5.848e-03, tstd=3.361e+00) *best\n","[ 65.0%] train loss=1.418e-05 | eval RMSE=1.814e-02 (rel=4.293e-03, tstd=4.225e+00) *best\n","[ 70.0%] train loss=1.194e-04 | eval RMSE=8.260e-02 (rel=1.402e-02, tstd=5.892e+00)\n","[ 75.0%] train loss=7.290e-05 | eval RMSE=3.653e-02 (rel=9.564e-03, tstd=3.820e+00)\n","[ 80.0%] train loss=3.468e-05 | eval RMSE=2.598e-02 (rel=8.607e-03, tstd=3.018e+00)\n","[ 85.0%] train loss=3.287e-05 | eval RMSE=6.269e-02 (rel=1.565e-02, tstd=4.006e+00)\n","[ 90.0%] train loss=3.117e-04 | eval RMSE=7.913e-02 (rel=2.623e-02, tstd=3.016e+00)\n","[ 95.0%] train loss=4.230e-05 | eval RMSE=3.772e-02 (rel=8.781e-03, tstd=4.296e+00)\n","[100.0%] train loss=2.212e-06 | eval RMSE=8.603e-03 (rel=2.294e-03, tstd=3.750e+00) *best\n","Final: RMSE=8.218e-03 (rel=2.420e-03) | time=1333.8s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=97.84, ridge=0.01\n","[pre  20.0%] train MSE=2.738e+07 | eval RMSE=5.211e+03\n","[pre  40.0%] train MSE=1.821e+07 | eval RMSE=4.257e+03\n","[pre  60.0%] train MSE=1.403e+07 | eval RMSE=3.739e+03\n","[pre  80.0%] train MSE=1.251e+07 | eval RMSE=3.540e+03\n","[pre 100.0%] train MSE=1.228e+07 | eval RMSE=3.499e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=10 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=121.05 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=2.622e-04 | eval RMSE=1.291e-02 (rel=1.481e-02, tstd=8.714e-01) *best\n","[ 10.0%] train loss=3.304e-04 | eval RMSE=2.008e-02 (rel=2.297e-02, tstd=8.742e-01)\n","[ 15.0%] train loss=6.855e-05 | eval RMSE=5.254e-03 (rel=6.149e-03, tstd=8.545e-01) *best\n","[ 20.0%] train loss=5.479e-05 | eval RMSE=1.385e-02 (rel=1.605e-02, tstd=8.630e-01)\n","[ 25.0%] train loss=9.033e-06 | eval RMSE=3.391e-03 (rel=3.892e-03, tstd=8.711e-01) *best\n","[ 30.0%] train loss=4.885e-06 | eval RMSE=3.172e-03 (rel=3.715e-03, tstd=8.537e-01) *best\n","[ 35.0%] train loss=7.811e-06 | eval RMSE=3.509e-03 (rel=4.102e-03, tstd=8.555e-01)\n","[ 40.0%] train loss=6.396e-05 | eval RMSE=5.699e-03 (rel=6.671e-03, tstd=8.543e-01)\n","[ 45.0%] train loss=5.292e-05 | eval RMSE=3.455e-03 (rel=4.050e-03, tstd=8.530e-01)\n","[ 50.0%] train loss=6.287e-06 | eval RMSE=4.902e-03 (rel=5.676e-03, tstd=8.636e-01)\n","[ 55.0%] train loss=2.658e-06 | eval RMSE=3.631e-03 (rel=4.198e-03, tstd=8.649e-01)\n","[ 60.0%] train loss=7.341e-06 | eval RMSE=3.504e-03 (rel=4.122e-03, tstd=8.501e-01)\n","[ 65.0%] train loss=4.271e-06 | eval RMSE=2.316e-03 (rel=2.721e-03, tstd=8.510e-01) *best\n","[ 70.0%] train loss=1.288e-06 | eval RMSE=2.253e-03 (rel=2.637e-03, tstd=8.544e-01) *best\n","[ 75.0%] train loss=4.657e-06 | eval RMSE=2.076e-03 (rel=2.415e-03, tstd=8.596e-01) *best\n","[ 80.0%] train loss=7.525e-06 | eval RMSE=2.275e-03 (rel=2.655e-03, tstd=8.567e-01)\n","[ 85.0%] train loss=3.195e-06 | eval RMSE=8.370e-04 (rel=9.584e-04, tstd=8.733e-01) *best\n","[ 90.0%] train loss=2.587e-06 | eval RMSE=1.931e-03 (rel=2.223e-03, tstd=8.687e-01)\n","[ 95.0%] train loss=1.230e-05 | eval RMSE=3.697e-03 (rel=4.334e-03, tstd=8.530e-01)\n","[100.0%] train loss=2.162e-06 | eval RMSE=1.719e-03 (rel=2.007e-03, tstd=8.563e-01)\n","Final: RMSE=4.851e-03 (rel=5.623e-03) | time=358.8s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=97.82, ridge=0.01\n","[pre  20.0%] train MSE=2.620e+07 | eval RMSE=5.107e+03\n","[pre  40.0%] train MSE=1.720e+07 | eval RMSE=4.140e+03\n","[pre  60.0%] train MSE=1.327e+07 | eval RMSE=3.638e+03\n","[pre  80.0%] train MSE=1.185e+07 | eval RMSE=3.441e+03\n","[pre 100.0%] train MSE=1.159e+07 | eval RMSE=3.404e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=20 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=61.82 → scale=0.10); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=4.623e-05 | eval RMSE=3.644e-03 (rel=9.994e-03, tstd=3.646e-01) *best\n","[ 10.0%] train loss=1.068e-02 | eval RMSE=7.225e-02 (rel=1.966e-01, tstd=3.674e-01)\n","[ 15.0%] train loss=6.663e-06 | eval RMSE=1.672e-03 (rel=4.569e-03, tstd=3.660e-01) *best\n","[ 20.0%] train loss=7.957e-05 | eval RMSE=4.437e-03 (rel=1.222e-02, tstd=3.632e-01)\n","[ 25.0%] train loss=4.696e-06 | eval RMSE=1.700e-03 (rel=4.564e-03, tstd=3.725e-01)\n","[ 30.0%] train loss=5.213e-06 | eval RMSE=2.696e-03 (rel=7.338e-03, tstd=3.674e-01)\n","[ 35.0%] train loss=1.653e-05 | eval RMSE=2.249e-03 (rel=6.134e-03, tstd=3.666e-01)\n","[ 40.0%] train loss=2.247e-05 | eval RMSE=1.056e-03 (rel=2.875e-03, tstd=3.675e-01) *best\n","[ 45.0%] train loss=6.727e-06 | eval RMSE=7.329e-04 (rel=2.000e-03, tstd=3.664e-01) *best\n","[ 50.0%] train loss=5.186e-06 | eval RMSE=1.116e-03 (rel=3.060e-03, tstd=3.648e-01)\n","[ 55.0%] train loss=2.535e-05 | eval RMSE=1.588e-03 (rel=4.304e-03, tstd=3.690e-01)\n","[ 60.0%] train loss=2.305e-06 | eval RMSE=6.174e-04 (rel=1.708e-03, tstd=3.615e-01) *best\n","[ 65.0%] train loss=5.139e-06 | eval RMSE=6.444e-04 (rel=1.742e-03, tstd=3.699e-01)\n","[ 70.0%] train loss=1.044e-05 | eval RMSE=9.631e-04 (rel=2.641e-03, tstd=3.646e-01)\n","[ 75.0%] train loss=3.201e-06 | eval RMSE=8.778e-04 (rel=2.354e-03, tstd=3.728e-01)\n","[ 80.0%] train loss=1.918e-06 | eval RMSE=1.361e-03 (rel=3.678e-03, tstd=3.701e-01)\n","[ 85.0%] train loss=5.832e-06 | eval RMSE=8.077e-04 (rel=2.208e-03, tstd=3.657e-01)\n","[ 90.0%] train loss=2.106e-06 | eval RMSE=6.173e-04 (rel=1.710e-03, tstd=3.611e-01) *best\n","[ 95.0%] train loss=1.171e-06 | eval RMSE=4.370e-04 (rel=1.171e-03, tstd=3.732e-01) *best\n","[100.0%] train loss=1.694e-06 | eval RMSE=6.932e-04 (rel=1.862e-03, tstd=3.723e-01)\n","Final: RMSE=2.448e-03 (rel=6.650e-03) | time=609.4s\n","ICNN pre-learn → quadratic: steps=500, batch=2048, lr=1.00e-03, ||Q||₂≈1.010, κ=100.21, ridge=0.01\n","[pre  20.0%] train MSE=2.522e+07 | eval RMSE=5.013e+03\n","[pre  40.0%] train MSE=1.658e+07 | eval RMSE=4.066e+03\n","[pre  60.0%] train MSE=1.290e+07 | eval RMSE=3.589e+03\n","[pre  80.0%] train MSE=1.158e+07 | eval RMSE=3.403e+03\n","[pre 100.0%] train MSE=1.134e+07 | eval RMSE=3.368e+03\n","ICNN pre-learn done in 1.0s\n","\n","================= rand_icnn2 — d=50 =================\n","ICNN(2) prelearn 500 steps to quadratic; median||y|| target=3.00 (obs=27.44 → scale=0.11); dual whitening set; loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=6.278e-05 | eval RMSE=6.721e-04 (rel=6.587e-03, tstd=1.020e-01) *best\n","[ 10.0%] train loss=2.521e-05 | eval RMSE=7.197e-04 (rel=7.033e-03, tstd=1.023e-01)\n","[ 15.0%] train loss=1.122e-05 | eval RMSE=4.719e-04 (rel=4.733e-03, tstd=9.971e-02) *best\n","[ 20.0%] train loss=3.426e-05 | eval RMSE=5.066e-04 (rel=5.023e-03, tstd=1.009e-01)\n","[ 25.0%] train loss=1.206e-05 | eval RMSE=6.481e-04 (rel=6.304e-03, tstd=1.028e-01)\n","[ 30.0%] train loss=2.478e-05 | eval RMSE=4.135e-04 (rel=4.031e-03, tstd=1.026e-01) *best\n","[ 35.0%] train loss=3.850e-05 | eval RMSE=8.971e-04 (rel=8.740e-03, tstd=1.026e-01)\n","[ 40.0%] train loss=5.345e-05 | eval RMSE=6.988e-04 (rel=6.751e-03, tstd=1.035e-01)\n","[ 45.0%] train loss=8.904e-06 | eval RMSE=4.072e-04 (rel=3.948e-03, tstd=1.032e-01) *best\n","[ 50.0%] train loss=1.579e-05 | eval RMSE=4.578e-04 (rel=4.562e-03, tstd=1.003e-01)\n","[ 55.0%] train loss=4.890e-05 | eval RMSE=3.909e-04 (rel=3.813e-03, tstd=1.025e-01) *best\n","[ 60.0%] train loss=4.681e-05 | eval RMSE=4.454e-04 (rel=4.423e-03, tstd=1.007e-01)\n","[ 65.0%] train loss=8.165e-06 | eval RMSE=4.708e-04 (rel=4.516e-03, tstd=1.042e-01)\n","[ 70.0%] train loss=1.104e-05 | eval RMSE=4.362e-04 (rel=4.291e-03, tstd=1.017e-01)\n","[ 75.0%] train loss=7.802e-06 | eval RMSE=3.216e-04 (rel=3.128e-03, tstd=1.028e-01) *best\n","[ 80.0%] train loss=8.605e-06 | eval RMSE=5.278e-04 (rel=5.177e-03, tstd=1.019e-01)\n","[ 85.0%] train loss=3.766e-06 | eval RMSE=2.901e-04 (rel=2.843e-03, tstd=1.020e-01) *best\n","[ 90.0%] train loss=8.525e-06 | eval RMSE=4.312e-04 (rel=4.215e-03, tstd=1.023e-01)\n","[ 95.0%] train loss=4.697e-06 | eval RMSE=3.171e-04 (rel=3.116e-03, tstd=1.018e-01)\n","[100.0%] train loss=4.027e-06 | eval RMSE=2.966e-04 (rel=2.900e-03, tstd=1.023e-01)\n","Final: RMSE=2.770e-04 (rel=2.727e-03) | time=1360.1s\n","\n","================= softplus_pairs — d=10 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=384, depth=4), params=1.20M\n","Sampler:  MACRO | batch=640 (B_macro) | steps=60000 | lr=1.00e-03\n","[  5.0%] train loss=6.464e-04 | eval RMSE=2.278e-02 (rel=2.353e-02, tstd=9.681e-01) *best\n","[ 10.0%] train loss=2.032e-04 | eval RMSE=2.292e-02 (rel=2.345e-02, tstd=9.775e-01)\n","[ 15.0%] train loss=1.106e-04 | eval RMSE=1.137e-02 (rel=1.152e-02, tstd=9.874e-01) *best\n","[ 20.0%] train loss=4.405e-04 | eval RMSE=2.666e-02 (rel=2.744e-02, tstd=9.714e-01)\n","[ 25.0%] train loss=1.147e-04 | eval RMSE=9.559e-03 (rel=9.782e-03, tstd=9.772e-01) *best\n","[ 30.0%] train loss=2.350e-05 | eval RMSE=8.938e-03 (rel=9.163e-03, tstd=9.754e-01) *best\n","[ 35.0%] train loss=1.074e-04 | eval RMSE=1.384e-02 (rel=1.426e-02, tstd=9.705e-01)\n","[ 40.0%] train loss=6.018e-05 | eval RMSE=1.079e-02 (rel=1.116e-02, tstd=9.670e-01)\n","[ 45.0%] train loss=3.496e-05 | eval RMSE=6.448e-03 (rel=6.443e-03, tstd=1.001e+00) *best\n","[ 50.0%] train loss=1.034e-05 | eval RMSE=3.110e-03 (rel=3.177e-03, tstd=9.789e-01) *best\n","[ 55.0%] train loss=2.225e-05 | eval RMSE=4.470e-03 (rel=4.573e-03, tstd=9.775e-01)\n","[ 60.0%] train loss=1.320e-05 | eval RMSE=3.345e-03 (rel=3.380e-03, tstd=9.896e-01)\n","[ 65.0%] train loss=9.789e-06 | eval RMSE=1.546e-03 (rel=1.583e-03, tstd=9.766e-01) *best\n","[ 70.0%] train loss=1.261e-05 | eval RMSE=3.371e-03 (rel=3.483e-03, tstd=9.679e-01)\n","[ 75.0%] train loss=1.987e-06 | eval RMSE=1.824e-03 (rel=1.885e-03, tstd=9.679e-01)\n","[ 80.0%] train loss=5.149e-06 | eval RMSE=2.525e-03 (rel=2.614e-03, tstd=9.659e-01)\n","[ 85.0%] train loss=2.147e-06 | eval RMSE=2.226e-03 (rel=2.289e-03, tstd=9.724e-01)\n","[ 90.0%] train loss=4.181e-06 | eval RMSE=3.325e-03 (rel=3.409e-03, tstd=9.754e-01)\n","[ 95.0%] train loss=1.310e-06 | eval RMSE=1.345e-03 (rel=1.362e-03, tstd=9.877e-01) *best\n","[100.0%] train loss=4.461e-07 | eval RMSE=9.942e-04 (rel=1.009e-03, tstd=9.850e-01) *best\n","Final: RMSE=1.011e-03 (rel=1.032e-03) | time=493.2s\n","\n","================= softplus_pairs — d=20 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=512, depth=5), params=2.66M\n","Sampler:  MACRO | batch=1280 (B_macro) | steps=90000 | lr=1.00e-03\n","[  5.0%] train loss=1.167e-04 | eval RMSE=4.903e-02 (rel=1.752e-02, tstd=2.799e+00) *best\n","[ 10.0%] train loss=4.791e-05 | eval RMSE=3.426e-02 (rel=1.221e-02, tstd=2.806e+00) *best\n","[ 15.0%] train loss=1.106e-04 | eval RMSE=4.396e-02 (rel=1.554e-02, tstd=2.828e+00)\n","[ 20.0%] train loss=1.751e-05 | eval RMSE=2.158e-02 (rel=7.712e-03, tstd=2.798e+00) *best\n","[ 25.0%] train loss=2.893e-05 | eval RMSE=2.488e-02 (rel=8.792e-03, tstd=2.830e+00)\n","[ 30.0%] train loss=1.180e-05 | eval RMSE=1.281e-02 (rel=4.547e-03, tstd=2.818e+00) *best\n","[ 35.0%] train loss=6.528e-05 | eval RMSE=3.087e-02 (rel=1.095e-02, tstd=2.820e+00)\n","[ 40.0%] train loss=2.072e-05 | eval RMSE=1.121e-02 (rel=3.978e-03, tstd=2.818e+00) *best\n","[ 45.0%] train loss=3.645e-06 | eval RMSE=6.012e-03 (rel=2.148e-03, tstd=2.798e+00) *best\n","[ 50.0%] train loss=4.696e-06 | eval RMSE=1.321e-02 (rel=4.691e-03, tstd=2.815e+00)\n","[ 55.0%] train loss=8.295e-06 | eval RMSE=1.203e-02 (rel=4.309e-03, tstd=2.792e+00)\n","[ 60.0%] train loss=9.774e-06 | eval RMSE=1.751e-02 (rel=6.218e-03, tstd=2.816e+00)\n","[ 65.0%] train loss=3.145e-06 | eval RMSE=1.090e-02 (rel=3.793e-03, tstd=2.874e+00)\n","[ 70.0%] train loss=4.170e-06 | eval RMSE=6.874e-03 (rel=2.435e-03, tstd=2.823e+00)\n","[ 75.0%] train loss=1.291e-06 | eval RMSE=6.074e-03 (rel=2.150e-03, tstd=2.826e+00)\n","[ 80.0%] train loss=1.103e-06 | eval RMSE=2.927e-03 (rel=1.023e-03, tstd=2.860e+00) *best\n","[ 85.0%] train loss=9.972e-07 | eval RMSE=3.000e-03 (rel=1.059e-03, tstd=2.833e+00)\n","[ 90.0%] train loss=1.364e-06 | eval RMSE=4.457e-03 (rel=1.569e-03, tstd=2.841e+00)\n","[ 95.0%] train loss=2.951e-06 | eval RMSE=6.618e-03 (rel=2.335e-03, tstd=2.834e+00)\n","[100.0%] train loss=2.475e-06 | eval RMSE=5.568e-03 (rel=2.009e-03, tstd=2.771e+00)\n","Final: RMSE=1.986e-02 (rel=7.020e-03) | time=1122.7s\n","\n","================= softplus_pairs — d=50 =================\n","Coupled Soft-plus: x_scale=0.5, block=64; dual whitening set; features: z=Wy(y-μ) (+ z^2 in model); loss=huber, δ=1.0\n","Model:    ResNet(width=768, depth=6), params=7.19M\n","Sampler:  MACRO | batch=3200 (B_macro) | steps=120000 | lr=1.00e-03\n","[  5.0%] train loss=3.316e-04 | eval RMSE=2.877e-01 (rel=2.511e-02, tstd=1.146e+01) *best\n","[ 10.0%] train loss=4.690e-04 | eval RMSE=3.487e-01 (rel=3.054e-02, tstd=1.142e+01)\n","[ 15.0%] train loss=3.703e-04 | eval RMSE=3.088e-01 (rel=2.703e-02, tstd=1.142e+01)\n","[ 20.0%] train loss=3.281e-04 | eval RMSE=2.981e-01 (rel=2.633e-02, tstd=1.132e+01)\n","[ 25.0%] train loss=2.917e-04 | eval RMSE=2.813e-01 (rel=2.478e-02, tstd=1.135e+01) *best\n","[ 30.0%] train loss=3.178e-04 | eval RMSE=2.785e-01 (rel=2.429e-02, tstd=1.146e+01) *best\n","[ 35.0%] train loss=3.011e-04 | eval RMSE=2.774e-01 (rel=2.408e-02, tstd=1.152e+01) *best\n","[ 40.0%] train loss=3.087e-04 | eval RMSE=2.701e-01 (rel=2.422e-02, tstd=1.115e+01) *best\n","[ 45.0%] train loss=2.886e-04 | eval RMSE=2.765e-01 (rel=2.441e-02, tstd=1.133e+01)\n","[ 50.0%] train loss=2.907e-04 | eval RMSE=2.796e-01 (rel=2.416e-02, tstd=1.157e+01)\n","[ 55.0%] train loss=2.780e-04 | eval RMSE=2.710e-01 (rel=2.415e-02, tstd=1.122e+01)\n","[ 60.0%] train loss=2.839e-04 | eval RMSE=2.689e-01 (rel=2.329e-02, tstd=1.154e+01) *best\n","[ 65.0%] train loss=2.668e-04 | eval RMSE=2.618e-01 (rel=2.295e-02, tstd=1.141e+01) *best\n","[ 70.0%] train loss=2.591e-04 | eval RMSE=2.616e-01 (rel=2.319e-02, tstd=1.128e+01) *best\n","[ 75.0%] train loss=2.490e-04 | eval RMSE=2.552e-01 (rel=2.282e-02, tstd=1.118e+01) *best\n","[ 80.0%] train loss=2.557e-04 | eval RMSE=2.554e-01 (rel=2.255e-02, tstd=1.133e+01)\n","[ 85.0%] train loss=2.636e-04 | eval RMSE=2.499e-01 (rel=2.181e-02, tstd=1.146e+01) *best\n","[ 90.0%] train loss=2.387e-04 | eval RMSE=2.466e-01 (rel=2.177e-02, tstd=1.133e+01) *best\n","[ 95.0%] train loss=2.439e-04 | eval RMSE=2.504e-01 (rel=2.200e-02, tstd=1.138e+01)\n","[100.0%] train loss=2.602e-04 | eval RMSE=2.637e-01 (rel=2.323e-02, tstd=1.135e+01)\n","Final: RMSE=2.606e-01 (rel=2.307e-02) | time=2852.1s\n","\n","========================= PER-TRIAL RESULTS =========================\n","trial |           task |    d |       RMSE |    relRMSE | time(s)\n","------------------------------------------------------------------------\n","    0 |       quad_spd |   10 |  1.345e-02 |  1.381e-02 |   296.7\n","    0 |       quad_spd |   20 |  5.813e-03 |  4.190e-03 |   515.8\n","    0 |       quad_spd |   50 |  5.021e-03 |  2.474e-03 |  1336.3\n","    0 |  exp_minus_lin |   10 |  1.576e-01 |  4.358e-02 |   307.7\n","    0 |  exp_minus_lin |   20 |  1.824e-02 |  5.710e-03 |   531.3\n","    0 |  exp_minus_lin |   50 |  4.432e-02 |  1.172e-02 |  1333.5\n","    0 |     rand_icnn2 |   10 |  4.550e-03 |  5.560e-03 |   356.6\n","    0 |     rand_icnn2 |   20 |  1.054e-03 |  3.052e-03 |   609.9\n","    0 |     rand_icnn2 |   50 |  2.823e-04 |  2.998e-03 |  1361.0\n","    0 | softplus_pairs |   10 |  6.289e-03 |  6.394e-03 |   493.3\n","    0 | softplus_pairs |   20 |  5.961e-03 |  2.121e-03 |  1123.4\n","    0 | softplus_pairs |   50 |  2.455e-01 |  2.169e-02 |  2856.0\n","    1 |       quad_spd |   10 |  9.141e-03 |  7.966e-03 |   298.4\n","    1 |       quad_spd |   20 |  1.090e-02 |  9.215e-03 |   516.4\n","    1 |       quad_spd |   50 |  2.084e-03 |  1.026e-03 |  1336.0\n","    1 |  exp_minus_lin |   10 |  2.281e-01 |  5.931e-02 |   305.7\n","    1 |  exp_minus_lin |   20 |  2.481e-01 |  6.858e-02 |   528.5\n","    1 |  exp_minus_lin |   50 |  1.939e-02 |  5.398e-03 |  1333.5\n","    1 |     rand_icnn2 |   10 |  9.155e-03 |  9.485e-03 |   358.0\n","    1 |     rand_icnn2 |   20 |  2.013e-03 |  5.523e-03 |   607.9\n","    1 |     rand_icnn2 |   50 |  2.602e-04 |  3.093e-03 |  1361.0\n","    1 | softplus_pairs |   10 |  1.129e-02 |  1.178e-02 |   496.4\n","    1 | softplus_pairs |   20 |  1.047e-02 |  3.716e-03 |  1130.7\n","    1 | softplus_pairs |   50 |  2.481e-01 |  2.195e-02 |  2858.9\n","    2 |       quad_spd |   10 |  7.508e-03 |  8.454e-03 |   296.2\n","    2 |       quad_spd |   20 |  1.412e-03 |  1.013e-03 |   515.3\n","    2 |       quad_spd |   50 |  4.475e-03 |  2.269e-03 |  1336.1\n","    2 |  exp_minus_lin |   10 |  1.368e-01 |  3.686e-02 |   309.0\n","    2 |  exp_minus_lin |   20 |  4.442e-02 |  1.310e-02 |   532.3\n","    2 |  exp_minus_lin |   50 |  4.462e-02 |  1.182e-02 |  1333.1\n","    2 |     rand_icnn2 |   10 |  1.560e-02 |  1.819e-02 |   361.7\n","    2 |     rand_icnn2 |   20 |  5.532e-04 |  1.422e-03 |   611.6\n","    2 |     rand_icnn2 |   50 |  2.657e-04 |  2.885e-03 |  1359.9\n","    2 | softplus_pairs |   10 |  4.268e-03 |  4.375e-03 |   493.8\n","    2 | softplus_pairs |   20 |  2.646e-03 |  9.254e-04 |  1131.9\n","    2 | softplus_pairs |   50 |  2.264e-01 |  1.986e-02 |  2857.2\n","    3 |       quad_spd |   10 |  1.410e-02 |  1.319e-02 |   296.0\n","    3 |       quad_spd |   20 |  1.055e-03 |  7.743e-04 |   515.6\n","    3 |       quad_spd |   50 |  7.456e-03 |  3.768e-03 |  1335.9\n","    3 |  exp_minus_lin |   10 |  1.128e-01 |  3.061e-02 |   307.1\n","    3 |  exp_minus_lin |   20 |  4.826e-02 |  1.131e-02 |   529.5\n","    3 |  exp_minus_lin |   50 |  3.373e-02 |  8.383e-03 |  1332.7\n","    3 |     rand_icnn2 |   10 |  7.419e-03 |  8.114e-03 |   358.7\n","    3 |     rand_icnn2 |   20 |  4.125e-04 |  1.158e-03 |   607.5\n","    3 |     rand_icnn2 |   50 |  2.594e-04 |  2.692e-03 |  1360.7\n","    3 | softplus_pairs |   10 |  6.931e-03 |  7.124e-03 |   493.5\n","    3 | softplus_pairs |   20 |  2.019e-03 |  7.078e-04 |  1122.5\n","    3 | softplus_pairs |   50 |  2.048e-01 |  1.811e-02 |  2858.6\n","    4 |       quad_spd |   10 |  1.417e-02 |  1.574e-02 |   299.2\n","    4 |       quad_spd |   20 |  1.768e-02 |  1.457e-02 |   518.0\n","    4 |       quad_spd |   50 |  1.161e-03 |  6.265e-04 |  1336.1\n","    4 |  exp_minus_lin |   10 |  1.712e-01 |  4.773e-02 |   306.7\n","    4 |  exp_minus_lin |   20 |  1.475e-01 |  3.580e-02 |   529.7\n","    4 |  exp_minus_lin |   50 |  8.218e-03 |  2.420e-03 |  1333.8\n","    4 |     rand_icnn2 |   10 |  4.851e-03 |  5.623e-03 |   358.8\n","    4 |     rand_icnn2 |   20 |  2.448e-03 |  6.650e-03 |   609.4\n","    4 |     rand_icnn2 |   50 |  2.770e-04 |  2.727e-03 |  1360.1\n","    4 | softplus_pairs |   10 |  1.011e-03 |  1.032e-03 |   493.2\n","    4 | softplus_pairs |   20 |  1.986e-02 |  7.020e-03 |  1122.7\n","    4 | softplus_pairs |   50 |  2.606e-01 |  2.307e-02 |  2852.1\n","\n","========================= AGGREGATED (mean ± sd over trials) =========================\n","          task |    d |           RMSE mean±sd |        relRMSE mean±sd |     time mean±sd (s)\n","--------------------------------------------------------------------------------------------\n"," exp_minus_lin |   10 |  1.613e-01 ±  4.339e-02 |  4.362e-02 ±  1.093e-02 |   307.2 ±     1.2\n"," exp_minus_lin |   20 |  1.013e-01 ±  9.568e-02 |  2.690e-02 ±  2.598e-02 |   530.3 ±     1.5\n"," exp_minus_lin |   50 |  3.006e-02 ±  1.597e-02 |  7.948e-03 ±  4.077e-03 |  1333.3 ±     0.4\n","      quad_spd |   10 |  1.167e-02 ±  3.125e-03 |  1.183e-02 ±  3.441e-03 |   297.3 ±     1.4\n","      quad_spd |   20 |  7.372e-03 ±  7.010e-03 |  5.953e-03 ±  5.901e-03 |   516.2 ±     1.1\n","      quad_spd |   50 |  4.039e-03 ±  2.497e-03 |  2.033e-03 ±  1.251e-03 |  1336.1 ±     0.1\n","    rand_icnn2 |   10 |  8.315e-03 ±  4.493e-03 |  9.395e-03 ±  5.195e-03 |   358.8 ±     1.9\n","    rand_icnn2 |   20 |  1.296e-03 ±  8.989e-04 |  3.561e-03 ±  2.450e-03 |   609.3 ±     1.6\n","    rand_icnn2 |   50 |  2.689e-04 ±  1.026e-05 |  2.879e-03 ±  1.718e-04 |  1360.6 ±     0.5\n","softplus_pairs |   10 |  5.958e-03 ±  3.769e-03 |  6.140e-03 ±  3.937e-03 |   494.0 ±     1.3\n","softplus_pairs |   20 |  8.192e-03 ±  7.335e-03 |  2.898e-03 ±  2.596e-03 |  1126.2 ±     4.7\n","softplus_pairs |   50 |  2.371e-01 ±  2.181e-02 |  2.094e-02 ±  1.957e-03 |  2856.6 ±     2.8\n"]}],"source":["# dlt_resnet_suite_v9_smoke.py — ResNet-only, implicit (DLT-style) learning of f* for:\n","#   • Quadratic (random SPD)\n","#   • Exp−Linear (unit a & unit b)\n","#   • 2‑layer ICNN (pre‑learned to quadratic, then frozen)\n","#   • Coupled Soft‑plus (chunked O(d^2))\n","#\n","# This file includes a quick smoke test at the bottom for dims {5,10}. Comment it out for full runs.\n","\n","import os, sys, time, math, argparse, copy\n","from dataclasses import dataclass\n","from typing import Optional, List, Dict, Tuple\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# ========================= Device & seed =========================\n","def get_device():\n","    if torch.cuda.is_available(): return torch.device(\"cuda\")\n","    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available(): return torch.device(\"mps\")\n","    return torch.device(\"cpu\")\n","\n","def set_seed(seed:int):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","# ========================= SPD helpers ===========================\n","@torch.no_grad()\n","def make_spd(d:int, seed:int, norm:str=\"spectral\", ridge:float=1e-2, device=None):\n","    torch.manual_seed(seed)\n","    A = torch.randn(d, d, dtype=torch.float32, device=device)\n","    Q = A.t() @ A\n","    if norm == \"spectral\":\n","        lam = torch.linalg.eigvalsh(Q)\n","        lam_max = lam.max().clamp_min(1e-12)\n","        Q = Q / lam_max\n","    elif norm == \"mean\":\n","        Q = Q / (torch.trace(Q) / float(d)).clamp_min(1e-12)\n","    else:\n","        raise ValueError(f\"unknown spd_norm '{norm}'\")\n","    Q = Q + ridge * torch.eye(d, device=device)\n","    return Q\n","\n","@torch.no_grad()\n","def spd_stats(Q:torch.Tensor)->Dict[str,float]:\n","    lam = torch.linalg.eigvalsh(Q)\n","    return {\"lambda_min\": float(lam.min()),\n","            \"lambda_max\": float(lam.max()),\n","            \"kappa\": float((lam.max()/lam.min()).item()),\n","            \"trace\": float(torch.trace(Q))}\n","\n","# ========================= Tasks ================================\n","@dataclass\n","class QuadSPD:\n","    d:int; Q:torch.Tensor; U:torch.Tensor; lam_inv_sqrt:torch.Tensor\n","    def sample_x(self,B:int): return torch.randn(B, self.d, device=self.Q.device)\n","    def f(self,x): return 0.5*torch.sum((x @ self.Q) * x, dim=-1)\n","    def grad(self,x): return x @ self.Q\n","    def z_from_y(self,y): return (y @ self.U) * self.lam_inv_sqrt\n","\n","@dataclass\n","class ExpMinusLin:\n","    d:int; a:torch.Tensor; b:torch.Tensor\n","    def sample_x(self,B:int): return torch.randn(B, self.d, device=self.a.device)\n","    def f(self,x):\n","        ax = x @ self.a\n","        return torch.exp(ax) - (x @ self.b)\n","    def grad(self,x):\n","        ax = x @ self.a\n","        return torch.exp(ax)[:,None]*self.a[None,:] - self.b[None,:]\n","    def z_from_y(self,y):\n","        alpha = ((y + self.b[None,:]) @ self.a)[:, None].clamp_min(1e-12)\n","        return torch.cat([alpha, torch.log(alpha)], dim=-1)\n","\n","# ---- Trainable 2-layer ICNN (pre-learning target) ----\n","class ICNN2Trainable(nn.Module):\n","    \"\"\"\n","    2-layer ICNN with softplus; convexity via softplus(raw) on z->z and z->out weights.\n","    \"\"\"\n","    def __init__(self, d:int, h1:int=128, h2:int=128, seed:int=0, device=None):\n","        super().__init__()\n","        g = torch.Generator(device=device); g.manual_seed(seed)\n","        self.U1 = nn.Parameter(torch.randn(d, h1, generator=g, device=device)/math.sqrt(d))\n","        self.U2 = nn.Parameter(torch.randn(d, h2, generator=g, device=device)/math.sqrt(d))\n","        self.b1 = nn.Parameter(torch.zeros(h1, device=device))\n","        self.b2 = nn.Parameter(torch.zeros(h2, device=device))\n","        self.raw_W21 = nn.Parameter(torch.randn(h1, h2, generator=g, device=device)*0.5)\n","        self.raw_wout= nn.Parameter(torch.randn(h2, 1, generator=g, device=device)*0.5)\n","        self.uout = nn.Parameter(torch.randn(d, 1, generator=g, device=device)/math.sqrt(d))\n","    def forward(self, x):\n","        W21 = F.softplus(self.raw_W21)    # nonnegative\n","        wout= F.softplus(self.raw_wout)   # nonnegative\n","        z1 = F.softplus(x @ self.U1 + self.b1)\n","        z2 = F.softplus(z1 @ W21 + x @ self.U2 + self.b2)\n","        return (z2 @ wout).squeeze(-1) + (x @ self.uout).squeeze(-1)\n","\n","@dataclass\n","class ICNNTask:\n","    d:int; net:nn.Module; device:torch.device\n","    x_scale:float; y_mu:torch.Tensor; Wy_U:torch.Tensor; Wy_diag_inv_sqrt:torch.Tensor\n","    def sample_x(self,B:int): return self.x_scale * torch.randn(B, self.d, device=self.device)\n","    def f(self,x): return self.net(x)\n","    def grad(self,x):\n","        # Enable grad on x ONLY to get y, then detach. Never backprop through f.\n","        if not x.requires_grad:\n","            x = x.detach().requires_grad_(True)\n","        fx = self.net(x)\n","        (gy,) = torch.autograd.grad(fx.sum(), x, create_graph=False, retain_graph=False)\n","        return gy.detach()\n","    def z_from_y(self,y): return ((y - self.y_mu[None,:]) @ self.Wy_U) * self.Wy_diag_inv_sqrt\n","\n","# ---- Coupled Soft-plus with chunked O(d^2) kernel ----\n","def softplus_pairs_f_grad(X:torch.Tensor, block:int=64):\n","    \"\"\"\n","    f(x)=sum_{i<j} softplus(x_i+x_j)\n","    Compute f and ∇f in O(d^2) time with O(B*block) memory.\n","    \"\"\"\n","    B, d = X.shape\n","    f = X.new_zeros(B)\n","    g = X.new_zeros(B, d)\n","    for i in range(d-1):\n","        xi = X[:, i:i+1]\n","        j = i + 1\n","        while j < d:\n","            J = slice(j, min(d, j+block))\n","            z = xi + X[:, J]\n","            sp = F.softplus(z)\n","            s  = torch.sigmoid(z)\n","            f += sp.sum(dim=1)\n","            g[:, i] += s.sum(dim=1)\n","            g[:, J] += s\n","            j += block\n","    return f, g\n","\n","@dataclass\n","class SoftplusPairsTask:\n","    d:int; device:torch.device; x_scale:float; block:int\n","    y_mu:torch.Tensor; Wy_U:torch.Tensor; Wy_diag_inv_sqrt:torch.Tensor\n","    def sample_x(self,B:int): return self.x_scale * torch.randn(B, self.d, device=self.device)\n","    def f(self,x): f,_ = softplus_pairs_f_grad(x, block=self.block); return f\n","    def grad(self,x): _,g = softplus_pairs_f_grad(x, block=self.block); return g\n","    def z_from_y(self,y): return ((y - self.y_mu[None,:]) @ self.Wy_U) * self.Wy_diag_inv_sqrt\n","\n","# ========================= Probing & whitening ===================\n","def icnn_probe_y_stats(net:nn.Module, d:int, device, x_scale:float, nsamp:int=16384):\n","    \"\"\"Uses autograd to compute y; then detaches for stats.\"\"\"\n","    B = 4096\n","    iters = max(1, (nsamp + B - 1)//B)\n","    norms = []\n","    y_sum = torch.zeros(d, device=device)\n","    yy_sum = torch.zeros(d, d, device=device)\n","    n = 0\n","    for _ in range(iters):\n","        b = min(B, nsamp - n)\n","        x = (x_scale * torch.randn(b, d, device=device)).requires_grad_(True)\n","        fx = net(x)\n","        (y,) = torch.autograd.grad(fx.sum(), x, create_graph=False, retain_graph=False)\n","        y = y.detach()\n","        norms.append(torch.linalg.norm(y, dim=1))\n","        y_sum += y.sum(dim=0)\n","        yy_sum += y.t() @ y\n","        n += b\n","    norms = torch.cat(norms, dim=0)\n","    mu = y_sum / n\n","    cov = (yy_sum / n) - mu[:,None] @ mu[None,:]\n","    eig = torch.linalg.eigvalsh(cov)\n","    return {\"med_norm\": float(torch.median(norms)),\n","            \"mu\": mu, \"cov\": cov,\n","            \"eig_min\": float(eig.min()), \"eig_max\": float(eig.max())}\n","\n","@torch.no_grad()\n","def softplus_probe_y_stats(d:int, device, x_scale:float, block:int, nsamp:int=16384):\n","    B = 2048\n","    iters = max(1, (nsamp + B - 1)//B)\n","    y_sum = torch.zeros(d, device=device)\n","    yy_sum = torch.zeros(d, d, device=device)\n","    n = 0\n","    for _ in range(iters):\n","        b = min(B, nsamp - n)\n","        x = x_scale * torch.randn(b, d, device=device)\n","        _, y = softplus_pairs_f_grad(x, block=block)\n","        y_sum += y.sum(dim=0)\n","        yy_sum += y.t() @ y\n","        n += b\n","    mu = y_sum / n\n","    cov = (yy_sum / n) - mu[:,None] @ mu[None,:]\n","    eig = torch.linalg.eigvalsh(cov)\n","    return {\"mu\": mu, \"cov\": cov, \"eig_min\": float(eig.min()), \"eig_max\": float(eig.max())}\n","\n","# ========================= Target scaler =========================\n","class TargetScaler:\n","    def __init__(self, momentum:float=0.98, eps:float=1e-8):\n","        self.momentum, self.eps = momentum, eps\n","        self.mu = None; self.sig = None\n","    @torch.no_grad()\n","    def fit_batch(self, t:torch.Tensor):\n","        bmu = t.mean()\n","        bsd = t.std(unbiased=False).clamp_min(self.eps)\n","        if self.mu is None: self.mu, self.sig = bmu, bsd\n","        else:\n","            self.mu  = self.momentum*self.mu  + (1-self.momentum)*bmu\n","            self.sig = self.momentum*self.sig + (1-self.momentum)*bsd\n","        return (t - self.mu)/self.sig, float(self.mu), float(self.sig)\n","    @torch.no_grad()\n","    def denorm(self, tzn:torch.Tensor):\n","        if self.mu is None or self.sig is None: return tzn\n","        return tzn*self.sig + self.mu\n","\n","# ========================= ResNet learner ========================\n","class PreActBlock(nn.Module):\n","    def __init__(self, dim:int, hidden:int, alpha:float=0.5):\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(dim)\n","        self.fc1 = nn.Linear(dim, hidden)\n","        self.act = nn.GELU()\n","        self.ln2 = nn.LayerNorm(hidden)\n","        self.fc2 = nn.Linear(hidden, dim)\n","        self.alpha = alpha\n","        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)\n","        nn.init.xavier_uniform_(self.fc2.weight, gain=0.1); nn.init.zeros_(self.fc2.bias)\n","    def forward(self, x):\n","        y = self.fc1(self.ln1(x)); y = self.act(y); y = self.fc2(self.ln2(y))\n","        return x + self.alpha*y\n","\n","class ResNetDLT(nn.Module):\n","    def __init__(self, in_dim:int, width:int, depth:int, square_feat:bool=True, use_layernorm:bool=False):\n","        super().__init__()\n","        self.square = square_feat\n","        eff_in = in_dim*(2 if self.square else 1)\n","        self.in_ln = nn.LayerNorm(eff_in) if use_layernorm else None\n","        self.embed = nn.Linear(eff_in, width)\n","        self.blocks = nn.ModuleList([PreActBlock(width, width) for _ in range(depth)])\n","        self.head_ln = nn.LayerNorm(width)\n","        self.head = nn.Linear(width, 1)\n","        nn.init.xavier_uniform_(self.embed.weight); nn.init.zeros_(self.embed.bias)\n","        nn.init.xavier_uniform_(self.head.weight);  nn.init.zeros_(self.head.bias)\n","    def forward(self, z):\n","        if self.square: z = torch.cat([z, z*z], dim=-1)\n","        if self.in_ln is not None: z = self.in_ln(z)\n","        h = self.embed(z)\n","        for b in self.blocks: h = b(h)\n","        return self.head(self.head_ln(h)).squeeze(-1)\n","\n","def resnet_size_for_dim(dim:int)->Tuple[int,int]:\n","    if dim <= 10:  return 384, 4\n","    if dim <= 20:  return 512, 5\n","    if dim <= 50:  return 768, 6\n","    if dim <= 100: return 1024, 7\n","    return 1280, 8\n","\n","# ========================= Batch rules & loss ====================\n","def macro_batch_size(d:int)->int:\n","    return max(600, 64*d)  # single fresh macro-batch per step\n","\n","def huber_loss(pred, target, delta:float=1.0):\n","    return F.huber_loss(pred, target, delta=delta, reduction=\"mean\")\n","\n","# ========================= ICNN pre-learning =====================\n","def prelearn_icnn_to_quadratic(net:ICNN2Trainable,\n","                               d:int, device,\n","                               steps:int=500, lr:float=1e-3, batch:int=2048,\n","                               spd_norm:str=\"spectral\", ridge:float=1e-2,\n","                               seed:int=12345):\n","    Q = make_spd(d, seed=seed, norm=spd_norm, ridge=ridge, device=device)\n","    stats = spd_stats(Q)\n","    opt = optim.AdamW(net.parameters(), lr=lr, weight_decay=1e-4)\n","    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=steps, eta_min=1e-5)\n","    print(f\"ICNN pre-learn → quadratic: steps={steps}, batch={batch}, lr={lr:.2e}, \"\n","          f\"||Q||₂≈{stats['lambda_max']:.3f}, κ={stats['kappa']:.2f}, ridge={ridge:g}\")\n","    t0 = time.time()\n","    for s in range(1, steps+1):\n","        x = torch.randn(batch, d, device=device)\n","        target = 0.5*torch.sum((x @ Q)*x, dim=-1)\n","        pred = net(x)\n","        loss = F.mse_loss(pred, target)\n","        opt.zero_grad(set_to_none=True); loss.backward()\n","        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n","        opt.step(); sched.step()\n","        if s % max(1, steps//5) == 0 or s == steps:\n","            with torch.no_grad():\n","                xe = torch.randn(batch, d, device=device)\n","                te = 0.5*torch.sum((xe @ Q)*xe, dim=-1)\n","                pe = net(xe)\n","                rmse = torch.sqrt(F.mse_loss(pe, te)).item()\n","            print(f\"[pre {100*s/steps:5.1f}%] train MSE={loss.item():.3e} | eval RMSE={rmse:.3e}\")\n","    print(f\"ICNN pre-learn done in {time.time()-t0:.1f}s\")\n","\n","# ========================= Macro/mini sample =====================\n","def compute_macro_batch(obj, B:int):\n","    \"\"\"\n","    Produce a single macro-/mini-batch (fresh randomness) and targets with *no gradient through f*.\n","    Returns (y_detached, t_detached, x_detached).\n","    \"\"\"\n","    with torch.enable_grad():\n","        x = obj[\"sample_x\"](B)\n","        x.requires_grad_(True)\n","        y = obj[\"grad\"](x)          # ICNN task returns y.detach(); others are analytic\n","        y = y.detach()\n","        x_det = x.detach()\n","    with torch.no_grad():\n","        t = (x_det*y).sum(dim=-1) - obj[\"f\"](x_det)\n","    return y, t, x_det\n","\n","# ========================= Evaluation ============================\n","def evaluate_generic(model:nn.Module,\n","                     task_name:str,\n","                     obj,\n","                     z_map,\n","                     tscaler:Optional[TargetScaler],\n","                     device,\n","                     zdim:int,\n","                     test_B:int=8192) -> Dict[str, float]:\n","    y, t, x = compute_macro_batch(obj, test_B)  # detached\n","    with torch.no_grad():\n","        z = z_map(y, x)\n","        pred_zn = model(z)\n","        pred = tscaler.denorm(pred_zn) if tscaler is not None else pred_zn\n","        mse  = torch.mean((pred - t)**2).item()\n","        rmse = math.sqrt(mse)\n","        mae  = torch.mean((pred - t).abs()).item()\n","        t_std = t.std(unbiased=False).item()\n","        rel = rmse / max(t_std, 1e-8)\n","    return {\"rmse\":rmse, \"mae\":mae, \"rel_rmse\":rel, \"t_std\":t_std}\n","\n","# ========================= Task builders =========================\n","def build_task_quad_spd(d:int, args, device):\n","    Q = make_spd(d, seed=args.seed + d*17, norm=args.spd_norm, ridge=args.ridge, device=device)\n","    stats = spd_stats(Q)\n","    lam, U = torch.linalg.eigh(Q)\n","    lam_inv_sqrt = (1.0 / torch.sqrt(lam.clamp_min(1e-12))).to(device)\n","    quad = QuadSPD(d=d, Q=Q, U=U, lam_inv_sqrt=lam_inv_sqrt)\n","    obj = {\"sample_x\": quad.sample_x, \"f\": quad.f, \"grad\": quad.grad,\n","           \"z_from_y\": lambda y,x: quad.z_from_y(y), \"z_dim\": d}\n","    meta = (f\"Q stats: ||Q||₂≈{stats['lambda_max']:.3f}, κ={stats['kappa']:.2f}, trace={stats['trace']:.1f} | \"\n","            f\"features: z=Q^(-1/2) y (+ z^2 in model)\")\n","    return obj, meta\n","\n","def build_task_exp_minus_lin(d:int, args, device):\n","    g = torch.Generator(device=device); g.manual_seed(args.seed + d*31)\n","    a_raw = torch.randn(d, generator=g, device=device)\n","    b_raw = torch.randn(d, generator=g, device=device)\n","    a = a_raw / (a_raw.norm() + 1e-12)  # unit vector (normalized)\n","    b = b_raw / (b_raw.norm() + 1e-12)  # unit vector (normalized)\n","    task = ExpMinusLin(d=d, a=a, b=b)\n","    obj = {\"sample_x\": task.sample_x, \"f\": task.f, \"grad\": task.grad,\n","           \"z_from_y\": lambda y,x: task.z_from_y(y), \"z_dim\": 2}\n","    meta = (f\"Exp−Linear: ||a||=1, ||b||=1 (both normalized) | features: z=[⟨y+b,a⟩, log⟨y+b,a⟩] (+ squares)\")\n","    return obj, meta\n","\n","def build_task_rand_icnn2(d:int, args, device):\n","    net = ICNN2Trainable(d=d, h1=128, h2=128, seed=args.seed + d*53, device=device)\n","    if args.icnn_prelearn_steps > 0:\n","        prelearn_icnn_to_quadratic(net, d, device,\n","                                   steps=args.icnn_prelearn_steps,\n","                                   lr=args.icnn_prelearn_lr,\n","                                   batch=args.icnn_prelearn_batch,\n","                                   spd_norm=args.icnn_prelearn_spd_norm,\n","                                   ridge=args.icnn_prelearn_ridge,\n","                                   seed=args.seed + 777*d)\n","    for p in net.parameters(): p.requires_grad_(False)\n","    net.eval()\n","    # Auto-scale then dual whitening\n","    probe1 = icnn_probe_y_stats(net, d, device, x_scale=1.0, nsamp=min(args.icnn_whiten_samples, 32768))\n","    med = probe1[\"med_norm\"]; target = args.icnn_target_y_med\n","    scale = float(np.clip(target / max(med, 1e-6), 0.1, 10.0))\n","    probe2 = icnn_probe_y_stats(net, d, device, x_scale=scale, nsamp=args.icnn_whiten_samples)\n","    mu_y = probe2[\"mu\"]; cov_y = probe2[\"cov\"] + args.icnn_cov_reg * torch.eye(d, device=device)\n","    lam, U = torch.linalg.eigh(cov_y); lam_inv_sqrt = (1.0 / torch.sqrt(lam.clamp_min(1e-12))).to(device)\n","    task = ICNNTask(d=d, net=net, device=device, x_scale=scale, y_mu=mu_y,\n","                    Wy_U=U, Wy_diag_inv_sqrt=lam_inv_sqrt)\n","    obj = {\"sample_x\": task.sample_x, \"f\": task.f, \"grad\": task.grad,\n","           \"z_from_y\": lambda y,x: task.z_from_y(y), \"z_dim\": d}\n","    meta = (f\"ICNN(2) prelearn {args.icnn_prelearn_steps} steps to quadratic; \"\n","            f\"median||y|| target={target:.2f} (obs={med:.2f} → scale={scale:.2f}); dual whitening set; \"\n","            f\"loss={args.loss}, δ={args.huber_delta}\")\n","    return obj, meta\n","\n","def build_task_softplus_pairs(d:int, args, device):\n","    probe = softplus_probe_y_stats(d, device, x_scale=args.softplus_x_scale,\n","                                   block=args.softplus_block, nsamp=args.softplus_whiten_samples)\n","    mu_y = probe[\"mu\"]; cov_y = probe[\"cov\"] + args.softplus_cov_reg * torch.eye(d, device=device)\n","    lam, U = torch.linalg.eigh(cov_y); lam_inv_sqrt = (1.0 / torch.sqrt(lam.clamp_min(1e-12))).to(device)\n","    task = SoftplusPairsTask(d=d, device=device, x_scale=args.softplus_x_scale, block=args.softplus_block,\n","                             y_mu=mu_y, Wy_U=U, Wy_diag_inv_sqrt=lam_inv_sqrt)\n","    obj = {\"sample_x\": task.sample_x, \"f\": task.f, \"grad\": task.grad,\n","           \"z_from_y\": lambda y,x: task.z_from_y(y), \"z_dim\": d}\n","    meta = (f\"Coupled Soft-plus: x_scale={args.softplus_x_scale}, block={args.softplus_block}; dual whitening set; \"\n","            f\"features: z=Wy(y-μ) (+ z^2 in model); loss={args.loss}, δ={args.huber_delta}\")\n","    return obj, meta\n","\n","TASK_BUILDERS = {\n","    \"quad_spd\": build_task_quad_spd,\n","    \"exp_minus_lin\": build_task_exp_minus_lin,\n","    \"rand_icnn2\": build_task_rand_icnn2,\n","    \"softplus_pairs\": build_task_softplus_pairs,\n","}\n","\n","# ========================= Training (macro / stream) =============\n","def train_task(task_name:str, d:int, build_task, args, device):\n","    obj, meta = build_task(d, args, device)\n","    zdim = obj[\"z_dim\"]\n","    width, depth = resnet_size_for_dim(max(d, zdim))\n","    model = ResNetDLT(in_dim=zdim, width=width, depth=depth,\n","                      square_feat=True, use_layernorm=False).to(device)\n","    n_params = sum(p.numel() for p in model.parameters())\n","\n","    total_steps = (args.steps if d<=10 else\n","                   int(args.steps*1.5) if d<=20 else\n","                   int(args.steps*2.0) if d<=50 else\n","                   int(args.steps*2.5) if d<=100 else\n","                   int(args.steps*3.0))\n","\n","    B_macro = macro_batch_size(d)\n","    if args.sampler == \"macro\":\n","        B = B_macro; lr_used = args.lr\n","    else:\n","        B = max(1, int(args.mb_factor * B_macro))\n","        lr_used = args.lr * (B / float(B_macro))\n","\n","    if task_name == \"softplus_pairs\" and args.softplus_max_batch > 0:\n","        B = min(B, args.softplus_max_batch)\n","\n","    opt = optim.AdamW(model.parameters(), lr=lr_used, weight_decay=1e-4)\n","    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=total_steps, eta_min=1e-6)\n","    tscaler = TargetScaler(momentum=0.98)\n","\n","    print(f\"\\n================= {task_name} — d={d} =================\")\n","    print(meta)\n","    print(f\"Model:    ResNet(width={width}, depth={depth}), params={n_params/1e6:.2f}M\")\n","    print(f\"Sampler:  {args.sampler.upper()} | batch={B} \"\n","          f\"({'B_macro' if args.sampler=='macro' else f'{args.mb_factor:.2f}×B_macro'}) \"\n","          f\"| steps={total_steps} | lr={lr_used:.2e}\")\n","\n","    best_rmse = float(\"inf\"); best_state = None\n","    t0 = time.time()\n","    for step in range(1, total_steps+1):\n","        y, t, x = compute_macro_batch(obj, B)    # detached; no grad through f\n","        z = obj[\"z_from_y\"](y, x)\n","        tzn, _, _ = tscaler.fit_batch(t)\n","\n","        pred = model(z)\n","        loss = huber_loss(pred, tzn, delta=args.huber_delta) if args.loss == \"huber\" \\\n","               else torch.mean((pred - tzn)**2)\n","\n","        opt.zero_grad(set_to_none=True)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0 if d <= 20 else 0.5)\n","        opt.step(); sched.step()\n","\n","        if step % max(1, total_steps//20) == 0 or step == total_steps:\n","            ev = evaluate_generic(model, task_name, obj, lambda yy,xx: obj[\"z_from_y\"](yy,xx),\n","                                  tscaler, device, zdim, test_B=max(8192, 2*B_macro//3))\n","            flag = \"\"\n","            if ev[\"rmse\"] < best_rmse:\n","                best_rmse = ev[\"rmse\"]; best_state = {k:v.detach().clone() for k,v in model.state_dict().items()}\n","                flag = \" *best\"\n","            print(f\"[{100.0*step/total_steps:5.1f}%] train loss={loss.item():.3e} | \"\n","                  f\"eval RMSE={ev['rmse']:.3e} (rel={ev['rel_rmse']:.3e}, tstd={ev['t_std']:.3e}){flag}\")\n","\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","\n","    total_time = time.time() - t0\n","    final = evaluate_generic(model, task_name, obj, lambda yy,xx: obj[\"z_from_y\"](yy,xx),\n","                             tscaler, device, zdim, test_B=max(16384, B_macro))\n","    print(f\"Final: RMSE={final['rmse']:.3e} (rel={final['rel_rmse']:.3e}) | time={total_time:.1f}s\")\n","    return {\"task\":task_name, \"d\":d, \"rmse\":final[\"rmse\"], \"rel_rmse\":final[\"rel_rmse\"],\n","            \"time\":total_time, \"params\":n_params}\n","\n","# ========================= Aggregation utils =====================\n","def _agg_mean_std(xs:List[float])->Tuple[float,float]:\n","    if not xs: return float(\"nan\"), float(\"nan\")\n","    if len(xs)==1: return xs[0], 0.0\n","    arr = np.array(xs, dtype=np.float64)\n","    return float(arr.mean()), float(arr.std(ddof=1))\n","\n","def print_aggregate_table(rows:List[Dict]):\n","    # rows: dict(task,d,rmse,rel_rmse,time,params,trial)\n","    key_to_vals = {}\n","    for r in rows:\n","        key = (r[\"task\"], r[\"d\"])\n","        key_to_vals.setdefault(key, {\"rmse\":[], \"rel\":[], \"time\":[], \"params\":r[\"params\"]})\n","        key_to_vals[key][\"rmse\"].append(r[\"rmse\"])\n","        key_to_vals[key][\"rel\"].append(r[\"rel_rmse\"])\n","        key_to_vals[key][\"time\"].append(r[\"time\"])\n","    print(\"\\n========================= AGGREGATED (mean ± sd over trials) =========================\")\n","    print(f\"{'task':>14} | {'d':>4} | {'RMSE mean±sd':>22} | {'relRMSE mean±sd':>22} | {'time mean±sd (s)':>20}\")\n","    print(\"-\"*92)\n","    for (task,d), vals in sorted(key_to_vals.items(), key=lambda k:(k[0][0], k[0][1])):\n","        m_rmse, s_rmse = _agg_mean_std(vals[\"rmse\"])\n","        m_rel,  s_rel  = _agg_mean_std(vals[\"rel\"])\n","        m_t,    s_t    = _agg_mean_std(vals[\"time\"])\n","        print(f\"{task:>14} | {d:4d} | {m_rmse:10.3e} ± {s_rmse:10.3e} | {m_rel:10.3e} ± {s_rel:10.3e} | {m_t:7.1f} ± {s_t:7.1f}\")\n","\n","# ========================= Orchestrators =========================\n","def build_parser():\n","    ap = argparse.ArgumentParser()\n","    ap.add_argument(\"--functions\", nargs=\"+\", default=[\"quad_spd\",\"exp_minus_lin\",\"rand_icnn2\",\"softplus_pairs\"])\n","    ap.add_argument(\"--dims\", nargs=\"+\", type=int, default=[10,20,50])\n","    ap.add_argument(\"--trials\", type=int, default=5, help=\"number of independent runs per (function, d)\")\n","    ap.add_argument(\"--steps\", type=int, default=60000)\n","    ap.add_argument(\"--lr\", type=float, default=1e-3)\n","    ap.add_argument(\"--seed\", type=int, default=42)\n","    ap.add_argument(\"--spd_norm\", choices=[\"spectral\",\"mean\"], default=\"spectral\")\n","    ap.add_argument(\"--ridge\", type=float, default=1e-2)\n","    # ICNN pre-learn\n","    ap.add_argument(\"--icnn_prelearn_steps\", type=int, default=500)\n","    ap.add_argument(\"--icnn_prelearn_lr\", type=float, default=1e-3)\n","    ap.add_argument(\"--icnn_prelearn_batch\", type=int, default=2048)\n","    ap.add_argument(\"--icnn_prelearn_spd_norm\", choices=[\"spectral\",\"mean\"], default=\"spectral\")\n","    ap.add_argument(\"--icnn_prelearn_ridge\", type=float, default=1e-2)\n","    # ICNN conditioning\n","    ap.add_argument(\"--icnn_whiten_samples\", type=int, default=32768)\n","    ap.add_argument(\"--icnn_target_y_med\", type=float, default=3.0)\n","    ap.add_argument(\"--icnn_cov_reg\", type=float, default=1e-3)\n","    # Soft-plus knobs\n","    ap.add_argument(\"--softplus_x_scale\", type=float, default=0.5)\n","    ap.add_argument(\"--softplus_block\", type=int, default=64)\n","    ap.add_argument(\"--softplus_max_batch\", type=int, default=0)  # 0 => no cap\n","    ap.add_argument(\"--softplus_whiten_samples\", type=int, default=32768)\n","    ap.add_argument(\"--softplus_cov_reg\", type=float, default=1e-3)\n","    # Learner loss\n","    ap.add_argument(\"--loss\", choices=[\"mse\",\"huber\"], default=\"huber\")\n","    ap.add_argument(\"--huber_delta\", type=float, default=1.0)\n","    # Sampler (no dataset/pool ever)\n","    ap.add_argument(\"--sampler\", choices=[\"macro\",\"stream\"], default=\"macro\",\n","                    help=\"macro: fresh macro-batch per step; stream: fresh mini-batch per step\")\n","    ap.add_argument(\"--mb_factor\", type=float, default=0.25,\n","                    help=\"mini-batch size as fraction of B(d); used when --sampler=stream\")\n","    return ap\n","\n","def run_trials(args):\n","    device = get_device()\n","    print(f\"Device: {device.type}, torch {torch.__version__}\")\n","\n","    all_rows = []\n","    base_seed = int(args.seed)\n","    for t in range(args.trials):\n","        trial_seed = base_seed + 1000003*t\n","        print(f\"\\n============================ TRIAL {t+1}/{args.trials} (seed={trial_seed}) ============================\")\n","        set_seed(trial_seed)\n","        # per-trial args copy so builders see different seeds\n","        args_t = copy.deepcopy(args)\n","        args_t.seed = trial_seed\n","\n","        for fn in args.functions:\n","            if fn not in TASK_BUILDERS:\n","                print(f\"Skipping unknown function '{fn}'\")\n","                continue\n","            for d in args.dims:\n","                out = train_task(fn, d, TASK_BUILDERS[fn], args_t, device)\n","                out[\"trial\"] = t\n","                all_rows.append(out)\n","                if torch.cuda.is_available():\n","                    torch.cuda.empty_cache()\n","\n","    # Per-trial table\n","    print(\"\\n========================= PER-TRIAL RESULTS =========================\")\n","    print(f\"{'trial':>5} | {'task':>14} | {'d':>4} | {'RMSE':>10} | {'relRMSE':>10} | {'time(s)':>7}\")\n","    print(\"-\"*72)\n","    for r in all_rows:\n","        print(f\"{r['trial']:5d} | {r['task']:>14} | {r['d']:4d} | {r['rmse']:10.3e} | {r['rel_rmse']:10.3e} | {r['time']:7.1f}\")\n","\n","    # Aggregated mean ± std\n","    print_aggregate_table(all_rows)\n","    return all_rows\n","\n","def run_cli(argv: Optional[List[str]] = None):\n","    parser = build_parser()\n","    args, _ = parser.parse_known_args(argv)\n","    rows = run_trials(args)\n","    return rows\n","\n","def run_notebook_trials(functions=(\"quad_spd\",\"exp_minus_lin\",\"rand_icnn2\",\"softplus_pairs\"),\n","                        dims=(10,20,50),\n","                        trials=5,\n","                        steps=60000, lr=1e-3, seed=42,\n","                        spd_norm=\"spectral\", ridge=1e-2,\n","                        icnn_prelearn_steps=500, icnn_prelearn_lr=1e-3, icnn_prelearn_batch=2048,\n","                        icnn_prelearn_spd_norm=\"spectral\", icnn_prelearn_ridge=1e-2,\n","                        softplus_x_scale=0.5, softplus_block=64, softplus_max_batch=0,\n","                        softplus_whiten_samples=32768, softplus_cov_reg=1e-3,\n","                        loss=\"huber\", huber_delta=1.0,\n","                        sampler=\"macro\", mb_factor=0.25):\n","    class Args: pass\n","    args = Args()\n","    args.functions=list(functions); args.dims=list(dims); args.trials=int(trials)\n","    args.steps=int(steps); args.lr=float(lr); args.seed=int(seed)\n","    args.spd_norm=str(spd_norm); args.ridge=float(ridge)\n","    args.icnn_prelearn_steps=int(icnn_prelearn_steps); args.icnn_prelearn_lr=float(icnn_prelearn_lr)\n","    args.icnn_prelearn_batch=int(icnn_prelearn_batch); args.icnn_prelearn_spd_norm=str(icnn_prelearn_spd_norm)\n","    args.icnn_prelearn_ridge=float(icnn_prelearn_ridge)\n","    args.icnn_whiten_samples=32768; args.icnn_target_y_med=3.0; args.icnn_cov_reg=1e-3\n","    args.softplus_x_scale=float(softplus_x_scale); args.softplus_block=int(softplus_block)\n","    args.softplus_max_batch=int(softplus_max_batch); args.softplus_whiten_samples=int(softplus_whiten_samples)\n","    args.softplus_cov_reg=float(softplus_cov_reg)\n","    args.loss=str(loss); args.huber_delta=float(huber_delta)\n","    args.sampler=str(sampler); args.mb_factor=float(mb_factor)\n","\n","    rows = run_trials(args)\n","    # programmatic aggregation\n","    agg = {}\n","    for r in rows:\n","        k = (r[\"task\"], r[\"d\"])\n","        agg.setdefault(k, []).append(r[\"rmse\"])\n","    agg_stats = {k: {\"rmse_mean\": float(np.mean(v)), \"rmse_std\": float(np.std(v, ddof=1)) if len(v)>1 else 0.0}\n","                 for k,v in agg.items()}\n","    return rows, agg_stats\n","\n","# ============================ QUICK SMOKE TEST MAIN ============================\n","# To run full experiments later, comment out this block and use normal CLI, e.g.:\n","if __name__ == \"__main__\":\n","    # quick_args = [\n","    #     \"--functions\", \"quad_spd\", \"exp_minus_lin\", \"rand_icnn2\", \"softplus_pairs\",\n","    #     \"--dims\", \"5\", \"10\",\n","    #     \"--trials\", \"2\",                 # small repeats for smoke test\n","    #     \"--steps\", \"8000\",               # shorter than full run\n","    #     \"--lr\", \"1e-3\",\n","    #     \"--sampler\", \"macro\",            # or \"stream\"\n","    #     \"--mb_factor\", \"0.25\",\n","    #     # Make ICNN pretrain and whitening lighter for the smoke test\n","    #     \"--icnn_prelearn_steps\", \"300\",\n","    #     \"--icnn_prelearn_lr\", \"1e-3\",\n","    #     \"--icnn_prelearn_batch\", \"1024\",\n","    #     \"--icnn_whiten_samples\", \"8192\",\n","    #     # Stability knobs\n","    #     \"--spd_norm\", \"spectral\",\n","    #     \"--ridge\", \"1e-2\",\n","    #     \"--loss\", \"huber\",\n","    #     \"--huber_delta\", \"1.0\",\n","    #     # Soft-plus guards for small GPUs\n","    #     \"--softplus_max_batch\", \"2048\",\n","    #     \"--softplus_block\", \"48\",\n","    #     # Base seed (each trial gets a large-stride offset)\n","    #     \"--seed\", \"123\"\n","    # ]\n","    # run_cli(quick_args)\n","    run_cli()\n","# ==============================================================================\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVrcDAX4QzwX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNys0LfJ3xhRsYVOWzaxbhO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}