{"cells":[{"cell_type":"markdown","metadata":{"id":"IJ6rYWYZWwcq"},"source":["\n","\n","### Comparing Different NN Architectures\n","\n","####  Comparison with Direct Learning (When Dual is Known)\n","\n","To validate our Deep Legendre Transform (DLT) approach, we compare it to *direct learning* of the convex conjugate in cases where the analytical form of \\$f^\\*\\$ is known.\n","\n","We benchmark DLT against direct learning across multiple convex functions and input dimensions.\n","\n","\n","\n","####  Sampling Strategy\n","\n","We carefully match sampling between primal and dual spaces via the gradient map \\$\\nabla f(x)\\$.\n","\n","* **Quadratic**:\n","  $f(x) = \\frac{1}{2} \\|x\\|^2 \\quad\\Rightarrow\\quad \\nabla f(x) = x,\\quad f^*(y) = \\frac{1}{2} \\|y\\|^2$\n","  Sample \\$x \\sim \\mathcal{N}(0, I)\\$ ⇒ \\$y = x\\$\n","\n","* **Neg. Log**:\n","  $f(x) = -\\sum_{i=1}^d \\log(x_i) \\quad\\Rightarrow\\quad \\nabla f(x) = -\\frac{1}{x_i},\\quad f^*(y) = -\\sum \\log(-y_i) - d$\n","  Sample \\$x\\$ in $\\[0.1, 10]^d\\$ ⇒ \\$y \\in \\[-10, -0.1]^d\\$\n","\n","* **Neg. Entropy**:\n","  $f(x) = \\sum x_i \\log x_i \\quad\\Rightarrow\\quad \\nabla f(x) = \\log x_i + 1,\\quad f^*(y) = \\sum \\exp(y_i - 1)$\n","  Sample \\$x\\$ in log-space ⇒ \\$y \\in \\[-1.3, 3.3]^d\\$\n","\n","#### Results and Analysis\n","\n","We test dimensions \\$d \\in {2, 5, 10, 20}\\$ for:\n","\n","$$\n","\\begin{aligned}\n","\\text{Quadratic:} \\quad & f(x) = \\frac{\\|x\\|^2}{2}, \\quad f^*(y) = \\frac{\\|y\\|^2}{2} \\\\\n","\\text{Neg. Log:} \\quad & f(x) = -\\sum \\log(x_i), \\quad f^*(y) = -\\sum \\log(-y_i) - d \\\\\n","\\text{Neg. Entropy:} \\quad & f(x) = \\sum x_i \\log x_i, \\quad f^*(y) = \\sum \\exp(y_i - 1)\n","\\end{aligned}\n","$$\n","\n","> For the quadratic case, \\$f = f^\\*\\$ and \\$\\nabla f(x) = x\\$ — making it an ideal baseline.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Md1DBJErty4e","executionInfo":{"status":"ok","timestamp":1747224613550,"user_tz":-120,"elapsed":2113975,"user":{"displayName":"Alexey Minabutdinov","userId":"01550160032560633467"}},"outputId":"4c9ca5e1-6120-49ec-e445-be3af38d5ea5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==============================================================================\n","Quadratic benchmark\n","==============================================================================\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [#################...]  85.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","MLP       L2impl 2.96e-05  L2dir 2.86e-05  time 85.5/84.4s  ρ 1.04 σ=0.00\n","MLP_ICNN  L2impl 6.21e+00  L2dir 3.15e+01  time 87.4/76.9s  ρ 0.20 σ=0.00\n","ResNet    L2impl 2.43e-05  L2dir 3.00e-05  time 106.8/101.2s  ρ 0.81 σ=0.00\n","ICNN      L2impl 1.79e-02  L2dir 1.23e-02  time 92.7/93.0s  ρ 1.45 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [################....]  80.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","MLP       L2impl 2.28e-04  L2dir 3.41e-04  time 84.9/84.0s  ρ 0.67 σ=0.00\n","MLP_ICNN  L2impl 1.20e+02  L2dir 1.13e+02  time 71.3/82.6s  ρ 1.06 σ=0.00\n","ResNet    L2impl 2.53e-04  L2dir 2.32e-04  time 107.7/104.9s  ρ 1.09 σ=0.00\n","ICNN      L2impl 3.45e-02  L2dir 3.48e-02  time 95.1/93.3s  ρ 0.99 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [#################...]  85.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","MLP       L2impl 3.55e-03  L2dir 3.59e-03  time 87.5/86.6s  ρ 0.99 σ=0.00\n","MLP_ICNN  L2impl 6.46e+02  L2dir 6.46e+02  time 87.3/76.6s  ρ 1.00 σ=0.00\n","ResNet    L2impl 2.92e-03  L2dir 3.25e-03  time 123.8/119.7s  ρ 0.90 σ=0.00\n","ICNN      L2impl 7.18e-02  L2dir 6.66e-02  time 97.4/97.8s  ρ 1.08 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [##################..]  90.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","MLP       L2impl 2.77e-02  L2dir 2.46e-02  time 95.0/95.3s  ρ 1.13 σ=0.00\n","MLP_ICNN  L2impl 2.54e+03  L2dir 2.59e+03  time 96.8/87.4s  ρ 0.98 σ=0.00\n","ResNet    L2impl 1.88e-02  L2dir 1.91e-02  time 136.3/132.0s  ρ 0.98 σ=0.00\n","ICNN      L2impl 9.59e-02  L2dir 9.06e-02  time 105.7/105.5s  ρ 1.06 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [#################...]  85.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","MLP       L2impl 1.43e-01  L2dir 1.50e-01  time 113.5/112.7s  ρ 0.95 σ=0.00\n","MLP_ICNN  L2impl 1.01e+04  L2dir 1.01e+04  time 97.6/113.7s  ρ 1.00 σ=0.00\n","ResNet    L2impl 9.53e-02  L2dir 9.40e-02  time 163.1/164.4s  ρ 1.01 σ=0.00\n","ICNN      L2impl 4.01e-01  L2dir 3.73e-01  time 124.1/125.8s  ρ 1.08 σ=0.00\n","\n","==============================================================================\n","Neg.\\ Log benchmark\n","==============================================================================\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [#############.......]  65.0%\n","[expl] [############........]  60.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [##########..........]  50.0%\n","[expl] [############........]  60.0%\n","MLP       L2impl 6.75e-04  L2dir 8.43e-04  time 94.1/94.4s  ρ 0.80 σ=0.00\n","MLP_ICNN  L2impl 4.23e+00  L2dir 4.23e+00  time 63.7/62.4s  ρ 1.00 σ=0.00\n","ResNet    L2impl 7.34e-04  L2dir 9.22e-04  time 108.7/111.0s  ρ 0.80 σ=0.00\n","ICNN      L2impl 6.47e+00  L2dir 4.22e+00  time 54.1/65.4s  ρ 1.53 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [##################..]  90.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [##############......]  70.0%\n","[expl] [############........]  60.0%\n","MLP       L2impl 4.31e-03  L2dir 3.60e-03  time 93.8/95.8s  ρ 1.20 σ=0.00\n","MLP_ICNN  L2impl 7.43e+00  L2dir 7.16e+00  time 86.2/96.8s  ρ 1.04 σ=0.00\n","ResNet    L2impl 5.36e-03  L2dir 6.20e-03  time 114.0/115.2s  ρ 0.87 σ=0.00\n","ICNN      L2impl 8.75e+00  L2dir 8.69e+00  time 71.4/63.5s  ρ 1.01 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [#############.......]  65.0%\n","[expl] [#############.......]  65.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [#############.......]  65.0%\n","[expl] [###########.........]  55.0%\n","MLP       L2impl 3.81e-01  L2dir 2.81e-01  time 97.5/97.8s  ρ 1.36 σ=0.00\n","MLP_ICNN  L2impl 2.03e+01  L2dir 2.03e+01  time 67.3/70.4s  ρ 1.00 σ=0.00\n","ResNet    L2impl 1.05e-01  L2dir 1.23e-01  time 133.5/130.7s  ρ 0.85 σ=0.00\n","ICNN      L2impl 2.03e+01  L2dir 2.06e+01  time 73.5/61.9s  ρ 0.99 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [##############......]  70.0%\n","[expl] [############........]  60.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [##############......]  70.0%\n","[expl] [############........]  60.0%\n","MLP       L2impl 8.56e+00  L2dir 6.67e+00  time 106.2/106.1s  ρ 1.28 σ=0.00\n","MLP_ICNN  L2impl 4.00e+01  L2dir 3.99e+01  time 73.0/66.2s  ρ 1.00 σ=0.00\n","ResNet    L2impl 1.38e+00  L2dir 1.93e+00  time 141.5/142.3s  ρ 0.72 σ=0.00\n","ICNN      L2impl 3.99e+01  L2dir 3.98e+01  time 80.3/71.1s  ρ 1.00 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###########.........]  55.0%\n","[expl] [############........]  60.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","MLP       L2impl 3.29e+01  L2dir 2.68e+01  time 124.7/125.1s  ρ 1.23 σ=0.00\n","MLP_ICNN  L2impl 8.21e+01  L2dir 8.20e+01  time 68.9/78.7s  ρ 1.00 σ=0.00\n","ResNet    L2impl 2.92e+01  L2dir 2.99e+01  time 168.7/174.1s  ρ 0.98 σ=0.00\n","ICNN      L2impl 1.70e+01  L2dir 1.68e+01  time 131.2/137.0s  ρ 1.01 σ=0.00\n","\n","==============================================================================\n","Neg.\\ Entropy benchmark\n","==============================================================================\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","MLP       L2impl 1.06e-03  L2dir 1.26e-03  time 95.6/100.3s  ρ 0.84 σ=0.00\n","MLP_ICNN  L2impl 4.56e+00  L2dir 4.01e+00  time 92.8/101.0s  ρ 1.14 σ=0.00\n","ResNet    L2impl 6.62e-04  L2dir 7.41e-04  time 109.8/115.7s  ρ 0.89 σ=0.00\n","ICNN      L2impl 1.31e-02  L2dir 1.74e-02  time 98.8/107.2s  ρ 0.75 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [#############.......]  65.0%\n","[expl] [#############.......]  65.0%\n","MLP       L2impl 5.88e-03  L2dir 7.83e-03  time 94.5/99.8s  ρ 0.75 σ=0.00\n","MLP_ICNN  L2impl 2.80e+01  L2dir 2.82e+01  time 92.7/102.2s  ρ 0.99 σ=0.00\n","ResNet    L2impl 3.88e-03  L2dir 4.24e-03  time 113.7/118.6s  ρ 0.91 σ=0.00\n","ICNN      L2impl 3.68e+01  L2dir 3.67e+01  time 68.1/72.3s  ρ 1.00 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###########.........]  55.0%\n","[expl] [############........]  60.0%\n","MLP       L2impl 2.71e-01  L2dir 3.91e-01  time 99.6/100.8s  ρ 0.69 σ=0.00\n","MLP_ICNN  L2impl 7.83e+01  L2dir 7.87e+01  time 97.5/104.6s  ρ 1.00 σ=0.00\n","ResNet    L2impl 6.93e-02  L2dir 7.27e-02  time 132.8/135.0s  ρ 0.95 σ=0.00\n","ICNN      L2impl 9.39e+01  L2dir 9.39e+01  time 59.8/69.1s  ρ 1.00 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###########.........]  55.0%\n","[expl] [#############.......]  65.0%\n","MLP       L2impl 1.77e+01  L2dir 1.65e+01  time 106.5/108.7s  ρ 1.07 σ=0.00\n","MLP_ICNN  L2impl 1.45e+02  L2dir 1.44e+02  time 103.5/111.2s  ρ 1.01 σ=0.00\n","ResNet    L2impl 1.53e+00  L2dir 1.32e+00  time 142.0/144.2s  ρ 1.16 σ=0.00\n","ICNN      L2impl 1.93e+02  L2dir 1.93e+02  time 62.9/79.9s  ρ 1.00 σ=0.00\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","[impl] [##########..........]  50.0%\n","[expl] [###################.]  95.0%\n","[impl] [###################.]  95.0%\n","[expl] [###################.]  95.0%\n","MLP       L2impl 4.92e+01  L2dir 5.07e+01  time 124.2/127.3s  ρ 0.97 σ=0.00\n","MLP_ICNN  L2impl 2.87e+02  L2dir 2.87e+02  time 125.3/133.3s  ρ 1.00 σ=0.00\n","ResNet    L2impl 5.08e+01  L2dir 4.02e+01  time 94.2/175.4s  ρ 1.26 σ=0.00\n","ICNN      L2impl 9.43e+01  L2dir 9.30e+01  time 131.9/139.2s  ρ 1.01 σ=0.00\n","\n","Combined LaTeX table (also saved to results/combined_table.tex):\n","\n","\\begin{table}[h]\n","  \\centering\n","  \\caption{Benchmark results comparing implicit DLT against direct learning with known duals}\n","  \\label{tab:combined_benchmark}\n","  \\begin{tabular}{ccc|cc|cc|cc}\n","    \\toprule\n","    \\multirow{2}{*}{Function} & \\multirow{2}{*}{$d$} & \\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{$L^2$ Error} & \\multicolumn{2}{c|}{Time (s)} & \\multicolumn{2}{c}{Ratio} \\\\\n","    & & & Impl. & Dir. & Impl. & Dir. & $\\mu$ & $\\sigma$ \\\\\n","    \\midrule\n","\\multirow{20}{*}{Quadratic} & \\multirow{4}{*}{10} & MLP & 2.96e-05 & 2.86e-05 & 85.5 & 84.4 & 1.04 & 0.00 \\\\\n","  &   & MLP_ICNN & 6.21e+00 & 3.15e+01 & 87.4 & 76.9 & 0.20 & 0.00 \\\\\n","  &   & ResNet & 2.43e-05 & 3.00e-05 & 106.8 & 101.2 & 0.81 & 0.00 \\\\\n","  &   & ICNN & 1.79e-02 & 1.23e-02 & 92.7 & 93.0 & 1.45 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{20} & MLP & 2.28e-04 & 3.41e-04 & 84.9 & 84.0 & 0.67 & 0.00 \\\\\n","  &   & MLP_ICNN & 1.20e+02 & 1.13e+02 & 71.3 & 82.6 & 1.06 & 0.00 \\\\\n","  &   & ResNet & 2.53e-04 & 2.32e-04 & 107.7 & 104.9 & 1.09 & 0.00 \\\\\n","  &   & ICNN & 3.45e-02 & 3.48e-02 & 95.1 & 93.3 & 0.99 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{50} & MLP & 3.55e-03 & 3.59e-03 & 87.5 & 86.6 & 0.99 & 0.00 \\\\\n","  &   & MLP_ICNN & 6.46e+02 & 6.46e+02 & 87.3 & 76.6 & 1.00 & 0.00 \\\\\n","  &   & ResNet & 2.92e-03 & 3.25e-03 & 123.8 & 119.7 & 0.90 & 0.00 \\\\\n","  &   & ICNN & 7.18e-02 & 6.66e-02 & 97.4 & 97.8 & 1.08 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{100} & MLP & 2.77e-02 & 2.46e-02 & 95.0 & 95.3 & 1.13 & 0.00 \\\\\n","  &   & MLP_ICNN & 2.54e+03 & 2.59e+03 & 96.8 & 87.4 & 0.98 & 0.00 \\\\\n","  &   & ResNet & 1.88e-02 & 1.91e-02 & 136.3 & 132.0 & 0.98 & 0.00 \\\\\n","  &   & ICNN & 9.59e-02 & 9.06e-02 & 105.7 & 105.5 & 1.06 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{200} & MLP & 1.43e-01 & 1.50e-01 & 113.5 & 112.7 & 0.95 & 0.00 \\\\\n","  &   & MLP_ICNN & 1.01e+04 & 1.01e+04 & 97.6 & 113.7 & 1.00 & 0.00 \\\\\n","  &   & ResNet & 9.53e-02 & 9.40e-02 & 163.1 & 164.4 & 1.01 & 0.00 \\\\\n","  &   & ICNN & 4.01e-01 & 3.73e-01 & 124.1 & 125.8 & 1.08 & 0.00 \\\\\n","    \\midrule\n","\\multirow{20}{*}{Neg.\\ Log} & \\multirow{4}{*}{10} & MLP & 6.75e-04 & 8.43e-04 & 94.1 & 94.4 & 0.80 & 0.00 \\\\\n","  &   & MLP_ICNN & 4.23e+00 & 4.23e+00 & 63.7 & 62.4 & 1.00 & 0.00 \\\\\n","  &   & ResNet & 7.34e-04 & 9.22e-04 & 108.7 & 111.0 & 0.80 & 0.00 \\\\\n","  &   & ICNN & 6.47e+00 & 4.22e+00 & 54.1 & 65.4 & 1.53 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{20} & MLP & 4.31e-03 & 3.60e-03 & 93.8 & 95.8 & 1.20 & 0.00 \\\\\n","  &   & MLP_ICNN & 7.43e+00 & 7.16e+00 & 86.2 & 96.8 & 1.04 & 0.00 \\\\\n","  &   & ResNet & 5.36e-03 & 6.20e-03 & 114.0 & 115.2 & 0.87 & 0.00 \\\\\n","  &   & ICNN & 8.75e+00 & 8.69e+00 & 71.4 & 63.5 & 1.01 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{50} & MLP & 3.81e-01 & 2.81e-01 & 97.5 & 97.8 & 1.36 & 0.00 \\\\\n","  &   & MLP_ICNN & 2.03e+01 & 2.03e+01 & 67.3 & 70.4 & 1.00 & 0.00 \\\\\n","  &   & ResNet & 1.05e-01 & 1.23e-01 & 133.5 & 130.7 & 0.85 & 0.00 \\\\\n","  &   & ICNN & 2.03e+01 & 2.06e+01 & 73.5 & 61.9 & 0.99 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{100} & MLP & 8.56e+00 & 6.67e+00 & 106.2 & 106.1 & 1.28 & 0.00 \\\\\n","  &   & MLP_ICNN & 4.00e+01 & 3.99e+01 & 73.0 & 66.2 & 1.00 & 0.00 \\\\\n","  &   & ResNet & 1.38e+00 & 1.93e+00 & 141.5 & 142.3 & 0.72 & 0.00 \\\\\n","  &   & ICNN & 3.99e+01 & 3.98e+01 & 80.3 & 71.1 & 1.00 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{200} & MLP & 3.29e+01 & 2.68e+01 & 124.7 & 125.1 & 1.23 & 0.00 \\\\\n","  &   & MLP_ICNN & 8.21e+01 & 8.20e+01 & 68.9 & 78.7 & 1.00 & 0.00 \\\\\n","  &   & ResNet & 2.92e+01 & 2.99e+01 & 168.7 & 174.1 & 0.98 & 0.00 \\\\\n","  &   & ICNN & 1.70e+01 & 1.68e+01 & 131.2 & 137.0 & 1.01 & 0.00 \\\\\n","    \\midrule\n","\\multirow{20}{*}{Neg.\\ Entropy} & \\multirow{4}{*}{10} & MLP & 1.06e-03 & 1.26e-03 & 95.6 & 100.3 & 0.84 & 0.00 \\\\\n","  &   & MLP_ICNN & 4.56e+00 & 4.01e+00 & 92.8 & 101.0 & 1.14 & 0.00 \\\\\n","  &   & ResNet & 6.62e-04 & 7.41e-04 & 109.8 & 115.7 & 0.89 & 0.00 \\\\\n","  &   & ICNN & 1.31e-02 & 1.74e-02 & 98.8 & 107.2 & 0.75 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{20} & MLP & 5.88e-03 & 7.83e-03 & 94.5 & 99.8 & 0.75 & 0.00 \\\\\n","  &   & MLP_ICNN & 2.80e+01 & 2.82e+01 & 92.7 & 102.2 & 0.99 & 0.00 \\\\\n","  &   & ResNet & 3.88e-03 & 4.24e-03 & 113.7 & 118.6 & 0.91 & 0.00 \\\\\n","  &   & ICNN & 3.68e+01 & 3.67e+01 & 68.1 & 72.3 & 1.00 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{50} & MLP & 2.71e-01 & 3.91e-01 & 99.6 & 100.8 & 0.69 & 0.00 \\\\\n","  &   & MLP_ICNN & 7.83e+01 & 7.87e+01 & 97.5 & 104.6 & 1.00 & 0.00 \\\\\n","  &   & ResNet & 6.93e-02 & 7.27e-02 & 132.8 & 135.0 & 0.95 & 0.00 \\\\\n","  &   & ICNN & 9.39e+01 & 9.39e+01 & 59.8 & 69.1 & 1.00 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{100} & MLP & 1.77e+01 & 1.65e+01 & 106.5 & 108.7 & 1.07 & 0.00 \\\\\n","  &   & MLP_ICNN & 1.45e+02 & 1.44e+02 & 103.5 & 111.2 & 1.01 & 0.00 \\\\\n","  &   & ResNet & 1.53e+00 & 1.32e+00 & 142.0 & 144.2 & 1.16 & 0.00 \\\\\n","  &   & ICNN & 1.93e+02 & 1.93e+02 & 62.9 & 79.9 & 1.00 & 0.00 \\\\\n","    \\cmidrule{2-9}\n","  & \\multirow{4}{*}{200} & MLP & 4.92e+01 & 5.07e+01 & 124.2 & 127.3 & 0.97 & 0.00 \\\\\n","  &   & MLP_ICNN & 2.87e+02 & 2.87e+02 & 125.3 & 133.3 & 1.00 & 0.00 \\\\\n","  &   & ResNet & 5.08e+01 & 4.02e+01 & 94.2 & 175.4 & 1.26 & 0.00 \\\\\n","  &   & ICNN & 9.43e+01 & 9.30e+01 & 131.9 & 139.2 & 1.01 & 0.00 \\\\\n","    \\bottomrule\n","  \\end{tabular}\n","\\end{table}\n","\n"]}],"source":["#!/usr/bin/env python3\n","# benchmark.py – implicit vs explicit convex-conjugate learning\n","# ν‑sampling • staircase LR • per‑model activations (relu|gelu|softplus)\n","# batch size: \"scale\" (d×64) or constant • repeats with σ\n","# prints rows + one combined LaTeX table\n","# --------------------------------------------------------------------\n","from __future__ import annotations\n","import os, sys, time, argparse\n","from functools import partial\n","from typing import Sequence, Callable, Dict\n","\n","import jax, jax.numpy as jnp, optax\n","from jax import random\n","from flax import linen as nn\n","from flax.training import train_state\n","import numpy as np\n","\n","# ═════ 1. convex test functions ═════════════════════════════════════\n","f_quad,  grad_quad  = lambda x: 0.5*jnp.sum(x**2, -1),        lambda x: x\n","fst_quad            = lambda y: 0.5*jnp.sum(y**2, -1)\n","\n","f_nlog,  grad_nlog  = lambda x:-jnp.sum(jnp.log(x), -1),      lambda x:-1./x\n","fst_nlog            = lambda y:-jnp.sum(jnp.log(-y), -1) - y.shape[-1]\n","\n","f_nent,  grad_nent  = lambda x:jnp.sum(x*jnp.log(x), -1),     lambda x:jnp.log(x)+1\n","fst_nent            = lambda y:jnp.sum(jnp.exp(y-1.), -1)\n","\n","def _u(rng, sh, lo, hi):\n","    return random.uniform(rng, shape=sh, minval=lo, maxval=hi,\n","                          dtype=jnp.float32)\n","\n","FUNCTIONS = {\n","    \"quadratic\":   (f_quad, grad_quad, fst_quad,\n","                    lambda k,s: random.normal(k, s, dtype=jnp.float32)),\n","    \"neg_log\":     (f_nlog, grad_nlog, fst_nlog,\n","                    lambda k,s: jnp.exp(_u(k, s, -2.3,  2.3))),\n","    \"neg_entropy\": (f_nent, grad_nent, fst_nent,\n","                    lambda k,s: jnp.exp(_u(k, s, -2.3,  2.3))),\n","}\n","FUNCPRINT = {\"quadratic\": \"Quadratic\",\n","             \"neg_log\":   \"Neg.\\ Log\",\n","             \"neg_entropy\":\"Neg.\\ Entropy\"}\n","\n","# ═════ 2. activations ═══════════════════════════════════════════════\n","def _act(name:str)->Callable:\n","    n=name.lower()\n","    if n==\"relu\":     return nn.relu\n","    if n==\"gelu\":     return jax.nn.gelu\n","    if n==\"softplus\": return jax.nn.softplus\n","    raise ValueError(f\"unknown activation {name}\")\n","\n","# ═════ 3. model zoo ═════════════════════════════════════════════════\n","class DensePos(nn.Module):\n","    features:int; use_bias:bool=True\n","    @nn.compact\n","    def __call__(self,x):\n","        W = nn.softplus(self.param(\"rawW\", nn.initializers.lecun_normal(),\n","                                   (x.shape[-1], self.features)))\n","        y = x @ W\n","        if self.use_bias:\n","            y += self.param(\"b\", nn.initializers.zeros, (self.features,))\n","        return y\n","\n","class MLP(nn.Module):\n","    hidden:Sequence[int]; act:Callable=nn.relu\n","    @nn.compact\n","    def __call__(self,x):\n","        for h in self.hidden: x = self.act(nn.Dense(h)(x))\n","        return jnp.squeeze(nn.Dense(1)(x), -1)\n","\n","class MLP_ICNN(nn.Module):\n","    hidden:Sequence[int]; act:Callable=nn.relu\n","    @nn.compact\n","    def __call__(self,x):\n","        z=x\n","        for h in self.hidden: z = self.act(DensePos(h)(z))\n","        out = DensePos(1, use_bias=False)(z) + nn.Dense(1, use_bias=False)(x)\n","        return jnp.squeeze(out, -1)\n","\n","class ICNN(nn.Module):\n","    hidden:Sequence[int]; act:Callable=nn.relu\n","    @nn.compact\n","    def __call__(self,x):\n","        z=jnp.zeros((x.shape[0],1))\n","        for h in self.hidden:\n","            z = self.act(DensePos(h)(z) + nn.Dense(h)(x))\n","        out = DensePos(1, use_bias=False)(z) + nn.Dense(1, use_bias=False)(x)\n","        return jnp.squeeze(out, -1)\n","\n","class ResBlock(nn.Module):\n","    f:int; act:Callable=nn.relu\n","    @nn.compact\n","    def __call__(self,x):\n","        y=self.act(nn.Dense(self.f)(x)); y=nn.Dense(self.f)(y)\n","        if x.shape[-1]!=self.f:\n","            x = nn.Dense(self.f, use_bias=False)(x)\n","        return self.act(x+y)\n","\n","class ResNet(nn.Module):\n","    hidden:Sequence[int]; act:Callable=nn.relu\n","    @nn.compact\n","    def __call__(self,x):\n","        for h in self.hidden: x = ResBlock(h, act=self.act)(x)\n","        return jnp.squeeze(nn.Dense(1)(x), -1)\n","\n","def parse_hidden(s:str)->tuple[int,...]:\n","    return tuple(int(v) for v in s.split(\",\") if v)\n","\n","# ═════ 4. optimiser / losses / jit helpers ═════════════════════════\n","class State(train_state.TrainState): ...\n","\n","def schedule(lr:float):\n","    return optax.exponential_decay(lr, 20_000, 0.5, staircase=True)\n","\n","def new_state(rng, model, d, lr):\n","    params = model.init(rng, jnp.zeros((1, d), jnp.float32))[\"params\"]\n","    return State.create(apply_fn=model.apply, params=params,\n","                        tx=optax.adam(schedule(lr)))\n","\n","loss_impl = lambda p,af,x,f,g: jnp.mean(\n","    (af({\"params\":p}, g(x)) - (jnp.sum(x*g(x), -1) - f(x)))**2)\n","loss_expl = lambda p,af,y,fst: jnp.mean((af({\"params\":p}, y) - fst(y))**2)\n","\n","@partial(jax.jit, static_argnums=(2,3))\n","def step_impl(st,b,f,g):\n","    l,gr = jax.value_and_grad(loss_impl)(st.params, st.apply_fn, b, f, g)\n","    return st.apply_gradients(grads=gr), l\n","\n","@partial(jax.jit, static_argnums=(2,))\n","def step_expl(st,b,fst):\n","    l,gr = jax.value_and_grad(loss_expl)(st.params, st.apply_fn, b, fst)\n","    return st.apply_gradients(grads=gr), l\n","\n","@partial(jax.jit, static_argnums=(1,3,4))\n","def _ei(p,af,x,f,g): return loss_impl(p,af,x,f,g)\n","def eval_impl(p,af,x,f,g): return float(_ei(p,af,x,f,g))\n","\n","@partial(jax.jit, static_argnums=(1,3))\n","def _ee(p,af,y,fst): return loss_expl(p,af,y,fst)\n","def eval_expl(p,af,y,fst): return float(_ee(p,af,y,fst))\n","\n","# ═════ 5. early stopping ═══════════════════════════════════════════\n","class Stopper:\n","    def __init__(self, pat:int, tol:float=1e-6):\n","        self.best=float(\"inf\"); self.pat=pat; self.tol=tol\n","        self.cnt=0; self.bp=None\n","    def update(self, loss, params):\n","        loss=float(loss)\n","        if loss+self.tol < self.best:\n","            self.best, self.cnt = loss, 0; self.bp = params\n","        else:\n","            self.cnt += 1\n","        return self.cnt >= self.pat or self.best < self.tol\n","    def res(self): return self.best, self.bp\n","\n","# ═════ 6. utilities ════════════════════════════════════════════════\n","def batch_size(d:int, arg:str)->int:\n","    return d*64 if arg==\"scale\" else int(arg)\n","\n","# ═════ 7. training routine (returns err & time) ════════════════════\n","def train(model_fn, d, f, g, samp, steps, lr, pat, seed,\n","          implicit:bool, batch:int, verb=False):\n","    st   = new_state(random.PRNGKey(seed), model_fn(), d, lr)\n","    stop = Stopper(pat)\n","    step = step_impl if implicit else step_expl\n","    tag  = \"impl\" if implicit else \"expl\"\n","    bar  = max(steps//20, 1)\n","    t0   = time.perf_counter()\n","\n","    for i in range(steps):\n","        mb = samp(random.fold_in(random.PRNGKey(seed+999), i), (batch, d))\n","        st, loss = step(st, mb, f, g) if implicit else step(st, mb, f)\n","        if stop.update(loss, st.params):\n","            break\n","        if not verb and i%bar==0:\n","            pct = i/steps; br = int(20*pct)\n","            sys.stdout.write(f\"\\r[{tag}] [{'#'*br}{'.'*(20-br)}] {pct*100:5.1f}%\")\n","            sys.stdout.flush()\n","        elif verb and i%bar==0:\n","            print(f\"[{tag}] {i:6d}/{steps} \"\n","                  f\"({100*i/steps:5.1f}%) loss {float(loss):.3e}\")\n","    if not verb:\n","        sys.stdout.write(\"\\n\")\n","\n","    _, bp = stop.res()\n","    rng0  = random.PRNGKey(0)\n","    err = (eval_impl if implicit else eval_expl)(\n","        bp, st.apply_fn,\n","        samp(rng0, (batch, d)),\n","        f, g) if implicit else \\\n","        eval_expl(bp, st.apply_fn, samp(rng0, (batch, d)), f)\n","    return err, time.perf_counter() - t0\n","\n","# ═════ 8. benchmark (means, σ, times) ══════════════════════════════\n","def bench(fn, d, steps, pat, models, runs, batch_arg, verb):\n","    f, g, fst, sampx = FUNCTIONS[fn]\n","    sampy = lambda k, sh: g(sampx(k, sh))\n","    bs = batch_size(d, batch_arg)\n","    rows = []\n","\n","    for nm, sp in models.items():\n","        l2I, l2E, tI, tE, ratios = [], [], [], [], []\n","        for r in range(runs):\n","            if verb: print(f\"\\n▶ {nm} ({fn}, d={d}) run {r+1}/{runs}\")\n","            errI, timeI = train(sp[\"make\"], d, f, g, sampx,\n","                                steps, sp[\"lr\"], pat,\n","                                7000+d*11+r*5, True,  bs, verb)\n","            errE, timeE = train(sp[\"make\"], d, fst, None, sampy,\n","                                steps, sp[\"lr\"], pat,\n","                                7100+d*13+r*5, False, bs, verb)\n","            l2I.append(errI); l2E.append(errE)\n","            tI.append(timeI);  tE.append(timeE)\n","            ratios.append(errI/errE if errE else 1.)\n","        rows.append(dict(model=nm, d=d,\n","                         l2I=float(np.mean(l2I)), l2E=float(np.mean(l2E)),\n","                         tI=float(np.mean(tI)),   tE=float(np.mean(tE)),\n","                         rho_mu=float(np.mean(ratios)),\n","                         rho_sigma=float(np.std(ratios))))\n","    return rows\n","\n","# ═════ 9. combined LaTeX helper ════════════════════════════════════\n","# ──────────────────────────────────────────────────────────────────────\n","# 9′.  Combined LaTeX helper  (4 rows per model‑block, keeps Time + σ)\n","# ──────────────────────────────────────────────────────────────────────\n","def tex_tables(res: Dict[str, list], dims):\n","    fun_order = [\"quadratic\", \"neg_log\", \"neg_entropy\"]\n","    model_order = [\"MLP\", \"MLP_ICNN\", \"ResNet\", \"ICNN\"]\n","\n","    tex = [\n","        \"\\\\begin{table}[h]\",\n","        \"  \\\\centering\",\n","        \"  \\\\caption{Benchmark results comparing implicit DLT against direct learning with known duals}\",\n","        \"  \\\\label{tab:combined_benchmark}\",\n","        \"  \\\\begin{tabular}{ccc|cc|cc|cc}\",\n","        \"    \\\\toprule\",\n","        \"    \\\\multirow{2}{*}{Function} & \\\\multirow{2}{*}{$d$} & \\\\multirow{2}{*}{Model}\"\n","        \" & \\\\multicolumn{2}{c|}{$L^2$ Error} & \\\\multicolumn{2}{c|}{Time (s)} & \\\\multicolumn{2}{c}{Ratio} \\\\\\\\\",\n","        \"    & & & Impl. & Dir. & Impl. & Dir. & $\\\\mu$ & $\\\\sigma$ \\\\\\\\\",\n","        \"    \\\\midrule\"\n","    ]\n","\n","    for fn in fun_order:\n","        rows_fn = sorted(res.get(fn, []),\n","                         key=lambda r: (r[\"d\"], model_order.index(r[\"model\"])))\n","        if not rows_fn:\n","            continue\n","        total_rows_fn = len(rows_fn)\n","        fn_first_row_written = False\n","\n","        for d in dims:\n","            rows_dim = [r for r in rows_fn if r[\"d\"] == d]\n","            if not rows_dim:\n","                continue\n","            rows_dim = sorted(rows_dim,\n","                              key=lambda r: model_order.index(r[\"model\"]))\n","            dim_first_row_written = False\n","\n","            for r in rows_dim:\n","                line_parts = []\n","                # Function column\n","                if not fn_first_row_written:\n","                    line_parts.append(\n","                        f\"\\\\multirow{{{total_rows_fn}}}{{*}}{{{FUNCPRINT[fn]}}}\")\n","                    fn_first_row_written = True\n","                else:\n","                    line_parts.append(\" \")\n","\n","                # Dimension column\n","                if not dim_first_row_written:\n","                    line_parts.append(\n","                        f\"\\\\multirow{{{len(rows_dim)}}}{{*}}{{{d}}}\")\n","                    dim_first_row_written = True\n","                else:\n","                    line_parts.append(\" \")\n","\n","                # Model + metrics\n","                line_parts.extend([\n","                    r[\"model\"],\n","                    f\"{r['l2I']:.2e}\", f\"{r['l2E']:.2e}\",\n","                    f\"{r['tI']:.1f}\",  f\"{r['tE']:.1f}\",\n","                    f\"{r['rho_mu']:.2f}\", f\"{r['rho_sigma']:.2f}\"\n","                ])\n","                tex.append(\" & \".join(line_parts) + \" \\\\\\\\\")\n","            # horizontal line between different d‑blocks\n","            if d != dims[-1]:\n","                tex.append(\"    \\\\cmidrule{2-9}\")\n","        # mid‑rule between functions\n","        if fn != fun_order[-1]:\n","            tex.append(\"    \\\\midrule\")\n","\n","    tex += [\n","        \"    \\\\bottomrule\",\n","        \"  \\\\end{tabular}\",\n","        \"\\\\end{table}\"\n","    ]\n","\n","    table = \"\\n\".join(tex)\n","    os.makedirs(\"results\", exist_ok=True)\n","    with open(\"results/combined_table.tex\", \"w\") as f:\n","        f.write(table)\n","    print(\"\\nCombined LaTeX table (also saved to results/combined_table.tex):\\n\")\n","    print(table + \"\\n\")\n","    return table\n","\n","\n","# ═════ 10. CLI & main ═══════════════════════════════════════════════\n","def build_parser():\n","    P = argparse.ArgumentParser()\n","    P.add_argument(\"--steps\", type=int, default=50_000)\n","    P.add_argument(\"--patience\", type=int, default=10_000)\n","    P.add_argument(\"--lr\", type=float, default=1e-3)\n","    P.add_argument(\"--runs\", type=int, default=1)\n","    P.add_argument(\"--batch\", default=\"scale\")\n","    P.add_argument(\"--dims\", nargs=\"+\", type=int, default=[10, 20, 50, 100, 200])\n","    P.add_argument(\"--verbose\", action=\"store_true\")\n","    # hidden sizes\n","    P.add_argument(\"--mlp_hidden\", default=\"512,512\")\n","    P.add_argument(\"--mlp_icnn_hidden\", default=\"512,512\")\n","    P.add_argument(\"--resnet_hidden\", default=\"512,512\")\n","    P.add_argument(\"--icnn_hidden\", default=\"512,512\")\n","    # learning rates\n","    P.add_argument(\"--mlp_lr\", type=float)\n","    P.add_argument(\"--mlp_icnn_lr\", type=float)\n","    P.add_argument(\"--resnet_lr\", type=float)\n","    P.add_argument(\"--icnn_lr\", type=float)\n","    # activations\n","    P.add_argument(\"--mlp_act\", default=\"gelu\")\n","    P.add_argument(\"--mlp_icnn_act\", default=\"softplus\")\n","    P.add_argument(\"--resnet_act\", default=\"gelu\")\n","    P.add_argument(\"--icnn_act\", default=\"softplus\")\n","    return P\n","\n","def main(argv=None):\n","    args, _ = build_parser().parse_known_args(argv or sys.argv[1:])\n","    base_lr = args.lr\n","    models = {\n","        \"MLP\": {\n","            \"make\": lambda: MLP(parse_hidden(args.mlp_hidden),\n","                                act=_act(args.mlp_act)),\n","            \"lr\": args.mlp_lr or base_lr},\n","        \"MLP_ICNN\": {\n","            \"make\": lambda: MLP_ICNN(parse_hidden(args.mlp_icnn_hidden),\n","                                     act=_act(args.mlp_icnn_act)),\n","            \"lr\": args.mlp_icnn_lr or base_lr*3},\n","        \"ResNet\": {\n","            \"make\": lambda: ResNet(parse_hidden(args.resnet_hidden),\n","                                   act=_act(args.resnet_act)),\n","            \"lr\": args.resnet_lr or base_lr},\n","        \"ICNN\": {\n","            \"make\": lambda: ICNN(parse_hidden(args.icnn_hidden),\n","                                 act=_act(args.icnn_act)),\n","            \"lr\": args.icnn_lr or base_lr*3},\n","    }\n","\n","    all_res = {}\n","    for fn in FUNCTIONS:\n","        print(\"\\n\" + \"=\"*78 + f\"\\n{FUNCPRINT[fn]} benchmark\\n\" + \"=\"*78)\n","        all_res[fn] = []\n","        for d in args.dims:\n","            rows = bench(fn, d, args.steps, args.patience,\n","                         models, args.runs, args.batch, args.verbose)\n","            all_res[fn].extend(rows)\n","            for r in rows:\n","                print(f\"{r['model']:<10}\"\n","                      f\"L2impl {r['l2I']:.2e}  L2dir {r['l2E']:.2e}  \"\n","                      f\"time {r['tI']:.1f}/{r['tE']:.1f}s  \"\n","                      f\"ρ {r['rho_mu']:.2f} σ={r['rho_sigma']:.2f}\")\n","\n","    tex_tables(all_res, args.dims)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iI01Pb0i97Qq"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMrKq9xM6xfE8YwIXqes20J"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}